/**
 * @license
 * Copyright 2021 Google LLC. All Rights Reserved.
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 * =============================================================================
 */
!function(t,e){"object"==typeof exports&&"undefined"!=typeof module?e(exports,require("@tensorflow/tfjs-core")):"function"==typeof define&&define.amd?define(["exports","@tensorflow/tfjs-core"],e):e((t=t||self).tf=t.tf||{},t.tf)}(this,(function(t,e){"use strict";const n="Add",s="BatchMatMul",i="BatchToSpaceND",r="Cast",a="Concat",o="Conv2D",l="Conv2DBackpropInput",u="Cosh",h="Cumsum",c="RealDiv",p="ExpandDims",d="Floor",f="FloorDiv",g="GatherV2",m="GreaterEqual",y="Identity",b="Maximum",w="Multiply",k="Pack",v="PadV2",S="Reshape",x="Reverse",N="Rsqrt",z="Select",I="Slice",A="Sinh",C="Sigmoid",T="Sqrt",$="SpaceToBatchND",E="SplitV",F="Tile",D="Transpose",L="Unpack",_="UnsortedSegmentSum",R="ZerosLike",M="Step";function O(t){throw new Error(`'${t}' not yet implemented or not found in the registry. This kernel may not be supported by the tfjs backend you have chosen`)}function B(t,e){if(!t)throw new Error("string"==typeof e?e:e())}function P(t,e=[],n=!1){if(null==e&&(e=[]),Array.isArray(t)||q(t)&&!n)for(let s=0;s<t.length;++s)P(t[s],e,n);else e.push(t);return e}function W(t){if(0===t.length)return 1;let e=t[0];for(let n=1;n<t.length;n++)e*=t[n];return e}function U(t,e){if(t===e)return!0;if(null==t||null==e)return!1;if(t.length!==e.length)return!1;for(let n=0;n<t.length;n++)if(t[n]!==e[n])return!1;return!0}function j(t){return t%1==0}function V(t,e){return e<=t.length?t:t+" ".repeat(e-t.length)}function K(t,e){const n=e.length;return B((t=null==t?e.map(((t,e)=>e)):[].concat(t)).every((t=>t>=-n&&t<n)),(()=>`All values in axis param must be in range [-${n}, ${n}) but got axis ${t}`)),B(t.every((t=>j(t))),(()=>`All values in axis param must be integers but got axis ${t}`)),t.map((t=>t<0?n+t:t))}function q(t){return t instanceof Float32Array||t instanceof Int32Array||t instanceof Uint8Array}function G(t){if("float32"===t||"int32"===t)return 4;if("complex64"===t)return 8;if("bool"===t)return 1;throw new Error(`Unknown dtype ${t}`)}function H(t){return"string"==typeof t||t instanceof String}function J(t){return Array.isArray(t)?J(t[0]):t instanceof Float32Array?"float32":t instanceof Int32Array||t instanceof Uint8Array?"int32":"number"==typeof t?"float32":H(t)?"string":function(t){return"boolean"==typeof t}(t)?"bool":"float32"}function Z(t){return!!(t&&t.constructor&&t.call&&t.apply)}function Y(t){const e=t.length;if(e<2)return[];const n=new Array(e-1);n[e-2]=t[e-1];for(let s=e-3;s>=0;--s)n[s]=n[s+1]*t[s+1];return n}function X(t,e,n,s=!1){const i=new Array;if(1===e.length){const r=e[0]*(s?2:1);for(let e=0;e<r;e++)i[e]=n[t+e]}else{const r=e[0],a=e.slice(1),o=a.reduce(((t,e)=>t*e))*(s?2:1);for(let e=0;e<r;e++)i[e]=X(t+e*o,a,n,s)}return i}function Q(t,e,n=!1){if(0===t.length)return e[0];const s=t.reduce(((t,e)=>t*e))*(n?2:1);if(0===s)return[];if(s!==e.length)throw new Error(`[${t}] does not match the input size ${e.length}${n?" for a complex tensor":""}.`);return X(0,t,e,n)}function tt(t,e){const n=et(t,e);for(let t=0;t<n.length;t++)n[t]=1;return n}function et(t,e){if(null==e||"float32"===e||"complex64"===e)return new Float32Array(t);if("int32"===e)return new Int32Array(t);if("bool"===e)return new Uint8Array(t);throw new Error(`Unknown data type ${e}`)}function nt(t){return t&&t.then&&"function"==typeof t.then}function st(...t){ot().getBool("IS_TEST")||ot().getBool("PROD")||console.warn(...t)}const it="tfjsflags";class rt{constructor(t){this.global=t,this.flags={},this.flagRegistry={},this.urlFlags={},this.getQueryParams=at,this.populateURLFlags()}setPlatform(t,e){null!=this.platform&&st(`Platform ${this.platformName} has already been set. Overwriting the platform with ${e}.`),this.platformName=t,this.platform=e}registerFlag(t,e,n){if(this.flagRegistry[t]={evaluationFn:e,setHook:n},null!=this.urlFlags[t]){const e=this.urlFlags[t];st(`Setting feature override from URL ${t}: ${e}.`),this.set(t,e)}}async getAsync(t){return t in this.flags||(this.flags[t]=await this.evaluateFlag(t)),this.flags[t]}get(t){if(t in this.flags)return this.flags[t];const e=this.evaluateFlag(t);if(nt(e))throw new Error(`Flag ${t} cannot be synchronously evaluated. Please use getAsync() instead.`);return this.flags[t]=e,this.flags[t]}getNumber(t){return this.get(t)}getBool(t){return this.get(t)}getFlags(){return this.flags}get features(){return this.flags}set(t,e){if(null==this.flagRegistry[t])throw new Error(`Cannot set flag ${t} as it has not been registered.`);this.flags[t]=e,null!=this.flagRegistry[t].setHook&&this.flagRegistry[t].setHook(e)}evaluateFlag(t){if(null==this.flagRegistry[t])throw new Error(`Cannot evaluate flag '${t}': no evaluation function found.`);return this.flagRegistry[t].evaluationFn()}setFlags(t){this.flags=Object.assign({},t)}reset(){this.flags={},this.urlFlags={},this.populateURLFlags()}populateURLFlags(){if(void 0===this.global||void 0===this.global.location||void 0===this.global.location.search)return;const t=this.getQueryParams(this.global.location.search);if(it in t){t.tfjsflags.split(",").forEach((t=>{const[e,n]=t.split(":");this.urlFlags[e]=function(t,e){if("true"===(e=e.toLowerCase())||"false"===e)return"true"===e;if(""+ +e===e)return+e;throw new Error(`Could not parse value flag value ${e} for flag ${t}.`)}(e,n)}))}}}function at(t){const e={};return t.replace(/[?&]([^=?&]+)(?:=([^&]*))?/g,((t,...n)=>(function(t,e,n){t[decodeURIComponent(e)]=decodeURIComponent(n||"")}(e,n[0],n[1]),n.join("=")))),e}function ot(){return ut}let lt,ut=null;function ht(){if(null==lt){let t;if("undefined"!=typeof window)t=window;else if("undefined"!=typeof global)t=global;else if("undefined"!=typeof process)t=process;else{if("undefined"==typeof self)throw new Error("Could not find a global object");t=self}lt=t}return lt}function ct(t,e){const n=function(){const t=ht();return null==t._tfGlobals&&(t._tfGlobals=new Map),t._tfGlobals}();if(n.has(t))return n.get(t);{const s=e();return n.set(t,s),n.get(t)}}const pt=ct("kernelRegistry",(()=>new Map)),dt=ct("gradRegistry",(()=>new Map));function ft(t,e){const n=function(t,e){return`${e}_${t}`}(t,e);return pt.get(n)}function gt(t){return dt.get(t)}function mt(t){const e=pt.entries(),n=[];for(;;){const{done:s,value:i}=e.next();if(s)break;const[r,a]=i,[o]=r.split("_");o===t&&n.push(a)}return n}function yt(t){const{kernelName:e}=t;dt.has(e)&&ot().getBool("DEBUG")&&st(`Overriding the gradient for '${e}'`),dt.set(e,t)}function bt(t,e){if("string"===e)throw new Error("Cannot convert a string[] to a TypedArray");if(Array.isArray(t)&&(t=P(t)),ot().getBool("DEBUG")&&function(t,e){for(let n=0;n<t.length;n++){const s=t[n];if(isNaN(s)||!isFinite(s))throw Error(`A tensor of type ${e} being uploaded contains ${s}.`)}}(t,e),function(t,e){return t instanceof Float32Array&&"float32"===e||t instanceof Int32Array&&"int32"===e||t instanceof Uint8Array&&"bool"===e}(t,e))return t;if(null==e||"float32"===e||"complex64"===e)return new Float32Array(t);if("int32"===e)return new Int32Array(t);if("bool"===e){const e=new Uint8Array(t.length);for(let n=0;n<e.length;++n)0!==Math.round(t[n])&&(e[n]=1);return e}throw new Error(`Unknown data type ${e}`)}function wt(){return ot().platform.now()}function kt(t,e="utf-8"){return e=e||"utf-8",ot().platform.decode(t,e)}class vt{constructor(t,e){this.backendTimer=t,this.logger=e,null==e&&(this.logger=new xt)}profileKernel(t,e,n){let s;const i=()=>{s=n()};let r;const a=wt();if(this.backendTimer.timerAvailable())r=this.backendTimer.time(i);else{i();for(const t of s)t.dataSync();r=Promise.resolve({kernelMs:wt()-a})}if(ot().getBool("CHECK_COMPUTATION_FOR_ERRORS"))for(let e=0;e<s.length;e++){const n=s[e];n.data().then((e=>{St(e,n.dtype,t)}))}return{kernelName:t,outputs:s,inputs:e,timeMs:r.then((t=>t.kernelMs)),extraInfo:r.then((t=>null!=t.getExtraProfileInfo?t.getExtraProfileInfo():""))}}logKernelProfile(t){const{kernelName:e,outputs:n,timeMs:s,inputs:i,extraInfo:r}=t;n.forEach((t=>{Promise.all([t.data(),s,r]).then((n=>{this.logger.logKernelProfile(e,t,n[0],n[1],i,n[2])}))}))}}function St(t,e,n){if("float32"!==e)return!1;for(let e=0;e<t.length;e++){const s=t[e];if(isNaN(s)||!isFinite(s))return console.warn(`Found ${s} in the result of '${n}'`),!0}return!1}class xt{logKernelProfile(t,e,n,s,i,r){const a="number"==typeof s?V(`${s}ms`,9):s.error,o=V(t,25),l=e.rank,u=e.size,h=V(e.shape.toString(),14);let c="";for(const t in i){const n=i[t];if(null!=n){const s=n.shape||e.shape,i=s.length;c+=`${t}: ${i}D ${i>0?s:""} `}}console.log(`%c${o}\t%c${a}\t%c${l}D ${h}\t%c${u}\t%c${c}\t%c${r}`,"font-weight:bold","color:red","color:blue","color: orange","color: green","color: steelblue")}}function Nt(t,e,n,s){const i=Y(e),r=function(t,e,n,s){const i=W(e),r=s[s.length-1],a=new Array(r).fill(0),o=e.length,l="complex64"===n?Ct(t):t;if(o>1)for(let t=0;t<i/r;t++){const e=t*r;for(let t=0;t<r;t++)a[t]=Math.max(a[t],zt(l[e+t],0,n).length)}return a}(t,e,n,i),a=e.length,o=At(t,e,n,i,r),l=["Tensor"];return s&&(l.push(`  dtype: ${n}`),l.push(`  rank: ${a}`),l.push(`  shape: [${e}]`),l.push("  values:")),l.push(o.map((t=>"    "+t)).join("\n")),l.join("\n")}function zt(t,e,n){let s;return s=Array.isArray(t)?`${parseFloat(t[0].toFixed(7))} + ${parseFloat(t[1].toFixed(7))}j`:H(t)?`'${t}'`:"bool"===n?It(t):parseFloat(t.toFixed(7)).toString(),V(s,e)}function It(t){return 0===t?"false":"true"}function At(t,e,n,s,i,r=!0){const a="complex64"===n?2:1,o=e[0],l=e.length;if(0===l){if("complex64"===n){return[zt(Ct(t)[0],0,n)]}return"bool"===n?[It(t[0])]:[t[0].toString()]}if(1===l){if(o>20){const e=3*a;let s=Array.from(t.slice(0,e)),r=Array.from(t.slice((o-3)*a,o*a));return"complex64"===n&&(s=Ct(s),r=Ct(r)),["["+s.map(((t,e)=>zt(t,i[e],n))).join(", ")+", ..., "+r.map(((t,e)=>zt(t,i[o-3+e],n))).join(", ")+"]"]}return["["+("complex64"===n?Ct(t):Array.from(t)).map(((t,e)=>zt(t,i[e],n))).join(", ")+"]"]}const u=e.slice(1),h=s.slice(1),c=s[0]*a,p=[];if(o>20){for(let e=0;e<3;e++){const s=e*c,r=s+c;p.push(...At(t.slice(s,r),u,n,h,i,!1))}p.push("...");for(let e=o-3;e<o;e++){const s=e*c,r=s+c;p.push(...At(t.slice(s,r),u,n,h,i,e===o-1))}}else for(let e=0;e<o;e++){const s=e*c,r=s+c;p.push(...At(t.slice(s,r),u,n,h,i,e===o-1))}const d=2===l?",":"";p[0]="["+p[0]+d;for(let t=1;t<p.length-1;t++)p[t]=" "+p[t]+d;let f=",\n";for(let t=2;t<l;t++)f+="\n";return p[p.length-1]=" "+p[p.length-1]+"]"+(r?"":f),p}function Ct(t){const e=[];for(let n=0;n<t.length;n+=2)e.push([t[n],t[n+1]]);return e}let Tt=null,$t=null;class Et{constructor(t,e,n,s){this.kept=!1,this.isDisposedInternal=!1,this.shape=t.slice(),this.dtype=e||"float32",this.size=W(t),this.strides=Y(t),this.dataId=n,this.id=s,this.rankType=this.rank<5?this.rank.toString():"higher"}get rank(){return this.shape.length}async buffer(){const t=await this.data();return $t.buffer(this.shape,this.dtype,t)}bufferSync(){return $t.buffer(this.shape,this.dtype,this.dataSync())}async array(){const t=await this.data();return Q(this.shape,t,"complex64"===this.dtype)}arraySync(){return Q(this.shape,this.dataSync(),"complex64"===this.dtype)}async data(){this.throwIfDisposed();const t=Tt().read(this.dataId);if("string"===this.dtype){const e=await t;try{return e.map((t=>kt(t)))}catch(t){throw new Error("Failed to decode the string bytes into utf-8. To get the original bytes, call tensor.bytes().")}}return t}dataSync(){this.throwIfDisposed();const t=Tt().readSync(this.dataId);if("string"===this.dtype)try{return t.map((t=>kt(t)))}catch(t){throw new Error("Failed to decode the string bytes into utf-8. To get the original bytes, call tensor.bytes().")}return t}async bytes(){this.throwIfDisposed();const t=await Tt().read(this.dataId);return"string"===this.dtype?t:new Uint8Array(t.buffer)}dispose(){this.isDisposed||(Tt().disposeTensor(this),this.isDisposedInternal=!0)}get isDisposed(){return this.isDisposedInternal}throwIfDisposed(){if(this.isDisposed)throw new Error("Tensor is disposed.")}print(t=!1){return $t.print(this,t)}clone(){return this.throwIfDisposed(),$t.clone(this)}toString(t=!1){return Nt(this.dataSync(),this.shape,this.dtype,t)}cast(t){return this.throwIfDisposed(),$t.cast(this,t)}variable(t=!0,e,n){return this.throwIfDisposed(),Tt().makeVariable(this,t,e,n)}}Object.defineProperty(Et,Symbol.hasInstance,{value:t=>!!t&&null!=t.data&&null!=t.dataSync&&null!=t.throwIfDisposed}),ct("Tensor",(()=>Et));class Ft extends Et{constructor(t,e,n,s){super(t.shape,t.dtype,t.dataId,s),this.trainable=e,this.name=n}assign(t){if(t.dtype!==this.dtype)throw new Error(`dtype of the new value (${t.dtype}) and previous value (${this.dtype}) must match`);if(!U(t.shape,this.shape))throw new Error(`shape of the new value (${t.shape}) and previous value (${this.shape}) must match`);Tt().disposeTensor(this),this.dataId=t.dataId,Tt().incRef(this,null)}dispose(){Tt().disposeVariable(this),this.isDisposedInternal=!0}}var Dt,Lt,_t,Rt,Mt;Object.defineProperty(Ft,Symbol.hasInstance,{value:t=>t instanceof Et&&null!=t.assign&&t.assign instanceof Function}),function(t){t.R0="R0",t.R1="R1",t.R2="R2",t.R3="R3",t.R4="R4",t.R5="R5",t.R6="R6"}(Dt||(Dt={})),function(t){t.float32="float32",t.int32="int32",t.bool="int32",t.complex64="complex64"}(Lt||(Lt={})),function(t){t.float32="float32",t.int32="int32",t.bool="bool",t.complex64="complex64"}(_t||(_t={})),function(t){t.float32="float32",t.int32="float32",t.bool="float32",t.complex64="complex64"}(Rt||(Rt={})),function(t){t.float32="complex64",t.int32="complex64",t.bool="complex64",t.complex64="complex64"}(Mt||(Mt={}));const Ot={float32:Rt,int32:Lt,bool:_t,complex64:Mt};function Bt(t,e){if(t.dtype===e.dtype)return[t,e];const n=function(t,e){if("string"===t||"string"===e){if("string"===t&&"string"===e)return"string";throw new Error(`Can not upcast ${t} with ${e}`)}return Ot[t][e]}(t.dtype,e.dtype);return[t.cast(n),e.cast(n)]}function Pt(t){const e=[];return Wt(t,e,new Set),e}function Wt(t,e,n){if(null==t)return;if(t instanceof Et)return void e.push(t);if(s=t,!Array.isArray(s)&&"object"!=typeof s)return;var s;const i=t;for(const t in i){const s=i[t];n.has(s)||(n.add(s),Wt(s,e,n))}}function Ut(t){return null!=t.kernelName}class jt{constructor(){this.registeredVariables={},this.nextTapeNodeId=0,this.numBytes=0,this.numTensors=0,this.numStringTensors=0,this.numDataBuffers=0,this.gradientDepth=0,this.kernelDepth=0,this.scopeStack=[],this.numDataMovesStack=[],this.nextScopeId=0,this.tensorInfo=new WeakMap,this.profiling=!1,this.activeProfile={newBytes:0,newTensors:0,peakBytes:0,kernels:[],result:null,get kernelNames(){return Array.from(new Set(this.kernels.map((t=>t.name))))}}}dispose(){for(const t in this.registeredVariables)this.registeredVariables[t].dispose()}}class Vt{constructor(t){this.ENV=t,this.registry={},this.registryFactory={},this.pendingBackendInitId=0,this.state=new jt}async ready(){if(null!=this.pendingBackendInit)return this.pendingBackendInit.then((()=>{}));if(null!=this.backendInstance)return;const t=this.getSortedBackends();for(let e=0;e<t.length;e++){const n=t[e];if(await this.initializeBackend(n).success)return void await this.setBackend(n)}throw new Error("Could not initialize any backends, all backend initializations failed.")}get backend(){if(null!=this.pendingBackendInit)throw new Error(`Backend '${this.backendName}' has not yet been initialized. Make sure to await tf.ready() or await tf.setBackend() before calling other methods`);if(null==this.backendInstance){const{name:t,asyncInit:e}=this.initializeBackendsAndReturnBest();if(e)throw new Error(`The highest priority backend '${t}' has not yet been initialized. Make sure to await tf.ready() or await tf.setBackend() before calling other methods`);this.setBackend(t)}return this.backendInstance}backendNames(){return Object.keys(this.registryFactory)}findBackend(t){if(!(t in this.registry)){if(!(t in this.registryFactory))return null;{const{asyncInit:e}=this.initializeBackend(t);if(e)return null}}return this.registry[t]}findBackendFactory(t){return t in this.registryFactory?this.registryFactory[t].factory:null}registerBackend(t,e,n=1){return t in this.registryFactory?(st(`${t} backend was already registered. Reusing existing backend factory.`),!1):(this.registryFactory[t]={factory:e,priority:n},!0)}async setBackend(t){if(null==this.registryFactory[t])throw new Error(`Backend name '${t}' not found in registry`);if(this.backendName=t,null==this.registry[t]){this.backendInstance=null;const{success:e,asyncInit:n}=this.initializeBackend(t);if(!(n?await e:e))return!1}return this.backendInstance=this.registry[t],this.setupRegisteredKernels(),this.profiler=new vt(this.backendInstance),!0}setupRegisteredKernels(){mt(this.backendName).forEach((t=>{null!=t.setupFunc&&t.setupFunc(this.backendInstance)}))}disposeRegisteredKernels(t){mt(t).forEach((e=>{null!=e.disposeFunc&&e.disposeFunc(this.registry[t])}))}initializeBackend(t){const e=this.registryFactory[t];if(null==e)throw new Error(`Cannot initialize backend ${t}, no registration found.`);try{const n=e.factory();if(!n||n instanceof class{refCount(t){return O("refCount")}incRef(t){return O("incRef")}timerAvailable(){return!0}time(t){return O("time")}read(t){return O("read")}readSync(t){return O("readSync")}numDataIds(){return O("numDataIds")}disposeData(t,e){return O("disposeData")}write(t,e,n){return O("write")}move(t,e,n,s,i){return O("move")}memory(){return O("memory")}floatPrecision(){return O("floatPrecision")}epsilon(){return 32===this.floatPrecision()?1e-7:1e-4}dispose(){return O("dispose")}}||"function"!=typeof n.then)return this.registry[t]=n,{success:!0,asyncInit:!1};{const e=++this.pendingBackendInitId,s=n.then((n=>!(e<this.pendingBackendInitId)&&(this.registry[t]=n,this.pendingBackendInit=null,!0))).catch((n=>(e<this.pendingBackendInitId||(this.pendingBackendInit=null,st(`Initialization of backend ${t} failed`),st(n.stack||n.message)),!1)));return this.pendingBackendInit=s,{success:s,asyncInit:!0}}}catch(e){return st(`Initialization of backend ${t} failed`),st(e.stack||e.message),{success:!1,asyncInit:!1}}}removeBackend(t){if(!(t in this.registryFactory))throw new Error(`${t} backend not found in registry`);this.backendName===t&&null!=this.pendingBackendInit&&this.pendingBackendInitId++,t in this.registry&&(this.disposeRegisteredKernels(t),this.registry[t].dispose(),delete this.registry[t]),delete this.registryFactory[t],this.backendName===t&&(this.pendingBackendInit=null,this.backendName=null,this.backendInstance=null)}getSortedBackends(){if(0===Object.keys(this.registryFactory).length)throw new Error("No backend found in registry.");return Object.keys(this.registryFactory).sort(((t,e)=>this.registryFactory[e].priority-this.registryFactory[t].priority))}initializeBackendsAndReturnBest(){const t=this.getSortedBackends();for(let e=0;e<t.length;e++){const n=t[e],{success:s,asyncInit:i}=this.initializeBackend(n);if(i||s)return{name:n,asyncInit:i}}throw new Error("Could not initialize any backends, all backend initializations failed.")}moveData(t,e){const n=this.state.tensorInfo.get(e),s=n.backend,i=this.readSync(e),r=s.refCount(e);s.disposeData(e,!0),n.backend=t,t.move(e,i,n.shape,n.dtype,r),this.shouldCheckForMemLeaks()&&this.state.numDataMovesStack[this.state.numDataMovesStack.length-1]++}tidy(t,e){let n,s=null;if(null==e){if("function"!=typeof t)throw new Error("Please provide a function to tidy()");e=t}else{if("string"!=typeof t&&!(t instanceof String))throw new Error("When calling with two arguments, the first argument to tidy() must be a string");if("function"!=typeof e)throw new Error("When calling with two arguments, the 2nd argument to tidy() must be a function");s=t}return this.scopedRun((()=>this.startScope(s)),(()=>this.endScope(n)),(()=>(n=e(),n instanceof Promise&&console.error("Cannot return a Promise inside of tidy."),n)))}scopedRun(t,e,n){t();try{const t=n();return e(),t}catch(t){throw e(),t}}nextTensorId(){return Vt.nextTensorId++}nextVariableId(){return Vt.nextVariableId++}clone(t){const e=Kt.runKernel(y,{x:t}),n={x:t};return this.addTapeNode(this.state.activeScope.name,n,[e],(t=>({x:()=>{const e={x:t},n={dtype:"float32"};return Kt.runKernel(r,e,n)}})),[],{}),e}runKernel(t,e,n){null==this.backendName&&this.backend;if(!(null!=ft(t,this.backendName)))throw new Error(`Kernel '${t}' not registered for backend '${this.backendName}'`);return this.runKernelFunc({kernelName:t,inputs:e,attrs:n})}shouldCheckForMemLeaks(){return this.ENV.getBool("IS_TEST")}checkKernelForMemLeak(t,e,n){const s=this.backend.numDataIds();let i=0;n.forEach((t=>{i+="complex64"===t.dtype?3:1}));const r=this.state.numDataMovesStack[this.state.numDataMovesStack.length-1],a=s-e-i-r;if(a>0)throw new Error(`Backend '${this.backendName}' has an internal memory leak (${a} data ids) after running '${t}'`)}runKernelFunc(t){let e,n=[];const s=this.isTapeOn(),i=this.state.numBytes,r=this.state.numTensors;let a,o;this.shouldCheckForMemLeaks()&&this.state.numDataMovesStack.push(0),null==this.backendName&&this.backend;const l=Ut(t)?t.kernelName:null!=this.state.activeScope?this.state.activeScope.name:"";if(Ut(t)){const{kernelName:e,inputs:i,attrs:r}=t;null==this.backendName&&this.backend;const l=ft(e,this.backendName);B(null!=l,(()=>`Cannot find registered kernel '${e}' for backend '${this.backendName}'`)),a=()=>{const t=this.backend.numDataIds();o=l.kernelFunc({inputs:i,attrs:r,backend:this.backend});const a=Array.isArray(o)?o:[o];this.shouldCheckForMemLeaks()&&this.checkKernelForMemLeak(e,t,a);const u=a.map((t=>{if(null!=t.rank)return t;const{dataId:e,shape:n,dtype:s}=t;return this.makeTensorFromDataId(e,n,s)}));if(s){const t=this.getTensorsForGradient(e,i,u);n=this.saveTensorsForBackwardMode(t)}return u}}else{const{forwardFunc:e}=t,i=t=>{s&&(n=t.map((t=>this.keep(this.clone(t)))))};a=()=>{const t=this.backend.numDataIds();o=this.tidy((()=>e(this.backend,i)));const n=Array.isArray(o)?o:[o];return this.shouldCheckForMemLeaks()&&this.checkKernelForMemLeak(l,t,n),n}}const{inputs:u,attrs:h}=t,c=Ut(t)?null:t.backwardsFunc;let p;return this.scopedRun((()=>this.state.kernelDepth++),(()=>this.state.kernelDepth--),(()=>{this.ENV.getBool("DEBUG")||this.state.profiling?(p=this.profiler.profileKernel(l,u,(()=>a())),this.ENV.getBool("DEBUG")&&this.profiler.logKernelProfile(p),e=p.outputs):e=a()})),s&&this.addTapeNode(l,u,e,c,n,h),this.state.profiling&&this.state.activeProfile.kernels.push({name:l,bytesAdded:this.state.numBytes-i,totalBytesSnapshot:this.state.numBytes,tensorsAdded:this.state.numTensors-r,totalTensorsSnapshot:this.state.numTensors,inputShapes:Object.keys(u).map((t=>null!=u[t]?u[t].shape:null)),outputShapes:e.map((t=>t.shape)),kernelTimeMs:p.timeMs,extraInfo:p.extraInfo}),Array.isArray(o)?e:e[0]}saveTensorsForBackwardMode(t){return t.map((t=>this.keep(this.clone(t))))}getTensorsForGradient(t,e,n){const s=gt(t);if(null!=s){const t=s.inputsToSave||[],i=s.outputsToSave||[];let r;s.saveAllInputs?(B(Array.isArray(e),(()=>"saveAllInputs is true, expected inputs to be an array.")),r=Object.keys(e).map((t=>e[t]))):r=t.map((t=>e[t]));const a=n.filter(((t,e)=>i[e]));return r.concat(a)}return[]}makeTensor(t,e,n,s){if(null==t)throw new Error("Values passed to engine.makeTensor() are null");n=n||"float32",s=s||this.backend;let i=t;"string"===n&&H(t[0])&&(i=t.map((t=>function(t,e="utf-8"){return e=e||"utf-8",ot().platform.encode(t,e)}(t))));const r=s.write(i,e,n),a=new Et(e,n,r,this.nextTensorId());if(this.trackTensor(a,s),"string"===n){const t=this.state.tensorInfo.get(r),e=function(t){if(null==t)return 0;let e=0;return t.forEach((t=>e+=t.length)),e}(i);this.state.numBytes+=e-t.bytes,t.bytes=e}return a}makeTensorFromDataId(t,e,n,s){const i=new Et(e,n=n||"float32",t,this.nextTensorId());return this.trackTensor(i,s),i}makeVariable(t,e=!0,n,s){n=n||this.nextVariableId().toString(),null!=s&&s!==t.dtype&&(t=t.cast(s));const i=new Ft(t,e,n,this.nextTensorId());if(null!=this.state.registeredVariables[i.name])throw new Error(`Variable with name ${i.name} was already registered`);return this.state.registeredVariables[i.name]=i,this.incRef(i,this.backend),i}trackTensor(t,e){this.state.numTensors++,"string"===t.dtype&&this.state.numStringTensors++;let n=0;"complex64"!==t.dtype&&"string"!==t.dtype&&(n=t.size*G(t.dtype)),this.state.numBytes+=n,this.state.tensorInfo.has(t.dataId)||(this.state.numDataBuffers++,this.state.tensorInfo.set(t.dataId,{backend:e||this.backend,dtype:t.dtype,shape:t.shape,bytes:n})),t instanceof Ft||this.track(t)}incRef(t,e){this.trackTensor(t,e),this.backend.incRef(t.dataId)}removeDataId(t,e){this.state.tensorInfo.has(t)&&this.state.tensorInfo.get(t).backend===e&&(this.state.tensorInfo.delete(t),this.state.numDataBuffers--)}disposeTensor(t){if(!this.state.tensorInfo.has(t.dataId))return;const e=this.state.tensorInfo.get(t.dataId);if(this.state.numTensors--,"string"===t.dtype&&(this.state.numStringTensors--,this.state.numBytes-=e.bytes),"complex64"!==t.dtype&&"string"!==t.dtype){const e=t.size*G(t.dtype);this.state.numBytes-=e}e.backend.disposeData(t.dataId)&&this.removeDataId(t.dataId,e.backend)}disposeVariables(){for(const t in this.state.registeredVariables){const e=this.state.registeredVariables[t];this.disposeVariable(e)}}disposeVariable(t){this.disposeTensor(t),null!=this.state.registeredVariables[t.name]&&delete this.state.registeredVariables[t.name]}memory(){const t=this.backend.memory();return t.numTensors=this.state.numTensors,t.numDataBuffers=this.state.numDataBuffers,t.numBytes=this.state.numBytes,this.state.numStringTensors>0&&(t.unreliable=!0,null==t.reasons&&(t.reasons=[]),t.reasons.push("Memory usage by string tensors is approximate (2 bytes per character)")),t}async profile(t){this.state.profiling=!0;const e=this.state.numBytes,n=this.state.numTensors;this.state.activeProfile.kernels=[],this.state.activeProfile.result=await t(),this.state.profiling=!1,this.state.activeProfile.peakBytes=Math.max(...this.state.activeProfile.kernels.map((t=>t.totalBytesSnapshot))),this.state.activeProfile.newBytes=this.state.numBytes-e,this.state.activeProfile.newTensors=this.state.numTensors-n;for(const t of this.state.activeProfile.kernels)t.kernelTimeMs=await t.kernelTimeMs,t.extraInfo=await t.extraInfo;return this.state.activeProfile}isTapeOn(){return this.state.gradientDepth>0&&0===this.state.kernelDepth}addTapeNode(t,e,n,s,i,r){const a={id:this.state.nextTapeNodeId++,kernelName:t,inputs:e,outputs:n,saved:i},o=gt(t);null!=o&&(s=o.gradFunc),null!=s&&(a.gradient=t=>(t=t.map(((t,e)=>{if(null==t){const t=n[e],s=et(t.size,t.dtype);return this.makeTensor(s,t.shape,t.dtype)}return t})),s(t.length>1?t:t[0],i,r))),this.state.activeTape.push(a)}keep(t){return t.kept=!0,t}startTape(){0===this.state.gradientDepth&&(this.state.activeTape=[]),this.state.gradientDepth++}endTape(){this.state.gradientDepth--}startScope(t){const e={track:[],name:"unnamed scope",id:this.state.nextScopeId++};t&&(e.name=t),this.state.scopeStack.push(e),this.state.activeScope=e}endScope(t){const e=Pt(t),n=new Set(e.map((t=>t.id)));for(let t=0;t<this.state.activeScope.track.length;t++){const e=this.state.activeScope.track[t];e.kept||n.has(e.id)||e.dispose()}const s=this.state.scopeStack.pop();this.state.activeScope=0===this.state.scopeStack.length?null:this.state.scopeStack[this.state.scopeStack.length-1],e.forEach((t=>{t.kept||t.scopeId!==s.id||this.track(t)}))}gradients(t,e,n,s=!1){if(B(e.length>0,(()=>"gradients() received an empty list of xs.")),null!=n&&"float32"!==n.dtype)throw new Error(`dy must have 'float32' dtype, but has '${n.dtype}'`);const i=this.scopedRun((()=>this.startTape()),(()=>this.endTape()),(()=>this.tidy("forward",t)));B(i instanceof Et,(()=>"The result y returned by f() must be a tensor."));const r=function(t,e,n){const s={},i={};for(let t=0;t<e.length;t++)s[e[t].id]=!0;for(let n=0;n<t.length;n++){const r=t[n],a=r.inputs;for(const t in a){const n=a[t];let o=!1;for(let t=0;t<e.length;t++)if(s[n.id]){r.outputs.forEach((t=>s[t.id]=!0)),o=!0,i[r.id]=!0;break}if(o)break}}const r={};r[n.id]=!0;const a={};for(let e=t.length-1;e>=0;e--){const n=t[e],s=n.inputs;for(let t=0;t<n.outputs.length;t++)if(r[n.outputs[t].id]){for(const t in s)r[s[t].id]=!0,a[n.id]=!0;break}}const o=[];for(let e=0;e<t.length;e++){const n=t[e];if(i[n.id]&&a[n.id]){const t={};for(const e in n.inputs){const i=n.inputs[e];s[i.id]&&(t[e]=i)}const e=Object.assign({},n);e.inputs=t,e.outputs=n.outputs,o.push(e)}}return o}(this.state.activeTape,e,i);if(!s&&0===r.length&&e.length>0)throw new Error("Cannot compute gradient of y=f(x) with respect to x. Make sure that the f you passed encloses all operations that lead from x to y.");return this.tidy("backward",(()=>{const t={};t[i.id]=null==n?function(t){const e=tt(W(t),"float32");return Kt.makeTensor(e,t,"float32")}(i.shape):n,function(t,e,n,s){for(let i=e.length-1;i>=0;i--){const r=e[i],a=[];if(r.outputs.forEach((e=>{const n=t[e.id];null!=n?a.push(n):a.push(null)})),null==r.gradient)throw new Error(`Cannot compute gradient: gradient function not found for ${r.kernelName}.`);const o=r.gradient(a);for(const e in r.inputs){if(!(e in o))throw new Error(`Cannot backprop through input ${e}. Available gradients found: ${Object.keys(o)}.`);const i=n((()=>o[e]()));if("float32"!==i.dtype)throw new Error(`Error in gradient for op ${r.kernelName}. The gradient of input ${e} must have 'float32' dtype, but has '${i.dtype}'`);const a=r.inputs[e];if(!U(i.shape,a.shape))throw new Error(`Error in gradient for op ${r.kernelName}. The gradient of input '${e}' has shape '${i.shape}', which does not match the shape of the input '${a.shape}'`);if(null==t[a.id])t[a.id]=i;else{const e=t[a.id];t[a.id]=s(e,i),e.dispose()}}}}(t,r,(t=>this.tidy(t)),qt);const s=e.map((e=>t[e.id]));return 0===this.state.gradientDepth&&(this.state.activeTape.forEach((t=>{for(const e of t.saved)e.dispose()})),this.state.activeTape=null),{value:i,grads:s}}))}customGrad(t){return B(Z(t),(()=>"The f passed in customGrad(f) must be a function.")),(...e)=>{let n;B(e.every((t=>t instanceof Et)),(()=>"The args passed in customGrad(f)(x1, x2,...) must all be tensors"));const s={};e.forEach(((t,e)=>{s[e]=t}));return this.runKernelFunc({forwardFunc:(s,i)=>(n=t(...e,i),B(n.value instanceof Et,(()=>"The function f passed in customGrad(f) must return an object where `obj.value` is a tensor")),B(Z(n.gradFunc),(()=>"The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function.")),n.value),backwardsFunc:(t,s)=>{const i=n.gradFunc(t,s),r=Array.isArray(i)?i:[i];B(r.length===e.length,(()=>"The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function that returns the same number of tensors as inputs passed to f(...).")),B(r.every((t=>t instanceof Et)),(()=>"The function f passed in customGrad(f) must return an object where `obj.gradFunc` is a function that returns a list of only tensors."));const a={};return r.forEach(((t,e)=>{a[e]=()=>t})),a},inputs:s})}}readSync(t){return this.state.tensorInfo.get(t).backend.readSync(t)}read(t){return this.state.tensorInfo.get(t).backend.read(t)}async time(t){const e=wt(),n=await this.backend.time(t);return n.wallMs=wt()-e,n}track(t){return null!=this.state.activeScope&&(t.scopeId=this.state.activeScope.id,this.state.activeScope.track.push(t)),t}get registeredVariables(){return this.state.registeredVariables}reset(){this.pendingBackendInitId++,this.state.dispose(),this.ENV.reset(),this.state=new jt;for(const t in this.registry)this.disposeRegisteredKernels(t),this.registry[t].dispose(),delete this.registry[t];this.backendName=null,this.backendInstance=null,this.pendingBackendInit=null}}Vt.nextTensorId=0,Vt.nextVariableId=0;const Kt=function(){const t=ht();if(null==t._tfengine){const e=new rt(t);t._tfengine=new Vt(e)}var e;return e=t._tfengine.ENV,ut=e,Tt=()=>t._tfengine,t._tfengine}();function qt(t,e){const s={a:t,b:e};return Kt.runKernel(n,s)}function Gt(t,e){let n=t;if(q(t))return"string"===e?[]:[t.length];if(!Array.isArray(t))return[];const s=[];for(;Array.isArray(n)||q(n)&&"string"!==e;)s.push(n.length),n=n[0];return Array.isArray(t)&&ot().getBool("TENSORLIKE_CHECK_SHAPE_CONSISTENCY")&&Ht(t,s,[]),s}function Ht(t,e,n){if(n=n||[],!Array.isArray(t)&&!q(t))return void B(0===e.length,(()=>`Element arr[${n.join("][")}] is a primitive, but should be an array/TypedArray of ${e[0]} elements`));B(e.length>0,(()=>`Element arr[${n.join("][")}] should be a primitive, but is an array of ${t.length} elements`)),B(t.length===e[0],(()=>`Element arr[${n.join("][")}] should have ${e[0]} elements, but has ${t.length} elements`));const s=e.slice(1);for(let e=0;e<t.length;++e)Ht(t[e],s,n.concat(e))}function Jt(t,e,n,s){if("string_or_numeric"!==t){if(null==t)throw new Error("Expected dtype cannot be null.");if("numeric"!==t&&t!==e||"numeric"===t&&"string"===e)throw new Error(`Argument '${n}' passed to '${s}' must be ${t} tensor, but got ${e} tensor`)}}function Zt(t,e,n,s="numeric"){if(t instanceof Et)return Jt(s,t.dtype,e,n),t;let i=J(t);if("string"!==i&&["bool","int32","float32"].indexOf(s)>=0&&(i=s),Jt(s,i,e,n),null==t||!q(t)&&!Array.isArray(t)&&"number"!=typeof t&&"boolean"!=typeof t&&"string"!=typeof t){const s=null==t?"null":t.constructor.name;throw new Error(`Argument '${e}' passed to '${n}' must be a Tensor or TensorLike, but got '${s}'`)}const r=Gt(t,i);q(t)||Array.isArray(t)||(t=[t]);const a="string"!==i?bt(t,i):P(t,[],!0);return Kt.makeTensor(a,r,i)}function Yt(t,e,n,s="numeric"){if(!Array.isArray(t))throw new Error(`Argument ${e} passed to ${n} must be a \`Tensor[]\` or \`TensorLike[]\``);return t.map(((t,i)=>Zt(t,`${e}[${i}]`,n,s)))}function Xt(t){const e=Object.keys(t);if(1!==e.length)throw new Error(`Please provide an object with a single key (operation name) mapping to a function. Got an object with ${e.length} keys.`);let n=e[0];const s=t[n];n.endsWith("_")&&(n=n.substring(0,n.length-1)),n+="__op";const i=(...t)=>{Kt.startScope(n);try{const e=s(...t);return nt(e)&&console.error("Cannot return a Promise inside of tidy."),Kt.endScope(e),e}catch(t){throw Kt.endScope(null),t}};return Object.defineProperty(i,"name",{value:n,configurable:!0}),i}const Qt=Xt({cast_:function(t,e){const n=Zt(t,"x","cast");if(!function(t){return"bool"===t||"complex64"===t||"float32"===t||"int32"===t||"string"===t}(e))throw new Error(`Failed to cast to unknown dtype ${e}`);if("string"===e&&"string"!==n.dtype||"string"!==e&&"string"===n.dtype)throw new Error("Only strings can be casted to strings");const s={x:n},i={dtype:e};return Kt.runKernel(r,s,i)}});const te=Xt({mul_:function(t,e){let n=Zt(t,"a","mul"),s=Zt(e,"b","mul");[n,s]=Bt(n,s);const i={a:n,b:s};return Kt.runKernel(w,i)}});const ee=Xt({step_:function(t,e=0){const n={x:Zt(t,"x","step")},s={alpha:e};return Kt.runKernel(M,n,s)}}),ne={kernelName:"Abs",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>te(t,ee(Qt(n,"float32"),-1))}}};const se=Xt({floorDiv_:function(t,e){let n=Zt(t,"a","floorDiv"),s=Zt(e,"b","floorDiv");[n,s]=Bt(n,s);const i={a:n,b:s};return Kt.runKernel(f,i)}});const ie=Xt({div_:function(t,e){let n=Zt(t,"a","div"),s=Zt(e,"b","div");if([n,s]=Bt(n,s),"int32"===n.dtype&&"int32"===s.dtype)return se(n,s);const i={a:n,b:s};return Kt.runKernel(c,i,{})}});const re=Xt({neg_:function(t){const e={x:Zt(t,"x","neg")};return Kt.runKernel("Neg",e)}});function ae(t,e,n,s){if(null==s&&(s=J(t)),"complex64"===s)throw new Error("Cannot construct a complex64 tensor directly. Please use tf.complex(real, imag).");if(!q(t)&&!Array.isArray(t)&&"number"!=typeof t&&"boolean"!=typeof t&&"string"!=typeof t)throw new Error("values passed to tensor(values) must be a number/boolean/string or an array of numbers/booleans/strings, or a TypedArray");if(null!=e){!function(t){t.forEach((e=>{B(Number.isInteger(e)&&e>=0,(()=>`Tensor must have a shape comprised of positive integers but got shape [${t}].`))}))}(e);const t=W(e),s=W(n);B(t===s,(()=>`Based on the provided shape, [${e}], the tensor should have ${t} values but has ${s}`));for(let t=0;t<n.length;++t){const s=n[t],i=t!==n.length-1||s!==W(e.slice(t));B(n[t]===e[t]||!i,(()=>`Error creating a new Tensor. Inferred shape (${n}) does not match the provided shape (${e}). `))}}return q(t)||Array.isArray(t)||(t=[t]),e=e||n,t="string"!==s?bt(t,s):P(t,[],!0),Kt.makeTensor(t,e,s)}function oe(t,e){if((q(t)&&"string"!==e||Array.isArray(t))&&"complex64"!==e)throw new Error("Error creating a new Scalar: value must be a primitive (number|boolean|string)");if("string"===e&&q(t)&&!(t instanceof Uint8Array))throw new Error("When making a scalar from encoded string, the value must be `Uint8Array`.");return ae(t,[],[],e)}const le=Xt({sqrt_:function(t){const e={x:Zt(t,"x","sqrt")};return Kt.runKernel(T,e)}});const ue=Xt({square_:function(t){const e=Zt(t,"x","square");return Kt.runKernel("Square",{x:e},{})}});const he=Xt({sub_:function(t,e){let n=Zt(t,"a","sub"),s=Zt(e,"b","sub");[n,s]=Bt(n,s);const i={a:n,b:s};return Kt.runKernel("Sub",i)}}),ce={kernelName:"Acos",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=ue(Qt(n,"float32")),s=le(he(oe(1),e));return re(ie(t,s))}}}},pe={kernelName:"Acosh",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=le(he(ue(Qt(n,"float32")),1));return ie(t,e)}}}};function de(t,e){const n=[];for(let s=0;s<e.length;s++){const i=t[t.length-s-1],r=e.length-s-1,a=e[r];(null==i||1===i&&a>1)&&n.unshift(r)}return n}function fe(t,e){const n=[],s=Math.max(t.length,e.length);for(let i=0;i<s;i++){let s=t[t.length-i-1];null==s&&(s=1);let r=e[e.length-i-1];if(null==r&&(r=1),1===s)n.unshift(r);else if(1===r)n.unshift(s);else{if(s!==r){throw Error(`Operands could not be broadcast together with shapes ${t} and ${e}.`)}n.unshift(s)}}return n}const ge=Xt({reshape_:function(t,e){const n={x:Zt(t,"x","reshape","string_or_numeric")},s={shape:e};return Kt.runKernel(S,n,s)}});const me=Xt({sum_:function(t,e=null,n=!1){let s=Zt(t,"x","sum");"bool"===s.dtype&&(s=Qt(s,"int32"));const i={x:s},r={axis:e,keepDims:n};return Kt.runKernel("Sum",i,r)}}),ye={kernelName:n,inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=fe(n.shape,s.shape);return{a:()=>{let e=t;const s=de(n.shape,i);return s.length>0&&(e=me(e,s)),ge(e,n.shape)},b:()=>{let e=t;const n=de(s.shape,i);return n.length>0&&(e=me(e,n)),ge(e,s.shape)}}}},be={kernelName:"AddN",saveAllInputs:!0,gradFunc:(t,e)=>{const n={};return e.forEach(((e,s)=>{n[s]=()=>t.clone()})),n}};const we=Xt({zerosLike_:function(t){const e={x:Zt(t,"x","zerosLike")};return Kt.runKernel(R,e)}}),ke={kernelName:"ArgMax",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>we(n)}}},ve={kernelName:"ArgMin",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>we(n)}}},Se={kernelName:"Asin",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>ie(t,le(he(oe(1),ue(Qt(n,"float32")))))}}};const xe=Xt({add_:function(t,e){let s=Zt(t,"a","add"),i=Zt(e,"b","add");[s,i]=Bt(s,i);const r={a:s,b:i};return Kt.runKernel(n,r)}}),Ne={kernelName:"Asinh",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=le(xe(oe(1),ue(Qt(n,"float32"))));return ie(t,e)}}}},ze={kernelName:"Atan2",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=fe(n.shape,s.shape);return{a:()=>{const e=xe(ue(n),ue(s));let r=te(t,ie(s,e));const a=de(n.shape,i);return a.length>0&&(r=me(r,a)),ge(r,n.shape)},b:()=>{const e=xe(ue(n),ue(s));let r=re(te(t,ie(n,e)));const a=de(s.shape,i);return a.length>0&&(r=me(r,a)),ge(r,s.shape)}}}},Ie={kernelName:"Atan",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>ie(t,xe(ue(Qt(n,"float32")),1))}}},Ae={kernelName:"Atanh",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>ie(t,he(oe(1),ue(Qt(n,"float32"))))}}};const Ce=Xt({avgPool3dGrad_:function(t,e,n,s,i,r){const a=Zt(t,"dy","avgPool3dGrad"),o=Zt(e,"input","avgPool3dGrad");let l=a,u=o,h=!1;4===o.rank&&(h=!0,l=ge(a,[1,a.shape[0],a.shape[1],a.shape[2],a.shape[3]]),u=ge(o,[1,o.shape[0],o.shape[1],o.shape[2],o.shape[3]])),B(5===l.rank,(()=>`Error in avgPool3dGrad: dy must be rank 5 but got rank ${l.rank}.`)),B(5===u.rank,(()=>`Error in avgPool3dGrad: input must be rank 5 but got rank ${u.rank}.`)),null!=r&&B(j(i),(()=>`Error in avgPool3dGrad: pad must be an integer when using, dimRoundingMode ${r} but got pad ${i}.`));const c={dy:l,input:u},p={filterSize:n,strides:s,pad:i,dimRoundingMode:r},d=Kt.runKernel("AvgPool3DGrad",c,p);return h?ge(d,[d.shape[1],d.shape[2],d.shape[3],d.shape[4]]):d}}),Te={kernelName:"AvgPool3D",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{filterSize:i,strides:r,pad:a,dimRoundingMode:o}=n;return{x:()=>Ce(t,s,i,r,a,o)}}};const $e=Xt({avgPoolGrad_:function(t,e,n,s,i){const r=Zt(t,"dy","avgPoolGrad"),a=Zt(e,"input","avgPoolGrad");B(a.rank===r.rank,(()=>`Rank of input (${a.rank}) does not match rank of dy (${r.rank})`));let o=a,l=r,u=!1;3===a.rank&&(u=!0,o=ge(a,[1,a.shape[0],a.shape[1],a.shape[2]]),l=ge(r,[1,r.shape[0],r.shape[1],r.shape[2]])),B(4===l.rank,(()=>`Error in avgPoolGrad: dy must be rank 4 but got rank ${l.rank}.`)),B(4===o.rank,(()=>`Error in avgPoolGrad: input must be rank 4 but got rank ${o.rank}.`));const h={dy:l,input:o},c={filterSize:n,strides:s,pad:i},p=Kt.runKernel("AvgPoolGrad",h,c);return u?ge(p,[p.shape[1],p.shape[2],p.shape[3]]):p}}),Ee={kernelName:"AvgPool",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{filterSize:i,strides:r,pad:a}=n;return{x:()=>$e(t,s,i,r,a)}}};const Fe=Xt({matMul_:function(t,e,n=!1,i=!1){let r=Zt(t,"a","matMul"),a=Zt(e,"b","matMul");[r,a]=Bt(r,a);const o={a:r,b:a},l={transposeA:n,transposeB:i};return Kt.runKernel(s,o,l)}}),De={kernelName:s,inputsToSave:["a","b"],gradFunc:(t,e,n)=>{const[s,i]=e,{transposeA:r,transposeB:a}=n;return r||a?!r&&a?{a:()=>Fe(t,i,!1,!1),b:()=>Fe(t,s,!0,!1)}:r&&!a?{a:()=>Fe(i,t,!1,!0),b:()=>Fe(s,t,!1,!1)}:{a:()=>Fe(i,t,!0,!0),b:()=>Fe(t,s,!0,!0)}:{a:()=>Fe(t,i,!1,!0),b:()=>Fe(s,t,!0,!1)}}};const Le=Xt({spaceToBatchND_:function(t,e,n){const s=Zt(t,"x","spaceToBatchND");B(s.rank>=1+e.length,(()=>`input rank ${s.rank} should be > than [blockShape] ${e.length}`)),B(n.length===e.length,(()=>`paddings.shape[0] ${n.length} must be equal to [blockShape] ${e.length}`)),B(s.shape.reduce(((t,s,i)=>i>0&&i<=e.length?t&&(s+n[i-1][0]+n[i-1][1])%e[i-1]==0:t),!0),(()=>`input spatial dimensions ${s.shape.slice(1)} with paddings ${n.toString()} must be divisible by blockShapes ${e.toString()}`));const i={x:s},r={blockShape:e,paddings:n};return Kt.runKernel($,i,r)}}),_e={kernelName:i,gradFunc:(t,e,n)=>{const{blockShape:s,crops:i}=n;return{x:()=>Le(t,s,i)}}},Re={kernelName:"BroadcastTo",gradFunc:(t,e,n)=>{const s=n,i=s.inputShape,r=s.shape,a=Array.from(r);for(let t=i.length-1;t>=0;t--)if(i[t]===r[t])a[t]=1;else if(1!==i[t])throw new Error(`broadcastTo(): [${i}] cannot be broadcast to [${r}].`);const o=[];for(let t=0;t<a.length;t++)a[t]>1&&o.push(t);return{x:()=>me(t,o,!0)}}},Me={kernelName:r,gradFunc:t=>({x:()=>t.clone()})},Oe={kernelName:"Ceil",gradFunc:t=>({x:()=>we(t)})};const Be=Xt({greaterEqual_:function(t,e){let n=Zt(t,"a","greaterEqual","string_or_numeric"),s=Zt(e,"b","greaterEqual","string_or_numeric");[n,s]=Bt(n,s),fe(n.shape,s.shape);const i={a:n,b:s};return Kt.runKernel(m,i)}});const Pe=Xt({lessEqual_:function(t,e){let n=Zt(t,"a","lessEqual","string_or_numeric"),s=Zt(e,"b","lessEqual","string_or_numeric");[n,s]=Bt(n,s),fe(n.shape,s.shape);const i={a:n,b:s};return Kt.runKernel("LessEqual",i)}});const We=Xt({logicalAnd_:function(t,e){const n=Zt(t,"a","logicalAnd","bool"),s=Zt(e,"b","logicalAnd","bool");fe(n.shape,s.shape);const i={a:n,b:s};return Kt.runKernel("LogicalAnd",i)}});const Ue=Xt({clone_:function(t){const e={x:Zt(t,"x","clone","string_or_numeric")};return Kt.runKernel(y,e)}});const je=Xt({broadcastTo_:function(t,e){let n=Zt(t,"broadcastTo","x");const s=n.shape;if(e.some((t=>!(t>0)||t%1!=0)))throw new Error(`broadcastTo(): Invalid broadcast shape [${e}].`);if(e.length<n.rank)throw new Error(`broadcastTo(): shape.length=${e.length} < input.rank=${n.rank}.`);if(e.length>n.rank){const t=n.shape.slice();for(;t.length<e.length;)t.unshift(1);n=ge(n,t)}const i=n.shape,r=Array.from(e);for(let t=e.length-1;t>=0;t--)if(i[t]===e[t])r[t]=1;else if(1!==n.shape[t])throw new Error(`broadcastTo(): [${s}] cannot be broadcast to [${e}].`);if(0===r.map(((t,e)=>t>1?e:-1)).filter((t=>t>=0)).length)return Ue(n);const a={x:n},o={reps:r};return Kt.runKernel(F,a,o)}});const Ve=Xt({where_:function(t,e,n){const s=Zt(e,"a","where"),i=Zt(n,"b","where"),r=Zt(t,"condition","where","bool"),a=fe(fe(r.shape,s.shape),i.shape),o={condition:je(r,a),t:je(s,a),e:je(i,a)};return Kt.runKernel(z,o)}}),Ke={kernelName:"ClipByValue",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{clipValueMin:i,clipValueMax:r}=n;return{x:()=>Ve(We(Be(s,i),Pe(s,r)),t,we(t))}}},qe={kernelName:"ComplexAbs",inputsToSave:["x"],gradFunc:ne.gradFunc};const Ge=Xt({split_:function(t,e,n=0){const s={x:Zt(t,"x","split")},i={numOrSizeSplits:e,axis:n};return Kt.runKernel(E,s,i)}}),He={kernelName:a,saveAllInputs:!0,gradFunc:(t,e,n)=>{const s=e.map((t=>t.shape)),{axis:i}=n,r=K(i,e[0].shape)[0],a=s.map((t=>t[r]));return Ge(t,a,r).map((t=>()=>t))}};const Je=Xt({conv2DBackpropFilter_:function(t,e,n,s,i,r="NHWC",a){let o=t;3===t.rank&&(o=ge(t,[1,t.shape[0],t.shape[1],t.shape[2]]));let l=e;3===l.rank&&(l=ge(e,[1,e.shape[0],e.shape[1],e.shape[2]])),B(4===o.rank,(()=>`Error in conv2dDerFilter: input must be rank 4, but got shape ${o.shape}.`)),B(4===l.rank,(()=>`Error in conv2dDerFilter: dy must be rank 4, but got shape ${l.shape}.`)),B(4===n.length,(()=>`Error in conv2dDerFilter: filterShape must be length 4, but got ${n}.`));const u="NHWC"===r?o.shape[3]:o.shape[1],h="NHWC"===r?l.shape[3]:l.shape[1];B(u===n[2],(()=>`Error in conv2dDerFilter: depth of input ${u}) must match input depth in filter (${n[2]}.`)),B(h===n[3],(()=>`Error in conv2dDerFilter: depth of dy (${h}) must match output depth for filter (${n[3]}).`)),null!=a&&B(j(i),(()=>`Error in conv2dDerFilter: pad must be an integer when using, dimRoundingMode ${a} but got pad ${i}.`));const c={x:o,dy:l},p={strides:s,pad:i,dataFormat:r,dimRoundingMode:a,filterShape:n};return Kt.runKernel("Conv2DBackpropFilter",c,p)}});const Ze=Xt({conv2DBackpropInput_:function(t,e,n,s,i,r="NHWC",a){B(t.length===e.rank,(()=>`Length of inShape (${t.length}) and rank of dy (${e.rank}) must match`));let o=t,u=e,h=!1;3===e.rank&&(h=!0,u=ge(e,[1,e.shape[0],e.shape[1],e.shape[2]]),o=[1,t[0],t[1],t[2]]),B(4===o.length,(()=>`Error in conv2dDerInput: inShape must be length 4, but got length ${o.length}.`)),B(4===u.rank,(()=>`Error in conv2dDerInput: dy must be rank 4, but got rank ${u.rank}`)),B(4===n.rank,(()=>`Error in conv2dDerInput: filter must be rank 4, but got rank ${n.rank}`));const c="NHWC"===r?o[3]:o[1],p="NHWC"===r?u.shape[3]:u.shape[1];B(c===n.shape[2],(()=>`Error in conv2dDerInput: depth of input (${c}) must match input depth for filter ${n.shape[2]}.`)),B(p===n.shape[3],(()=>`Error in conv2dDerInput: depth of output (${p}) must match output depth for filter ${n.shape[3]}.`)),null!=a&&B(j(i),(()=>`Error in conv2dDerInput: pad must be an integer when using, dimRoundingMode ${a} but got pad ${i}.`));const d={dy:u,filter:n},f={strides:s,pad:i,dataFormat:r,dimRoundingMode:a,inputShape:o},g=Kt.runKernel(l,d,f);return h?ge(g,[g.shape[1],g.shape[2],g.shape[3]]):g}});function Ye(t){const[e,n,s]=function(t){return"number"==typeof t?[t,t,t]:2===t.length?[t[0],t[1],1]:t}(t);return 1===e&&1===n&&1===s}function Xe(t,e){return Ye(t)||Ye(e)}const Qe={kernelName:o,inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const[s,i]=e,{dilations:r,strides:a,pad:o,dataFormat:l}=n;return B(Ye(r),(()=>`Error in gradient of conv2D: dilation rates greater than 1 are not yet supported in gradients. Got dilations '${r}'`)),{x:()=>Ze(s.shape,t,i,a,o,l),filter:()=>Je(s,t,i.shape,a,o,l)}}};const tn=Xt({conv2d_:function(t,e,n,s,i="NHWC",r=[1,1],a){const l=Zt(t,"x","conv2d"),u=Zt(e,"filter","conv2d");let h=l,c=!1;3===l.rank&&(c=!0,h=ge(l,[1,l.shape[0],l.shape[1],l.shape[2]])),B(4===h.rank,(()=>`Error in conv2d: input must be rank 4, but got rank ${h.rank}.`)),B(4===u.rank,(()=>`Error in conv2d: filter must be rank 4, but got rank ${u.rank}.`)),null!=a&&B(j(s),(()=>`Error in conv2d: pad must be an integer when using, dimRoundingMode ${a} but got pad ${s}.`));const p="NHWC"===i?h.shape[3]:h.shape[1];B(p===u.shape[2],(()=>`Error in conv2d: depth of input (${p}) must match input depth for filter ${u.shape[2]}.`)),B(Xe(n,r),(()=>`Error in conv2D: Either strides or dilations must be 1. Got strides ${n} and dilations '${r}'`));const d={x:h,filter:u},f={strides:n,pad:s,dataFormat:i,dilations:r,dimRoundingMode:a},g=Kt.runKernel(o,d,f);return c?ge(g,[g.shape[1],g.shape[2],g.shape[3]]):g}}),en={kernelName:l,inputsToSave:["dy","filter"],gradFunc:(t,e,n)=>{const[s,i]=e,{strides:r,pad:a,dataFormat:o,dimRoundingMode:l}=n;return{dy:()=>tn(t,i,r,a,o,1,l),filter:()=>Je(t,s,i.shape,r,a,o,l)}}};const nn=Xt({conv3DBackpropFilter_:function(t,e,n,s,i){let r=t;4===t.rank&&(r=ge(t,[1,t.shape[0],t.shape[1],t.shape[2],t.shape[3]]));let a=e;4===a.rank&&(a=ge(e,[1,e.shape[0],e.shape[1],e.shape[2],e.shape[3]])),B(5===r.rank,(()=>`Error in conv3dDerFilter: input must be rank 5, but got shape ${r.shape}.`)),B(5===a.rank,(()=>`Error in conv3dDerFilter: dy must be rank 5, but got shape ${a.shape}.`)),B(5===n.length,(()=>`Error in conv3dDerFilter: filterShape must be length 5, but got ${n}.`)),B(r.shape[4]===n[3],(()=>`Error in conv3dDerFilter: depth of input ${r.shape[4]}) must match input depth in filter (${n[3]}.`)),B(a.shape[4]===n[4],(()=>`Error in conv3dDerFilter: depth of dy (${a.shape[4]}) must match output depth for filter (${n[4]}).`));const o={x:r,dy:a},l={strides:s,pad:i,filterShape:n};return Kt.runKernel("Conv3DBackpropFilterV2",o,l)}});const sn=Xt({conv3DBackpropInput_:function(t,e,n,s,i){B(t.length===e.rank,(()=>`Length of inShape (${t.length}) and rank of dy (${e.rank}) must match`));let r=t,a=e,o=!1;4===e.rank&&(o=!0,a=ge(e,[1,e.shape[0],e.shape[1],e.shape[2],e.shape[3]]),r=[1,t[0],t[1],t[2],t[3]]);const l=r[4],u=a.shape[4];B(5===r.length,(()=>`Error in conv3dDerInput: inShape must be length 5, but got length ${r.length}.`)),B(5===a.rank,(()=>`Error in conv3dDerInput: dy must be rank 5, but got rank ${a.rank}`)),B(5===n.rank,(()=>`Error in conv3dDerInput: filter must be rank 5, but got rank ${n.rank}`)),B(l===n.shape[3],(()=>`Error in conv3dDerInput: depth of input (${l}) must match input depth for filter ${n.shape[3]}.`)),B(u===n.shape[4],(()=>`Error in conv3dDerInput: depth of output (${u}) must match output depth for filter ${n.shape[4]}.`));const h={dy:a,filter:n},c={pad:i,strides:s,inputShape:r},p=Kt.runKernel("Conv3DBackpropInputV2",h,c);return o?ge(p,[p.shape[1],p.shape[2],p.shape[3],p.shape[4]]):p}}),rn={kernelName:"Conv3D",inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const{dilations:s,strides:i,pad:r}=n;B(Ye(s),(()=>`Error in gradient of conv3D: dilation rates greater than 1 are not yet supported in gradients. Got dilations '${s}'`));const[a,o]=e;return{x:()=>sn(a.shape,t,o,i,r),filter:()=>nn(a,t,o.shape,i,r)}}};const an=Xt({sin_:function(t){const e={x:Zt(t,"x","sin")};return Kt.runKernel("Sin",e)}}),on={kernelName:"Cos",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>te(re(an(Qt(n,"float32"))),t)}}};const ln=Xt({sinh_:function(t){const e={x:Zt(t,"x","sinh")};return Kt.runKernel(A,e)}}),un={kernelName:u,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>te(ln(Qt(n,"float32")),t)}}};function hn(t,e){return function(t,e,n){const s=t.length+e.length,i=[];let r=0,a=0;for(let o=0;o<s;o++)-1===n.indexOf(o)?i.push(t[r++]):i.push(e[a++]);return i}(t,e.map((t=>1)),e)}function cn(t){return t.map(((t,e)=>[e,t])).sort(((t,e)=>t[1]-e[1])).map((t=>t[0]))}const pn=Xt({cumsum_:function(t,e=0,n=!1,s=!1){const i={x:Zt(t,"x","cumsum")},r={axis:e,exclusive:n,reverse:s};return Kt.runKernel(h,i,r)}});const dn=Xt({transpose_:function(t,e){const n=Zt(t,"x","transpose");if(null==e&&(e=n.shape.map(((t,e)=>e)).reverse()),B(n.rank===e.length,(()=>`Error in transpose: rank of input ${n.rank} must match length of perm ${e}.`)),e.forEach((t=>{B(t>=0&&t<n.rank,(()=>"All entries in 'perm' must be between 0 and "+(n.rank-1)+` but got ${e}`))})),n.rank<=1)return n.clone();const s={x:n},i={perm:e};return Kt.runKernel(D,s,i)}}),fn={kernelName:h,inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{axis:i,exclusive:r,reverse:a}=n;return{x:()=>{const e=function(t,e){if(function(t,e){for(let n=0;n<t.length;++n)if(t[t.length-n-1]!==e-1-n)return!1;return!0}(t,e))return null;const n=[];for(let s=0;s<e;++s)-1===t.indexOf(s)&&n.push(s);return t.forEach((t=>n.push(t))),n}([i],s.rank);let n=pn(t,i,r,!a);return null!=e&&(n=dn(n,e)),n}}}};const gn=Xt({depthwiseConv2dNativeBackpropFilter_:function(t,e,n,s,i,r=[1,1],a){let o=t;3===t.rank&&(o=ge(t,[1,t.shape[0],t.shape[1],t.shape[2]]));let l=e;3===l.rank&&(l=ge(e,[1,e.shape[0],e.shape[1],e.shape[2]]));const u={x:o,dy:l},h={strides:s,pad:i,dimRoundingMode:a,dilations:r,filterShape:n};return Kt.runKernel("DepthwiseConv2dNativeBackpropFilter",u,h)}});const mn=Xt({depthwiseConv2dNativeBackpropInput_:function(t,e,n,s,i,r=[1,1],a){let o=e,l=!1;3===e.rank&&(l=!0,o=ge(e,[1,e.shape[0],e.shape[1],e.shape[2]]));const u={dy:o,filter:n},h={strides:s,pad:i,dimRoundingMode:a,dilations:r,inputShape:t},c=Kt.runKernel("DepthwiseConv2dNativeBackpropInput",u,h);return l?ge(c,[c.shape[1],c.shape[2],c.shape[3]]):c}}),yn={kernelName:"DepthwiseConv2dNative",inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const{dilations:s,strides:i,pad:r,dimRoundingMode:a}=n,o=null==s?[1,1]:s;B(Ye(o),(()=>`Error in gradient of depthwiseConv2dNative: dilation rates greater than 1 are not yet supported. Got dilations '${o}'`));const[l,u]=e;return B(4===l.rank,(()=>`Error in gradient of depthwiseConv2dNative: input must be rank 4, but got rank ${l.rank}.`)),B(4===u.rank,(()=>`Error in gradient of depthwiseConv2dNative: filter must be rank 4, but got rank ${u.rank}.`)),B(l.shape[3]===u.shape[2],(()=>`Error in gradient of depthwiseConv2d: number of input channels (${l.shape[3]}) must match the inChannels dimension in filter ${u.shape[2]}.`)),B(Xe(i,o),(()=>`Error in gradient of depthwiseConv2d: Either strides or dilations must be  1. Got strides ${i} and dilations '${o}'.`)),null!=a&&B(j(r),(()=>`Error in depthwiseConv2d: pad must be an integer when using, dimRoundingMode ${a} but got pad ${r}.`)),{x:()=>mn(l.shape,t,u,i,r,o,a),filter:()=>gn(l,t,u.shape,i,r,o,a)}}},bn={kernelName:"Dilation2D",inputsToSave:["x","filter"],gradFunc:(t,e,n)=>{const[s,i]=e,r={x:s,filter:i,dy:t},a={x:s,filter:i,dy:t};return{x:()=>Kt.runKernel("Dilation2DBackpropInput",r,n),filter:()=>Kt.runKernel("Dilation2DBackpropFilter",a,n)}}},wn={kernelName:"Elu",outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e,s={dy:t,y:n};return{x:()=>Kt.runKernel("EluGrad",s)}}};const kn=Xt({exp_:function(t){const e={x:Zt(t,"x","exp")};return Kt.runKernel("Exp",e)}}),vn={kernelName:"Erf",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e,s=te(kn(re(ue(n))),2/Math.sqrt(Math.PI));return{x:()=>te(t,s)}}},Sn={kernelName:"Exp",outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e;return{x:()=>te(t,n)}}},xn={kernelName:p,inputsToSave:["input"],gradFunc:(t,e)=>{const[n]=e;return{input:()=>ge(t,n.shape)}}},Nn={kernelName:"Expm1",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>te(t,kn(n))}}},zn={kernelName:d,gradFunc:t=>({x:()=>we(t)})},In={kernelName:f,inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=fe(n.shape,s.shape);return{a:()=>{const e=ie(t,Qt(s,"float32")),r=de(n.shape,i);return r.length>0?ge(me(e,r),n.shape):e},b:()=>{let e=te(t,Qt(n,"float32"));const r=de(s.shape,i);r.length>0&&(e=ge(me(e,r),s.shape));const a=ue(s);return re(ie(e,Qt(a,"float32")))}}}};const An=Xt({rsqrt_:function(t){const e={x:Zt(t,"x","rsqrt")};return Kt.runKernel(N,e)}});const Cn=Xt({tile_:function(t,e){const n=Zt(t,"x","tile","string_or_numeric");B(n.rank===e.length,(()=>`Error in transpose: rank of input ${n.rank} must match length of reps ${e}.`));const s={x:n},i={reps:e};return Kt.runKernel(F,s,i)}}),Tn={kernelName:"FusedBatchNorm",inputsToSave:["x","mean","variance","scale"],gradFunc:(t,e,n)=>{const{varianceEpsilon:s}=n,[i,r,a,o]=e,l=null==o?oe(1):o,u=de(r.shape,i.shape),h=[];if(1===r.rank){for(let t=0;t<i.shape.length-1;++t)h.push(i.shape[t]);h.push(1)}const c=he(i,r),p=te(t,l),d=An(xe(a,oe(s))),f=te(te(te(d,d),d),oe(-.5));return{x:()=>1===r.rank?ge(te(te(t,Cn(ge(d,[1,1,1,r.shape[0]]),h)),l),i.shape):ge(te(te(t,d),l),i.shape),mean:()=>{let t=te(te(d,oe(-1)),p);return 1===r.rank&&(t=me(t,u)),ge(t,r.shape)},variance:()=>{let t=te(te(f,c),p);return 1===r.rank&&(t=me(t,u)),ge(t,r.shape)},scale:()=>{const e=te(c,d);let n=te(t,e);return 1===r.rank&&(n=me(n,u)),ge(n,r.shape)},offset:()=>{let e=t;return 1===r.rank&&(e=me(e,u)),ge(e,r.shape)}}}};const $n=Xt({unsortedSegmentSum_:function(t,e,n){const s=Zt(t,"x","unsortedSegmentSum"),i=Zt(e,"segmentIds","unsortedSegmentSum","int32");B(j(n),(()=>"numSegments must be of dtype int"));const r={x:s,segmentIds:i},a={numSegments:n};return Kt.runKernel(_,r,a)}}),En={kernelName:g,inputsToSave:["x","indices"],gradFunc:(t,e,n)=>{const[s,i]=e,{axis:r}=n,a=K(r,s.shape)[0];return{x:()=>{const e=s.shape,n=i.size,o=e.slice(0,a),l=o.length,u=e.slice(r,e.length).slice(1),h=u.length,c=Fn(0,l),p=Fn(l+1,l+1+h),d=Dn([o,[n],u]),f=ge(t,d),g=ge(i,[n]),m=Dn([[l],c,p]),y=dn(f,m);let b=$n(y,g,s.shape[a]);const w=cn(m);return b=dn(b,w),b},indices:()=>i}}};function Fn(t,e){const n=[];for(let s=t;s<e;++s)n.push(s);return n}function Dn(t){const e=[];for(let n=0;n<t.length;++n)for(let s=0;s<t[n].length;++s)e.push(t[n][s]);return e}const Ln={kernelName:m,inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e;return{a:()=>we(n),b:()=>we(s)}}},_n={kernelName:y,gradFunc:t=>({x:()=>Qt(t,"float32")})},Rn={kernelName:"IsFinite",gradFunc:t=>({x:()=>we(t)})},Mn={kernelName:"IsInf",gradFunc:t=>({x:()=>we(t)})},On={kernelName:"IsNan",gradFunc:t=>({x:()=>we(t)})};const Bn=Xt({greater_:function(t,e){let n=Zt(t,"a","greater","string_or_numeric"),s=Zt(e,"b","greater","string_or_numeric");[n,s]=Bt(n,s),fe(n.shape,s.shape);const i={a:n,b:s};return Kt.runKernel("Greater",i)}}),Pn={kernelName:"LeakyRelu",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{alpha:i}=n,r=Bn(s,0);return{x:()=>Ve(r,t,te(t,i))}}},Wn={kernelName:"Log1p",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>ie(t,xe(n,1))}}},Un={kernelName:"Log",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>ie(t,Qt(n,"float32"))}}},jn={kernelName:"LogSoftmax",inputsToSave:[],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s]=e,{axis:i}=n;return{logits:()=>{const e=kn(s);return he(t,te(me(t,i,!0),e))}}}};const Vn=Xt({localResponseNormalizationBackprop_:function(t,e,n,s=5,i=1,r=1,a=.5){const o={x:t,y:e,dy:n},l={depthRadius:s,bias:i,alpha:r,beta:a};return Kt.runKernel("LRNGrad",o,l)}}),Kn={kernelName:"LRN",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s,i]=e,{depthRadius:r,bias:a,alpha:o,beta:l}=n;return{x:()=>Vn(s,i,t,r,a,o,l)}}};const qn=Xt({equal_:function(t,e){let n=Zt(t,"a","equal","string_or_numeric"),s=Zt(e,"b","equal","string_or_numeric");[n,s]=Bt(n,s),fe(n.shape,s.shape);const i={a:n,b:s};return Kt.runKernel("Equal",i)}});function Gn(t,e,n,s){return e.rank<n.rank&&(e=ge(e,hn(e.shape,s))),t.rank<n.rank&&(t=ge(t,hn(t.shape,s))),{x:()=>te(t,Qt(qn(n,e),t.dtype))}}const Hn={kernelName:"Max",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const s=n,{reductionIndices:i}=s,r=e[0],a=Gn(t,e[1],r,K(i,r.shape));return{x:()=>a.x()}}};const Jn=Xt({less_:function(t,e){let n=Zt(t,"a","less","string_or_numeric"),s=Zt(e,"b","less","string_or_numeric");[n,s]=Bt(n,s),fe(n.shape,s.shape);const i={a:n,b:s};return Kt.runKernel("Less",i)}}),Zn={kernelName:b,inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e;return{a:()=>te(t,Qt(Be(n,s),"float32")),b:()=>te(t,Qt(Jn(n,s),"float32"))}}};const Yn=Xt({maxPool3dGrad_:function(t,e,n,s,i,r,a){const o=Zt(t,"dy","maxPool3dGrad"),l=Zt(e,"input","maxPool3dGrad"),u=Zt(n,"output","maxPool3dGrad");let h=o,c=l,p=u,d=!1;4===l.rank&&(d=!0,h=ge(o,[1,o.shape[0],o.shape[1],o.shape[2],o.shape[3]]),c=ge(l,[1,l.shape[0],l.shape[1],l.shape[2],l.shape[3]]),p=ge(u,[1,u.shape[0],u.shape[1],u.shape[2],u.shape[3]])),B(5===h.rank,(()=>`Error in maxPool3dGrad: dy must be rank 5 but got rank ${h.rank}.`)),B(5===c.rank,(()=>`Error in maxPool3dGrad: input must be rank 5 but got rank ${c.rank}.`)),B(5===p.rank,(()=>`Error in maxPool3dGrad: output must be rank 5 but got rank ${p.rank}.`)),null!=a&&B(j(r),(()=>`Error in maxPool3dGrad: pad must be an integer when using, dimRoundingMode ${a} but got pad ${r}.`));const f={dy:h,input:c,output:p},g={filterSize:s,strides:i,pad:r,dimRoundingMode:a},m=Kt.runKernel("MaxPool3DGrad",f,g);return d?ge(m,[m.shape[1],m.shape[2],m.shape[3],m.shape[4]]):m}}),Xn={kernelName:"MaxPool3D",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s,i]=e,{filterSize:r,strides:a,pad:o,dimRoundingMode:l}=n;return{x:()=>Yn(t,s,i,r,a,o,l)}}};const Qn=Xt({maxPoolGrad_:function(t,e,n,s,i,r,a){const o=Zt(t,"dy","maxPoolGrad"),l=Zt(e,"input","maxPoolGrad"),u=Zt(n,"output","maxPoolGrad");B(l.rank===o.rank,(()=>`Rank of input (${l.rank}) does not match rank of dy (${o.rank})`)),B(4===o.rank,(()=>`Error in maxPoolGrad: dy must be rank 4 but got rank ${o.rank}.`)),B(4===l.rank,(()=>`Error in maxPoolGrad: input must be rank 4 but got rank ${l.rank}.`)),null!=a&&B(j(r),(()=>`Error in maxPoolGrad: pad must be an integer when using, dimRoundingMode ${a} but got pad ${r}.`));const h={dy:o,input:l,output:u},c={filterSize:s,strides:i,pad:r,dimRoundingMode:a};return Kt.runKernel("MaxPoolGrad",h,c)}}),ts={kernelName:"MaxPool",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s,i]=e,{filterSize:r,strides:a,pad:o}=n;return{x:()=>Qn(t,s,i,r,a,o)}}};const es=Xt({complex_:function(t,e){const n=Zt(t,"real","complex"),s=Zt(e,"imag","complex");!function(t,e,n=""){B(U(t,e),(()=>n+` Shapes ${t} and ${e} must match`))}(n.shape,s.shape,`real and imag shapes, ${n.shape} and ${s.shape}, must match in call to tf.complex().`);const i={real:n,imag:s};return Kt.runKernel("Complex",i)}});function ns(t,e="float32"){if("complex64"===e){const e=ns(t,"float32"),n=ns(t,"float32");return es(e,n)}const n=et(W(t),e);return Kt.makeTensor(n,t,e)}function ss(t,e="float32"){if("complex64"===e){const e=ss(t,"float32"),n=ns(t,"float32");return es(e,n)}const n=tt(W(t),e);return Kt.makeTensor(n,t,e)}const is={kernelName:"Mean",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{axis:i}=n,r=K(i,s.shape),a=W(function(t,e){const n=[],s=t.length;for(let i=0;i<s;i++)-1===e.indexOf(i)&&n.push(t[i]);return[n,e.map((e=>t[e]))]}(s.shape,r)[1]);return{x:()=>{const e=s.shape.slice();r.forEach((t=>{e[t]=1}));const n=ge(t,e);return ie(te(n,ss(s.shape,"float32")),a)}}}},rs={kernelName:"Min",inputsToSave:["x"],outputsToSave:[!0],gradFunc:(t,e,n)=>{const s=n,{axis:i}=s,[r,a]=e,o=Gn(t,a,r,K(i,r.shape));return{x:()=>o.x()}}},as={kernelName:"Minimum",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e;return{a:()=>te(t,Qt(Pe(n,s),"float32")),b:()=>te(t,Qt(Bn(n,s),"float32"))}}};const os=Xt({slice_:function(t,e,n){const s=Zt(t,"x","slice","string_or_numeric");if(0===s.rank)throw new Error("Slicing scalar is not possible");const i={x:s},r={begin:e,size:n};return Kt.runKernel(I,i,r)}}),ls={kernelName:"MirrorPad",inputsToSave:["x"],gradFunc:(t,e,n)=>{const s=e[0],{paddings:i}=n,r=i.map((t=>t[0]));return{x:()=>os(t,r,s.shape)}}};const us=Xt({floor_:function(t){const e={x:Zt(t,"x","floor")};return Kt.runKernel(d,e)}}),hs={kernelName:"Mod",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=fe(n.shape,s.shape);return{a:()=>{const e=de(n.shape,i);return e.length>0?ge(me(t,e),n.shape):t},b:()=>{const e=te(t,re(us(ie(n,s)))),r=de(s.shape,i);return r.length>0?ge(me(e,r),s.shape):e}}}},cs={kernelName:w,inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=fe(n.shape,s.shape);return{a:()=>{const e=te(t,Qt(s,"float32")),r=de(n.shape,i);return r.length>0?ge(me(e,r),n.shape):e},b:()=>{const e=te(t,Qt(n,"float32")),r=de(s.shape,i);return r.length>0?ge(me(e,r),s.shape):e}}}},ps={kernelName:"Neg",gradFunc:t=>({x:()=>re(t)})},ds={kernelName:"OneHot",inputsToSave:["indices"],gradFunc:(t,e)=>{const n=e[0];return{indices:()=>ns(n.shape,"float32")}}},fs={kernelName:"OnesLike",gradFunc:t=>({x:()=>we(t)})};const gs=Xt({unstack_:function(t,e=0){const n=Zt(t,"x","unstack","string_or_numeric");B(e>=-n.shape.length&&e<n.shape.length,(()=>`Axis = ${e} is not in [-${n.shape.length}, ${n.shape.length})`));const s={value:n},i={axis:e};return Kt.runKernel(L,s,i)}}),ms={kernelName:k,saveAllInputs:!0,gradFunc:(t,e,n)=>{const{axis:s}=n;return gs(t,s).map((t=>()=>t))}},ys={kernelName:v,inputsToSave:["x"],gradFunc:(t,e,n)=>{const s=e[0],{paddings:i}=n,r=i.map((t=>t[0]));return{x:()=>os(t,r,s.shape)}}};const bs=Xt({log_:function(t){const e={x:Zt(t,"x","log")};return Kt.runKernel("Log",e)}});const ws=Xt({pow_:function(t,e){let n=Zt(t,"base","pow"),s=Zt(e,"exp","pow");[n,s]=Bt(n,s);const i={a:n,b:s};return Kt.runKernel("Pow",i)}}),ks={kernelName:"Pow",inputsToSave:["a","b"],outputsToSave:[!0],gradFunc:(t,e)=>{const[n,s,i]=e,r=n,a=s,o=fe(r.shape,a.shape);return{a:()=>{const e=Qt(a,"float32");let n=te(t,te(e,ws(r,he(e,oe(1)))));const s=de(r.shape,o);return s.length>0&&(n=me(n,s)),ge(n,r.shape)},b:()=>{const e=Bn(r,0),n=Ve(e,bs(r),we(r));let s=te(t,te(i,n));const l=de(a.shape,o);return l.length>0&&(s=me(s,l)),ge(s,a.shape)}}}},vs={kernelName:"Prelu",inputsToSave:["x","alpha"],gradFunc:(t,e)=>{const[n,s]=e,i=Bn(n,0);return{x:()=>Ve(i,t,te(t,s)),alpha:()=>{let e=Ve(i,we(t),te(t,n));const r=de(s.shape,t.shape);return r.length>0&&(e=me(e,r)),ge(e,s.shape)}}}},Ss={kernelName:c,inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=fe(n.shape,s.shape);return{a:()=>{const e=ie(t,Qt(s,"float32")),r=de(n.shape,i);return r.length>0?ge(me(e,r),n.shape):e},b:()=>{let e=te(t,Qt(n,"float32"));const r=de(s.shape,i);r.length>0&&(e=ge(me(e,r),s.shape));const a=ue(s);return re(ie(e,Qt(a,"float32")))}}}},xs={kernelName:"Reciprocal",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>ie(t,re(ue(n)))}}},Ns={kernelName:"Relu6",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e,s=te(Pe(n,6),ee(n));return{x:()=>te(t,Qt(s,"float32"))}}},zs={kernelName:"Relu",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>te(t,Qt(ee(n),"float32"))}}},Is={kernelName:S,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>ge(t,n.shape)}}},As={kernelName:"ResizeBilinear",inputsToSave:["images"],gradFunc:(t,e,n)=>{const[s]=e,i={dy:t,images:s};return{images:()=>Kt.runKernel("ResizeBilinearGrad",i,n)}}},Cs={kernelName:"ResizeNearestNeighbor",inputsToSave:["images"],gradFunc:(t,e,n)=>{const[s]=e,i={dy:t,images:s};return{images:()=>Kt.runKernel("ResizeNearestNeighborGrad",i,n)}}};const Ts=Xt({reverse_:function(t,e){const n={x:Zt(t,"x","reverse")},s={dims:e};return Kt.runKernel(x,n,s)}}),$s={kernelName:x,gradFunc:(t,e,n)=>{const{dims:s}=n,i=K(s,t.shape);return{x:()=>Ts(t,i)}}},Es={kernelName:"Round",gradFunc:t=>({x:()=>we(t)})},Fs={kernelName:N,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>re(ie(t,te(ws(n,1.5),2)))}}};const Ds=Xt({logicalNot_:function(t){const e={x:Zt(t,"x","logicalNot","bool")};return Kt.runKernel("LogicalNot",e)}}),Ls={kernelName:z,inputsToSave:["condition"],gradFunc:(t,e)=>{const[n]=e;return{condition:()=>Qt(we(n),"float32"),t:()=>te(t,Qt(n,t.dtype)),e:()=>te(t,Qt(Ds(n),t.dtype))}}},_s={kernelName:"Selu",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>{const e=Bn(n,oe(0)),s=oe(1.7580993408473768),i=oe(1.0507009873554805),r=te(t,i),a=te(te(t,s),kn(Qt(n,"float32")));return Ve(e,r,a)}}}},Rs={kernelName:C,outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e;return{x:()=>te(t,te(n,he(oe(1),n)))}}},Ms={kernelName:"Sign",gradFunc:t=>({x:()=>we(t)})};const Os=Xt({cos_:function(t){const e={x:Zt(t,"x","cos")};return Kt.runKernel("Cos",e)}}),Bs={kernelName:"Sin",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>te(Os(Qt(n,"float32")),t)}}};const Ps=Xt({cosh_:function(t){const e={x:Zt(t,"x","cosh")};return Kt.runKernel(u,e)}}),Ws={kernelName:A,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>te(Ps(Qt(n,"float32")),t)}}};const Us=Xt({pad_:function(t,e,n=0){const s=Zt(t,"x","pad");if(0===s.rank)throw new Error("pad(scalar) is not defined. Pass non-scalar to pad");const i={paddings:e,constantValue:n},r={x:s};return Kt.runKernel(v,r,i)}});const js={kernelName:I,inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{begin:i,size:r}=n,a=s.shape,[o,l]=function(t,e,n){let s;const i=t.shape.length;let r;return s="number"==typeof e?[e,...new Array(i-1).fill(0)]:e.length<i?e.concat(new Array(i-e.length).fill(0)):e.slice(),s.forEach((t=>{B(-1!==t,(()=>"slice() does not support negative begin indexing."))})),r=null==n?new Array(i).fill(-1):"number"==typeof n?[n,...new Array(i-1).fill(-1)]:n.length<i?n.concat(new Array(i-n.length).fill(-1)):n,r=r.map(((e,n)=>e>=0?e:(B(-1===e,(()=>`Negative size values should be exactly -1 but got ${e} for the slice() size at index ${n}.`)),t.shape[n]-s[n]))),[s,r]}(s,i,r),u=[];for(let e=0;e<t.rank;e++)u.push([o[e],a[e]-o[e]-l[e]]);return{x:()=>Us(t,u)}}},Vs={kernelName:"Softmax",outputsToSave:[!0],gradFunc:(t,e,n)=>{const[s]=e,{dim:i}=n,r=te(t,s);return{logits:()=>he(r,te(me(r,[i],true),s))}}};const Ks=Xt({sigmoid_:function(t){const e={x:Zt(t,"x","sigmoid")};return Kt.runKernel(C,e)}}),qs={kernelName:"Softplus",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>te(t,Ks(n))}}};const Gs=Xt({batchToSpaceND_:function(t,e,n){const s=Zt(t,"x","batchToSpaceND"),r=e.reduce(((t,e)=>t*e));B(s.rank>=1+e.length,(()=>`input rank is ${s.rank} but should be > than blockShape.length ${e.length}`)),B(n.length===e.length,(()=>`crops.length is ${n.length} but should be equal to blockShape.length  ${e.length}`)),B(s.shape[0]%r==0,(()=>`input tensor batch is ${s.shape[0]} but is not divisible by the product of the elements of blockShape ${e.join(" * ")} === ${r}`));const a={x:s},o={blockShape:e,crops:n};return Kt.runKernel(i,a,o)}}),Hs={kernelName:$,gradFunc:(t,e,n)=>{const{blockShape:s,paddings:i}=n;return{x:()=>Gs(t,s,i)}}};const Js=Xt({concat_:function(t,e=0){B(t.length>=1,(()=>"Pass at least one tensor to concat"));const n=Yt(t,"tensors","concat","string_or_numeric");if("complex64"===n[0].dtype&&n.forEach((t=>{if("complex64"!==t.dtype)throw new Error(`Cannot concatenate complex64 tensors with a tensor\n          with dtype ${t.dtype}. `)})),1===n.length)return Ue(n[0]);const s=n,i={axis:e};return Kt.runKernel(a,s,i)}}),Zs={kernelName:E,gradFunc:(t,e,n)=>{const{axis:s}=n;return{x:()=>Js(t,s)}}},Ys={kernelName:T,inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>ie(t,te(le(Qt(n,"float32")),2))}}},Xs={kernelName:"Square",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>te(t,te(Qt(n,"float32"),2))}}},Qs={kernelName:"SquaredDifference",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=oe(2);return{a:()=>te(t,te(i,he(n,s))),b:()=>te(t,te(i,he(s,n)))}}},ti={kernelName:M,gradFunc:t=>({x:()=>we(t)})},ei={kernelName:"Sub",inputsToSave:["a","b"],gradFunc:(t,e)=>{const[n,s]=e,i=fe(n.shape,s.shape);return{a:()=>{let e=t;const s=de(n.shape,i);return s.length>0&&(e=me(e,s)),ge(e,n.shape)},b:()=>{let e=t;const n=de(s.shape,i);return n.length>0&&(e=me(e,n)),ge(re(e),s.shape)}}}},ni={kernelName:"Sum",inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,i=s.shape.slice(),{axis:r}=n;K(r,s.shape).forEach((t=>{i[t]=1}));const a=ge(t,i),o=te(a,ss(s.shape,"float32"));return{x:()=>o}}},si={kernelName:"Tan",inputsToSave:["x"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>ie(t,ue(Os(n)))}}},ii={kernelName:"Tanh",outputsToSave:[!0],gradFunc:(t,e)=>{const[n]=e;return{x:()=>te(he(oe(1),ue(n)),t)}}},ri={kernelName:F,inputsToSave:["x"],gradFunc:(t,e,n)=>{const[s]=e,{reps:i}=n;return{x:()=>{let e=we(s);if(1===s.rank)for(let n=0;n<i[0];++n)e=xe(e,os(t,[n*s.shape[0]],[s.shape[0]]));else if(2===s.rank)for(let n=0;n<i[0];++n)for(let r=0;r<i[1];++r)e=xe(e,os(t,[n*s.shape[0],r*s.shape[1]],[s.shape[0],s.shape[1]]));else if(3===s.rank)for(let n=0;n<i[0];++n)for(let r=0;r<i[1];++r)for(let a=0;a<i[2];++a)e=xe(e,os(t,[n*s.shape[0],r*s.shape[1],a*s.shape[2]],[s.shape[0],s.shape[1],s.shape[2]]));else{if(4!==s.rank)throw new Error(`Gradient for tile operation is not implemented for rank-${s.rank} tensors yet.`);for(let n=0;n<i[0];++n)for(let r=0;r<i[1];++r)for(let a=0;a<i[2];++a)for(let o=0;o<i[3];++o)e=xe(e,os(t,[n*s.shape[0],r*s.shape[1],a*s.shape[2],o*s.shape[3]],[s.shape[0],s.shape[1],s.shape[2],s.shape[3]]))}return e}}}},ai={kernelName:D,gradFunc:(t,e,n)=>{const s=n,{perm:i}=s,r=cn(i);return{x:()=>dn(t,r)}}};const oi=Xt({stack_:function(t,e=0){const n=Yt(t,"tensors","stack","string_or_numeric");B(n.length>=1,(()=>"Pass at least one tensor to tf.stack")),n.length>0&&B(e<=n[0].rank,(()=>"Axis must be <= rank of the tensor"));const s=n,i={axis:e};return Kt.runKernel(k,s,i)}}),li={kernelName:L,gradFunc:(t,e,n)=>{const s=n,{axis:i}=s;return{value:()=>oi(t,i)}}};const ui=Xt({expandDims_:function(t,e=0){const n=Zt(t,"x","expandDims","string_or_numeric");B(e<=n.rank,(()=>"Axis must be <= rank of the tensor"));const s={input:n},i={dim:e};return Kt.runKernel(p,s,i)}});const hi=Xt({gather_:function(t,e,n=0,s=0){const i={x:Zt(t,"x","gather"),indices:Zt(e,"indices","gather","int32")},r={axis:n,batchDims:s};return Kt.runKernel(g,i,r)}});const ci=Xt({maximum_:function(t,e){let n=Zt(t,"a","maximum"),s=Zt(e,"b","maximum");[n,s]=Bt(n,s),"bool"===n.dtype&&(n=Qt(n,"int32"),s=Qt(s,"int32")),fe(n.shape,s.shape);const i={a:n,b:s};return Kt.runKernel(b,i)}});const pi=[ne,ce,pe,ye,be,ke,ve,Se,Ne,ze,Ie,Ae,Te,Ee,De,_e,Re,Me,Oe,Ke,qe,He,en,Qe,rn,on,un,fn,yn,bn,Ss,wn,vn,Sn,xn,Nn,In,zn,Tn,En,Ln,_n,Rn,Mn,On,Pn,Wn,Un,jn,Kn,Hn,Hn,Zn,Xn,ts,is,rs,as,ls,hs,cs,ps,ds,fs,ms,ys,ys,ks,vs,xs,Ns,zs,Is,As,Cs,$s,Es,Fs,Ls,_s,Rs,Ms,Bs,Ws,js,Vs,qs,Hs,Hs,Zs,Zs,Ys,Qs,Xs,ti,ei,ni,si,ii,ri,ai,li,{kernelName:_,inputsToSave:["segmentIds"],gradFunc:(t,e)=>{const[n]=e;return{x:()=>function(t,e){const n=ci(e,we(e)),s=hi(t,n);let i=Be(e,oe(0,"int32"));const r=s.rank-i.rank;for(let t=0;t<r;++t)i=ui(i,t+1);i=We(i,ss(s.shape,"bool"));const a=we(s);return Ve(i,s,a)}(t,n)}}},{kernelName:R,gradFunc:t=>({x:()=>we(t)})}];for(const t of pi)yt(t);let di;function fi(){return null==di&&(di=e.backend().epsilon()),di}class gi extends Error{constructor(t){super(t),Object.setPrototypeOf(this,gi.prototype)}}class mi extends Error{constructor(t){super(t),Object.setPrototypeOf(this,mi.prototype)}}class yi extends Error{constructor(t){super(t),Object.setPrototypeOf(this,yi.prototype)}}class bi extends Error{constructor(t){super(t),Object.setPrototypeOf(this,bi.prototype)}}class wi extends Error{constructor(t){super(t),Object.setPrototypeOf(this,wi.prototype)}}function ki(t,e){if(Array.isArray(t)){let n=[];for(let s=0;s<e;s++)n=n.concat(t);return n}{const n=new Array(e);return n.fill(t),n}}function vi(t,e){if(!t)throw new wi(e)}function Si(t,e){let n=0;for(const s of t)s===e&&n++;return n}function xi(t){return 1===t.length?t[0]:t}function Ni(t){return Array.isArray(t)?t:[t]}function zi(t){const e=t.replace(/(.)([A-Z][a-z0-9]+)/g,"$1_$2").replace(/([a-z])([A-Z])/g,"$1_$2").toLowerCase();return"_"!==e[0]?e:"private"+e}function Ii(t){return t.length<=1||-1===t.indexOf("_")?t:t.replace(/[_]+(\w|$)/g,((t,e)=>e.toUpperCase()))}let Ai={};function Ci(t){if(null==t)return null;const e={};return e.className=t.getClassName(),e.config=t.getConfig(),e}function Ti(t){if(null!=t&&"object"==typeof t)if(Array.isArray(t))t.forEach((t=>Ti(t)));else{const e=Object.keys(t);for(const n of e){const e=t[n];null!=e&&"object"==typeof e&&(Array.isArray(e)||"ndarray"!==e.type||"number"!=typeof e.value?Ti(e):t[n]=e.value)}}}function $i(t,e={},n={},s="object",i=!1){if("string"==typeof t){const i=t;let r;if(i in n)r=n[i];else if(i in Ai)r=Ai[i];else if(r=e[i],null==r)throw new yi(`Unknown ${s}: ${t}. This may be due to one of the following reasons:\n1. The ${s} is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.\n2. The custom ${s} is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().`);return r}{const r=t;if(null==r.className||null==r.config)throw new yi(`${s}: Improper config format: ${JSON.stringify(r)}.\n'className' and 'config' must set.`);const a=r.className;let o,l;if(a in n?[o,l]=n[a]:a in Ai?[o,l]=Ai.className:a in e&&([o,l]=e[a]),null==o)throw new yi(`Unknown ${s}: ${a}. This may be due to one of the following reasons:\n1. The ${s} is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.\n2. The custom ${s} is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().`);if(null!=l){const t={};for(const e of Object.keys(Ai))t[e]=Ai[e];for(const e of Object.keys(n))t[e]=n[e];r.config.customObjects=t;const e=Object.assign({},Ai);for(const t of Object.keys(n))Ai[t]=n[t];Ti(r.config);const s=l(o,r.config,n,i);return Ai=Object.assign({},e),s}{const t=Object.assign({},Ai);for(const t of Object.keys(n))Ai[t]=n[t];const e=new o(r.config);return Ai=Object.assign({},t),e}}}function Ei(t,e){return-1*function(t,e){return t<e?-1:t>e?1:0}(t,e)}function Fi(t){if(null==t)return t;const e=[];for(const n of t)-1===e.indexOf(n)&&e.push(n);return e}function Di(t){if(null==t)throw new yi(`Invalid value in obj: ${JSON.stringify(t)}`);for(const e in t)if(t.hasOwnProperty(e))return!1;return!0}function Li(t,e,n){if(null!=n&&t.indexOf(n)<0)throw new yi(`${n} is not a valid ${e}.  Valid values are ${t} or null/undefined.`)}function _i(t,e,n=0,s=1/0){return vi(n>=0),vi(s>=n),Array.isArray(t)&&t.length>=n&&t.length<=s&&t.every((t=>typeof t===e))}function Ri(t,n){Array.isArray(t)?(e.util.assert(t.length>0,(()=>`${n} is unexpectedly an empty array.`)),t.forEach(((t,e)=>Ri(t,`element ${e+1} of ${n}`)))):e.util.assert(Number.isInteger(t)&&t>0,(()=>`Expected ${n} to be a positive integer, but got ${Mi(t)}.`))}function Mi(t){return null===t?"null":Array.isArray(t)?"["+t.map((t=>Mi(t))).join(",")+"]":"string"==typeof t?`"${t}"`:`${t}`}function Oi(t){return"relu"===t?"relu":"linear"===t?"linear":"elu"===t?"elu":null}function Bi(t,n){return e.tidy((()=>e.sqrt(e.sum(e.mul(t,t),n,!0))))}class Pi extends e.serialization.Serializable{getConfig(){return{}}}class Wi extends Pi{constructor(t){super(),this.defaultMaxValue=2,this.defaultAxis=0,this.maxValue=null!=t.maxValue?t.maxValue:this.defaultMaxValue,this.axis=null!=t.axis?t.axis:this.defaultAxis}apply(t){return e.tidy((()=>{const n=Bi(t,this.axis),s=e.clipByValue(n,0,this.maxValue);return e.mul(t,e.div(s,e.add(fi(),n)))}))}getConfig(){return{maxValue:this.maxValue,axis:this.axis}}}Wi.className="MaxNorm",e.serialization.registerClass(Wi);class Ui extends Pi{constructor(t){super(),this.defaultAxis=0,this.axis=null!=t.axis?t.axis:this.defaultAxis}apply(t){return e.tidy((()=>e.div(t,e.add(fi(),Bi(t,this.axis)))))}getConfig(){return{axis:this.axis}}}Ui.className="UnitNorm",e.serialization.registerClass(Ui);class ji extends Pi{apply(t){return e.relu(t)}}ji.className="NonNeg",e.serialization.registerClass(ji);class Vi extends Pi{constructor(t){super(),this.defaultMinValue=0,this.defaultMaxValue=1,this.defaultRate=1,this.defaultAxis=0,this.minValue=null!=t.minValue?t.minValue:this.defaultMinValue,this.maxValue=null!=t.maxValue?t.maxValue:this.defaultMaxValue,this.rate=null!=t.rate?t.rate:this.defaultRate,this.axis=null!=t.axis?t.axis:this.defaultAxis}apply(t){return e.tidy((()=>{const n=Bi(t,this.axis),s=e.add(e.mul(this.rate,e.clipByValue(n,this.minValue,this.maxValue)),e.mul(1-this.rate,n));return e.mul(t,e.div(s,e.add(fi(),n)))}))}getConfig(){return{minValue:this.minValue,maxValue:this.maxValue,rate:this.rate,axis:this.axis}}}Vi.className="MinMaxNorm",e.serialization.registerClass(Vi);const Ki={maxNorm:"MaxNorm",minMaxNorm:"MinMaxNorm",nonNeg:"NonNeg",unitNorm:"UnitNorm"};function qi(t){return Ci(t)}function Gi(t,n={}){return $i(t,e.serialization.SerializationMap.getMap().classNameMap,n,"constraint")}function Hi(t){if(null==t)return null;if("string"==typeof t){return Gi({className:t in Ki?Ki[t]:t,config:{}})}return t instanceof Pi?t:Gi(t)}var Ji=Object.freeze({__proto__:null,maxNorm:function(t){return new Wi(t)},unitNorm:function(t){return new Ui(t)},nonNeg:function(){return new ji},minMaxNorm:function(t){return new Vi(t)}});const Zi=["channelsFirst","channelsLast"],Yi=["nearest","bilinear"],Xi=["valid","same","causal"],Qi=["max","avg"],tr=["sum","mul","concat","ave"],er=new Map;function nr(t){Li(Zi,"DataFormat",t)}function sr(t){Li(Xi,"PaddingMode",t)}function ir(t){Li(Qi,"PoolMode",t)}const rr=[];function ar(t,e){rr.push(t);try{const t=e();return rr.pop(),t}catch(t){throw rr.pop(),t}}function or(t){if(!hr(t))throw new Error("Not a valid tensor name: '"+t+"'");return(0===rr.length?"":rr.join("/")+"/")+t}function lr(t){if(!hr(t))throw new Error("Not a valid tensor name: '"+t+"'");er.has(t)||er.set(t,0);const e=er.get(t);if(er.set(t,er.get(t)+1),e>0){const n=`${t}_${e}`;return er.set(n,1),n}return t}const ur=new RegExp(/^[A-Za-z0-9][-A-Za-z0-9\._\/]*$/);function hr(t){return!!t.match(ur)}function cr(t,e,n){null==e&&(e=0),null==n&&(n=t.length);let s=1;for(let i=e;i<n;++i)s*=t[i];return s}function pr(t){if(0===t.length)return Number.NaN;let e=Number.POSITIVE_INFINITY;for(let n=0;n<t.length;n++){const s=t[n];s<e&&(e=s)}return e}function dr(t){if(0===t.length)return Number.NaN;let e=Number.NEGATIVE_INFINITY;for(let n=0;n<t.length;n++){const s=t[n];s>e&&(e=s)}return e}function fr(t,e){if(e<t)throw new yi(`end (${e}) < begin (${t}) is forbidden.`);const n=[];for(let s=t;s<e;++s)n.push(s);return n}function gr(t,n){return e.cast(t,n)}function mr(t,n=-1){const s=t.shape.slice();return n<0&&(n=s.length+n+1),s.splice(n,0,1),e.reshape(t,s)}function yr(t,n,s){return e.tidy((()=>{switch(t.rank){case 1:return e.slice1d(t,n,s);case 2:return e.slice2d(t,[n,0],[s,t.shape[1]]);case 3:return e.slice3d(t,[n,0,0],[s,t.shape[1],t.shape[2]]);case 4:return e.slice4d(t,[n,0,0,0],[s,t.shape[1],t.shape[2],t.shape[3]]);case 5:return e.slice(t,[n,0,0,0,0],[s,t.shape[1],t.shape[2],t.shape[3],t.shape[4]]);case 6:return e.slice(t,[n,0,0,0,0,0],[s,t.shape[1],t.shape[2],t.shape[3],t.shape[4],t.shape[5]]);default:throw new yi(`sliceAlongFirstAxis() received an unsupported tensor rank: ${t.rank}`)}}))}function br(t,n,s){return e.tidy((()=>{switch(t.rank){case 1:return e.slice1d(t,n,s);case 2:return e.slice2d(t,[0,n],[t.shape[0],s]);case 3:return e.slice3d(t,[0,0,n],[t.shape[0],t.shape[1],s]);case 4:return e.slice4d(t,[0,0,0,n],[t.shape[0],t.shape[1],t.shape[2],s]);default:throw new yi(`sliceAlongLastAxis() received an unsupported tensor rank: ${t.rank}`)}}))}function wr(t,n,s,i){return e.tidy((()=>{switch(t.rank){case 1:return e.slice1d(t,n,s);case 2:switch(i){case 1:return yr(t,n,s);case 2:return br(t,n,s);default:throw new yi(`The axis is not within the rank of the tensor ${i}`)}case 3:switch(i){case 1:return yr(t,n,s);case 2:return e.slice3d(t,[0,n,0],[t.shape[0],s,t.shape[2]]);case 3:return br(t,n,s);default:throw new yi(`The axis is not within the rank of the tensor ${i}`)}case 4:switch(i){case 1:return yr(t,n,s);case 2:return e.slice4d(t,[0,n,0,0],[t.shape[0],s,t.shape[2],t.shape[3]]);case 3:return e.slice4d(t,[0,0,n,0],[t.shape[0],t.shape[1],s,t.shape[3]]);case 4:return br(t,n,s);default:throw new yi(`The axis is not within the rank of the tensor ${i}`)}default:throw new yi(`sliceAlongLastAxis() received an unsupported tensor rank: ${t.rank}`)}}))}function kr(t,n=-1){let s;return n<0&&(s=t[0].rank,n=0!==s?s:0),n===t[0].rank&&(n=-1),e.concat(t,n)}function vr(t,n){switch(t.rank){case 1:return e.concat1d([t,n]);case 2:return e.concat2d([t,n],0);case 3:return e.concat3d([t,n],0);case 4:return e.concat4d([t,n],0);default:throw new yi(`concatAlongFirstAxis() received an unsupported tensor rank: ${t.rank}`)}}function Sr(t,n){if(Array.isArray(n)||(n=[n]),t.rank!==n.length)throw new yi(`The length of input n (${n.length}) does not match the number of dimensions in input x (${t.rank})`);return e.tile(t,n)}function xr(t,n=0,s=1,i,r){return e.randomNormal(t,n,s,i,r)}function Nr(t,n,s,i){if(t.rank<2||n.rank<2)throw new bi(`dot requires both inputs to be rank >= 2 but got x shape = ${t.shape} and y shape = ${n.shape}`);if(n.rank>=3){if(t.shape.slice(-1)[0]!==n.shape.slice(-2)[0])throw new bi(`If rank y >= 3, then the second last dim of y must equal the last dim of x but got x shape = ${t.shape} and  y shape = ${n.shape}`)}if(2===t.rank&&2===n.rank){const r=!1,a=!1;return e.fused.matMul({a:t,b:n,transposeA:r,transposeB:a,bias:i?Ar(t.rank,i,"channelsLast"):null,activation:s})}{const r=t.shape.slice(),a=r.pop();t=e.reshape(t,[-1,a]);const o=n.shape.slice(),l=o.pop(),u=o.pop(),h=[...o,l],c=Array.from({length:n.rank},((t,e)=>0===e?n.rank-2:e<=n.rank-2?e-1:e));n=e.reshape(e.transpose(n,c),[u,-1]);const p=[...r,...h],d=!1,f=!1;return e.reshape(e.fused.matMul({a:t,b:n,transposeA:d,transposeB:f,bias:i?Ar(t.rank,i,"channelsLast"):null,activation:s}),p)}}function zr(t,n,s){return e.tidy((()=>(n=Array.isArray(n)?e.tensor1d(n,"int32"):e.cast(n,"int32"),e.gather(t,n,s))))}function Ir(t){return e.mul(t,t)}function Ar(t,n,s){const i=n.shape;if(1!==n.rank&&n.rank!==t)throw new yi(`Unexpected bias dimensions: ${n.rank}; expected it to be 1 or ${t}`);if(5===t){if("channelsFirst"===s)return 1===i.length?e.reshape(n,[1,i[0],1,1,1]):e.reshape(n,[1,i[3],i[0],i[1],i[2]]);if("channelsLast"===s)return 1===i.length?e.reshape(n,[1,1,1,1,i[0]]):e.reshape(n,[1].concat(i))}else if(4===t){if("channelsFirst"===s)return 1===i.length?e.reshape(n,[1,i[0],1,1]):e.reshape(n,[1,i[2],i[0],i[1]]);if("channelsLast"===s)return 1===i.length?e.reshape(n,[1,1,1,i[0]]):e.reshape(n,[1].concat(i))}else if(3===t){if("channelsFirst"===s)return 1===i.length?e.reshape(n,[1,i[0],1]):e.reshape(n,[1,i[1],i[0]]);if("channelsLast"===s)return 1===i.length?e.reshape(n,[1,1,i[0]]):e.reshape(n,[1].concat(i))}else if(t<3)return n;throw new yi(`Unsupported input rank by biasAdd: ${n.rank}`)}function Cr(t,n,s){return e.tidy((()=>(null==s&&(s="channelsLast"),nr(s),e.add(t,Ar(t.rank,n,s)))))}function Tr(t,n,s,i){return e.tidy((()=>e.dropout(t,n,s,i)))}function $r(t,e,n=!1){return n?t():e()}const Er=["fanIn","fanOut","fanAvg"],Fr=["normal","uniform","truncatedNormal"];class Dr extends e.serialization.Serializable{fromConfigUsesCustomObjects(){return!1}getConfig(){return{}}}class Lr extends Dr{apply(t,n){return e.zeros(t,n)}}Lr.className="Zeros",e.serialization.registerClass(Lr);class _r extends Dr{apply(t,n){return e.ones(t,n)}}_r.className="Ones",e.serialization.registerClass(_r);class Rr extends Dr{constructor(t){if(super(),"object"!=typeof t)throw new yi(`Expected argument of type ConstantConfig but got ${t}`);if(void 0===t.value)throw new yi(`config must have value set but got ${t}`);this.value=t.value}apply(t,n){return e.tidy((()=>e.mul(e.scalar(this.value),e.ones(t,n))))}getConfig(){return{value:this.value}}}Rr.className="Constant",e.serialization.registerClass(Rr);class Mr extends Dr{constructor(t){super(),this.DEFAULT_MINVAL=-.05,this.DEFAULT_MAXVAL=.05,this.minval=t.minval||this.DEFAULT_MINVAL,this.maxval=t.maxval||this.DEFAULT_MAXVAL,this.seed=t.seed}apply(t,n){return e.randomUniform(t,this.minval,this.maxval,n)}getConfig(){return{minval:this.minval,maxval:this.maxval,seed:this.seed}}}Mr.className="RandomUniform",e.serialization.registerClass(Mr);class Or extends Dr{constructor(t){super(),this.DEFAULT_MEAN=0,this.DEFAULT_STDDEV=.05,this.mean=t.mean||this.DEFAULT_MEAN,this.stddev=t.stddev||this.DEFAULT_STDDEV,this.seed=t.seed}apply(t,e){if("float32"!==(e=e||"float32")&&"int32"!==e)throw new bi(`randomNormal does not support dType ${e}.`);return xr(t,this.mean,this.stddev,e,this.seed)}getConfig(){return{mean:this.mean,stddev:this.stddev,seed:this.seed}}}Or.className="RandomNormal",e.serialization.registerClass(Or);class Br extends Dr{constructor(t){super(),this.DEFAULT_MEAN=0,this.DEFAULT_STDDEV=.05,this.mean=t.mean||this.DEFAULT_MEAN,this.stddev=t.stddev||this.DEFAULT_STDDEV,this.seed=t.seed}apply(t,n){if("float32"!==(n=n||"float32")&&"int32"!==n)throw new bi(`truncatedNormal does not support dType ${n}.`);return e.truncatedNormal(t,this.mean,this.stddev,n,this.seed)}getConfig(){return{mean:this.mean,stddev:this.stddev,seed:this.seed}}}Br.className="TruncatedNormal",e.serialization.registerClass(Br);class Pr extends Dr{constructor(t){super(),this.gain=null!=t.gain?t.gain:1}apply(t,n){return e.tidy((()=>{if(2!==t.length||t[0]!==t[1])throw new yi("Identity matrix initializer can only be used for 2D square matrices.");return e.mul(this.gain,e.eye(t[0]))}))}getConfig(){return{gain:this.gain}}}Pr.className="Identity",e.serialization.registerClass(Pr);class Wr extends Dr{constructor(t){if(super(),t.scale<0)throw new yi(`scale must be a positive float. Got: ${t.scale}`);var e;this.scale=null==t.scale?1:t.scale,this.mode=null==t.mode?"fanIn":t.mode,e=this.mode,Li(Er,"FanMode",e),this.distribution=null==t.distribution?"normal":t.distribution,function(t){Li(Fr,"Distribution",t)}(this.distribution),this.seed=t.seed}apply(t,n){const s=function(t,e="channelsLast"){let n,s;if(nr(e),2===t.length)n=t[0],s=t[1];else if(-1!==[3,4,5].indexOf(t.length)){if("channelsFirst"===e){const e=cr(t,2);n=t[1]*e,s=t[0]*e}else if("channelsLast"===e){const e=cr(t,0,t.length-2);n=t[t.length-2]*e,s=t[t.length-1]*e}}else{const e=cr(t);n=Math.sqrt(e),s=Math.sqrt(e)}return[n,s]}(t),i=s[0],r=s[1];let a=this.scale;if("fanIn"===this.mode?a/=Math.max(1,i):"fanOut"===this.mode?a/=Math.max(1,r):a/=Math.max(1,(i+r)/2),"normal"===this.distribution){const s=Math.sqrt(a);if("float32"!==(n=n||"float32")&&"int32"!==n)throw new bi(`${this.getClassName()} does not support dType ${n}.`);return e.truncatedNormal(t,0,s,n,this.seed)}{const s=Math.sqrt(3*a);return e.randomUniform(t,-s,s,n)}}getConfig(){return{scale:this.scale,mode:this.mode,distribution:this.distribution,seed:this.seed}}}Wr.className="VarianceScaling",e.serialization.registerClass(Wr);class Ur extends Wr{constructor(t){super({scale:1,mode:"fanAvg",distribution:"uniform",seed:null==t?null:t.seed})}getClassName(){return Wr.className}}Ur.className="GlorotUniform",e.serialization.registerClass(Ur);class jr extends Wr{constructor(t){super({scale:1,mode:"fanAvg",distribution:"normal",seed:null==t?null:t.seed})}getClassName(){return Wr.className}}jr.className="GlorotNormal",e.serialization.registerClass(jr);class Vr extends Wr{constructor(t){super({scale:2,mode:"fanIn",distribution:"normal",seed:null==t?null:t.seed})}getClassName(){return Wr.className}}Vr.className="HeNormal",e.serialization.registerClass(Vr);class Kr extends Wr{constructor(t){super({scale:2,mode:"fanIn",distribution:"uniform",seed:null==t?null:t.seed})}getClassName(){return Wr.className}}Kr.className="HeUniform",e.serialization.registerClass(Kr);class qr extends Wr{constructor(t){super({scale:1,mode:"fanIn",distribution:"normal",seed:null==t?null:t.seed})}getClassName(){return Wr.className}}qr.className="LeCunNormal",e.serialization.registerClass(qr);class Gr extends Wr{constructor(t){super({scale:1,mode:"fanIn",distribution:"uniform",seed:null==t?null:t.seed})}getClassName(){return Wr.className}}Gr.className="LeCunNormal",e.serialization.registerClass(Gr);class Hr extends Dr{constructor(t){if(super(),this.DEFAULT_GAIN=1,this.gain=null==t.gain?this.DEFAULT_GAIN:t.gain,this.seed=t.seed,null!=this.seed)throw new bi("Random seed is not implemented for Orthogonal Initializer yet.")}apply(t,n){return e.tidy((()=>{if(t.length<2)throw new bi("Shape must be at least 2D.");t[0]*t[1]>2e3&&console.warn(`Orthogonal initializer is being called on a matrix with more than 2000 (${t[0]*t[1]}) elements: Slowness may result.`);const n=xr(t[0]>t[1]?[t[1],t[0]]:t,0,1,"float32");let s=e.linalg.gramSchmidt(n);return t[0]>t[1]&&(s=e.transpose(s)),e.mul(this.gain,s)}))}getConfig(){return{gain:this.gain,seed:this.seed}}}Hr.className="Orthogonal",e.serialization.registerClass(Hr);const Jr={constant:"Constant",glorotNormal:"GlorotNormal",glorotUniform:"GlorotUniform",heNormal:"HeNormal",heUniform:"HeUniform",identity:"Identity",leCunNormal:"LeCunNormal",leCunUniform:"LeCunUniform",ones:"Ones",orthogonal:"Orthogonal",randomNormal:"RandomNormal",randomUniform:"RandomUniform",truncatedNormal:"TruncatedNormal",varianceScaling:"VarianceScaling",zeros:"Zeros"};function Zr(t,n={}){return $i(t,e.serialization.SerializationMap.getMap().classNameMap,n,"initializer")}function Yr(t){return Ci(t)}function Xr(t){if("string"==typeof t){const e=t in Jr?Jr[t]:t;if("GlorotNormal"===e)return new jr;if("GlorotUniform"===e)return new Ur;if("HeNormal"===e)return new Vr;if("HeUniform"===e)return new Kr;if("LeCunNormal"===e)return new qr;if("LeCunUniform"===e)return new Gr;{const t={};return t.className=e,t.config={},Zr(t)}}return t instanceof Dr?t:Zr(t)}var Qr=Object.freeze({__proto__:null,zeros:function(){return new Lr},ones:function(){return new _r},constant:function(t){return new Rr(t)},randomUniform:function(t){return new Mr(t)},randomNormal:function(t){return new Or(t)},truncatedNormal:function(t){return new Br(t)},identity:function(t){return new Pr(t)},varianceScaling:function(t){return new Wr(t)},glorotUniform:function(t){return new Ur(t)},glorotNormal:function(t){return new jr(t)},heNormal:function(t){return new Vr(t)},heUniform:function(t){return new Kr(t)},leCunNormal:function(t){return new qr(t)},leCunUniform:function(t){return new Gr(t)},orthogonal:function(t){return new Hr(t)}});let ta=0;function ea(){return ta++}const na={};function sa(t=""){return t in na||(na[t]=0),na[t]+=1,t+na[t].toString()}function ia(t){return Array.isArray(t)&&Array.isArray(t[0])}function ra(t){return 0===t.length?[]:Array.isArray(t[0])?t:[t]}function aa(t){let e;if(Array.isArray(t)){if(1!==t.length)throw new yi(`Expected Tensor length to be 1; got ${t.length}`);e=t[0]}else e=t;return e}function oa(t){if(Array.isArray(t)&&Array.isArray(t[0])){if(1===t.length)return(t=t)[0];throw new yi(`Expected exactly 1 Shape; got ${t.length}`)}return t}function la(t){let e=0;for(const n of t)0===n.shape.length?e+=1:e+=n.shape.reduce(((t,e)=>t*e));return e}const ua="Variable";class ha{constructor(t,n="float32",s="Variable",i=!0,r=null){this.dtype=null==n?"float32":n,this.shape=t.shape,this.id=ea(),s=null==s?ua:s,this.originalName=or(s),this.name=lr(this.originalName),this.trainable_=i,this.constraint=r,this.val=e.variable(t,this.trainable_,this.name,this.dtype)}read(){return this.assertNotDisposed(),this.val}write(t){return this.assertNotDisposed(),function(t,e){if(t.shape.toString()!==e.shape.toString())throw new Error("Shape mismatch: "+JSON.stringify(t.shape)+" vs. "+JSON.stringify(e.shape))}(this.val,t),this.val.id!==t.id&&(this.val.assign(t),null!=this.constraint&&this.val.assign(this.constraint.apply(this.val))),this}dispose(){this.assertNotDisposed(),this.val.dispose()}assertNotDisposed(){if(this.val.isDisposed)throw new Error(`LayersVariable ${this.name} is already disposed.`)}get trainable(){return this.trainable_}set trainable(t){this.trainable_=t,this.val.trainable=t}}function ca(t){return t.map((t=>t.read()))}function pa(t){t.forEach((t=>{t[0].write(t[1])}))}class da{constructor(t){this.dtype=t.dtype,this.shape=t.shape,null!=t.shape?this.ndim=t.shape.length:this.ndim=t.ndim,this.maxNDim=t.maxNDim,this.minNDim=t.minNDim,this.axes=t.axes||{}}}class fa{constructor(t,e,n,s,i,r,a){this.dtype=t,this.shape=e,this.sourceLayer=n,this.inputs=s,this.callArgs=i,this.outputTensorIndex=a,this.id=ea(),null!=r&&(this.originalName=or(r),this.name=lr(this.originalName)),this.rank=e.length}}let ga=0;class ma{constructor(t,e){this.callArgs=e,this.id=ga++,this.outboundLayer=t.outboundLayer,this.inboundLayers=t.inboundLayers,this.nodeIndices=t.nodeIndices,this.tensorIndices=t.tensorIndices,this.inputTensors=t.inputTensors,this.outputTensors=t.outputTensors,this.inputMasks=t.inputMasks,this.outputMasks=t.outputMasks,this.inputShapes=t.inputShapes,this.outputShapes=t.outputShapes;for(const e of t.inboundLayers)null!=e&&e.outboundNodes.push(this);t.outboundLayer.inboundNodes.push(this)}getConfig(){const t=[];for(const e of this.inboundLayers)null!=e?t.push(e.name):t.push(null);return{outboundLayer:this.outboundLayer?this.outboundLayer.name:null,inboundLayers:t,nodeIndices:this.nodeIndices,tensorIndices:this.tensorIndices}}}let ya=0;class ba extends e.serialization.Serializable{constructor(t={}){super(),this._callHook=null,this._addedWeightNames=[],this._stateful=!1,this.id=ya++,this.activityRegularizer=null,this.inputSpec=null,this.supportsMasking=!1,this._trainableWeights=[],this._nonTrainableWeights=[],this._losses=[],this._updates=[],this._built=!1,this.inboundNodes=[],this.outboundNodes=[];let e=t.name;if(!e){const t=this.getClassName();e=zi(t)+"_"+sa(t)}if(this.name=e,this.trainable_=null==t.trainable||t.trainable,null!=t.inputShape||null!=t.batchInputShape){let e;if(null!=t.batchInputShape)e=t.batchInputShape;else if(null!=t.inputShape){let n=null;null!=t.batchSize&&(n=t.batchSize),e=[n].concat(t.inputShape)}this.batchInputShape=e;let n=t.dtype;null==n&&(n=t.inputDType),null==n&&(n="float32"),this.dtype=n}null!=t.weights?this.initialWeights=t.weights:this.initialWeights=null,this._refCount=null,this.fastWeightInitDuringBuild=!1}static nodeKey(t,e){return t.name+"_ib-"+e.toString()}getNodeAtIndex(t,e){if(0===this.inboundNodes.length)throw new mi(`The layer has never been called and thus has no defined ${e}.`);if(this.inboundNodes.length<=t)throw new yi(`Asked to get ${e} at node ${t}, but the layer has only ${this.inboundNodes.length} inbound nodes.`);return this.inboundNodes[t]}getInputAt(t){return xi(this.getNodeAtIndex(t,"input").inputTensors)}getOutputAt(t){return xi(this.getNodeAtIndex(t,"output").outputTensors)}get input(){if(this.inboundNodes.length>1)throw new gi(`Layer ${this.name} has multiple inbound nodes, hence the notion of "layer input" is ill-defined. Use \`getInputAt(nodeIndex)\` instead.`);if(0===this.inboundNodes.length)throw new gi(`Layer ${this.name} is not connected, no input to return.`);return xi(this.getNodeAtIndex(0,"input").inputTensors)}get output(){if(0===this.inboundNodes.length)throw new gi(`Layer ${this.name} has no inbound nodes.`);if(this.inboundNodes.length>1)throw new gi(`Layer ${this.name} has multiple inbound nodes, hence the notion of "layer output" is ill-defined. Use \`getOutputAt(nodeIndex)\` instead.`);return xi(this.getNodeAtIndex(0,"output").outputTensors)}get losses(){return this._losses}calculateLosses(){return this.losses.map((t=>t()))}get updates(){return this._updates}get built(){return this._built}set built(t){this._built=t}get trainable(){return this.trainable_}set trainable(t){this._trainableWeights.forEach((e=>e.trainable=t)),this.trainable_=t}get trainableWeights(){return this.trainable_?this._trainableWeights.filter((t=>t.trainable)):[]}set trainableWeights(t){this._trainableWeights=t}get nonTrainableWeights(){return this.trainable?this._trainableWeights.filter((t=>!t.trainable)).concat(this._nonTrainableWeights):this._trainableWeights.concat(this._nonTrainableWeights)}set nonTrainableWeights(t){this._nonTrainableWeights=t}get weights(){return this.trainableWeights.concat(this.nonTrainableWeights)}get stateful(){return this._stateful}resetStates(){if(!this.stateful)throw new Error("Cannot call the resetStates() method of a non-stateful Layer object.")}assertInputCompatibility(t){if(t=Ni(t),null==this.inputSpec||0===this.inputSpec.length)return;const e=Ni(this.inputSpec);if(t.length!==e.length)throw new yi(`Layer ${this.name} expects ${e.length} inputs, but it received ${t.length} input tensors. Input received: ${t}`);for(let n=0;n<t.length;n++){const s=t[n],i=e[n];if(null==i)continue;const r=s.rank;if(null!=i.ndim&&r!==i.ndim)throw new yi(`Input ${n} is incompatible with layer ${this.name}: expected ndim=${i.ndim}, found ndim=${r}`);if(null!=i.maxNDim&&r>i.maxNDim)throw new yi(`Input ${n} is incompatible with layer ${this.name}: expected max_ndim=${i.maxNDim}, found ndim=${r}`);if(null!=i.minNDim&&r<i.minNDim)throw new yi(`Input ${n} is incompatible with layer ${this.name}: expected min_ndim=${i.minNDim}, found ndim=${r}.`);if(null!=i.dtype&&s.dtype!==i.dtype)throw new yi(`Input ${n} is incompatible with layer ${this.name} : expected dtype=${i.dtype}, found dtype=${s.dtype}.`);if(i.axes){const t=s.shape;for(const e in i.axes){const s=Number(e),r=i.axes[e],a=s>=0?t[s]:t[t.length+s];if(null!=r&&-1===[r,null].indexOf(a))throw new yi(`Input ${n} is incompatible with layer ${this.name}: expected axis ${s} of input shape to have value ${r} but got shape ${t}.`)}}if(null!=i.shape)for(let t=0;t<i.shape.length;++t){const e=i.shape[t],r=s.shape[t];if(null!=e&&null!=r&&e!==r)throw new yi(`Input ${n} is incompatible with layer ${this.name}: expected shape=${i.shape}, found shape=${s.shape}.`)}}}call(t,e){return t}invokeCallHook(t,e){null!=this._callHook&&this._callHook(t,e)}setCallHook(t){this._callHook=t}clearCallHook(){this._callHook=null}apply(t,e){e=e||{},this.assertNotDisposed();const n=Ni(t);let s=!0;for(const t of n)if(!(t instanceof fa)){s=!1;break}let i=!0;for(const t of n)if(t instanceof fa){i=!1;break}if(s===i)throw new yi("Arguments to apply() must be all SymbolicTensors or all Tensors");return ar(this.name,(()=>{if(!this.built){this.assertInputCompatibility(t);const e=[];for(const n of Ni(t))e.push(n.shape);this.build(xi(e)),this.built=!0,this.initialWeights&&this.setWeights(this.initialWeights),null===this._refCount&&i&&(this._refCount=1)}if(this.assertInputCompatibility(t),i){let s=this.call(t,e);const i=Ni(s),r=[];for(let t of i)-1!==n.indexOf(t)&&(t=t.clone()),r.push(t);if(s=xi(r),null!=this.activityRegularizer)throw new bi("Layer invocation in the presence of activity regularizer(s) is not supported yet.");return s}{const n=function(t){t=Ni(t);const e=[];for(const n of t)e.push(n.shape);return xi(e)}(t),s=this.computeOutputShape(n);let i;const r="float32";if(this.warnOnIncompatibleInputShape(Array.isArray(t)?n[0]:n),i=null!=s&&s.length>0&&Array.isArray(s[0])?s.map(((n,s)=>new fa(r,n,this,Ni(t),e,this.name,s))):new fa(r,s,this,Ni(t),e,this.name),this.addInboundNode(t,i,null,null,n,s,e),this._refCount++,null!=this.activityRegularizer)throw new bi("Layer invocation in the presence of activity regularizer(s) is not supported yet.");return i}}))}warnOnIncompatibleInputShape(t){if(null!=this.batchInputShape)if(t.length!==this.batchInputShape.length)console.warn(`The rank of the input tensor provided (shape: ${JSON.stringify(t)}) does not match that of the batchInputShape (${JSON.stringify(this.batchInputShape)}) of the layer ${this.name}`);else{let e=!1;this.batchInputShape.forEach(((n,s)=>{null!=n&&null!=t[s]&&t[s]!==n&&(e=!0)})),e&&console.warn(`The shape of the input tensor (${JSON.stringify(t)}) does not match the expectation of layer ${this.name}: ${JSON.stringify(this.batchInputShape)}`)}}get outputShape(){if(null==this.inboundNodes||0===this.inboundNodes.length)throw new gi(`The layer ${this.name} has never been called and thus has no defined output shape.`);const t=[];for(const e of this.inboundNodes){const n=JSON.stringify(e.outputShapes);-1===t.indexOf(n)&&t.push(n)}if(1===t.length){const t=this.inboundNodes[0].outputShapes;return Array.isArray(t)&&Array.isArray(t[0])&&1===t.length?t[0]:t}throw new gi(`The layer ${this.name} has multiple inbound nodes with different output shapes. Hence the notion of "output shape" is ill-defined for the layer.`)}countParams(){if(!this.built)throw new mi(`You tried to call countParams() on ${this.name}, but the layer is not built yet. Build it first by calling build(batchInputShape).`);return la(this.weights)}build(t){this.built=!0}getWeights(t=!1){return ca(t?this.trainableWeights:this.weights)}setWeights(t){e.tidy((()=>{const n=this.weights;if(n.length!==t.length)throw new yi(`You called setWeights(weights) on layer "${this.name}" with a weight list of length ${t.length}, but the layer was expecting ${n.length} weights. Provided weights: ${t}...`);if(0===n.length)return;const s=[],i=ca(n);for(let r=0;r<i.length;++r){const a=i[r],o=n[r],l=t[r];if(!e.util.arraysEqual(a.shape,l.shape))throw new yi(`Layer weight shape ${a.shape} not compatible with provided weight shape ${l.shape}`);s.push([o,l])}pa(s)}))}addWeight(t,e,n,s,i,r,a){if(-1!==this._addedWeightNames.indexOf(t))throw new yi(`Duplicate weight name ${t} for layer ${this.name}`);this._addedWeightNames.push(t),null==n&&(n="float32"),this.fastWeightInitDuringBuild&&(s=Xr("zeros"));const o=s.apply(e,n),l=new ha(o,n,t,r,a);return o.dispose(),null!=i&&this.addLoss((()=>i.apply(l.read()))),null==r&&(r=!0),r?this._trainableWeights.push(l):this._nonTrainableWeights.push(l),l}setFastWeightInitDuringBuild(t){this.fastWeightInitDuringBuild=t}addLoss(t){null==t||Array.isArray(t)&&0===t.length||(t=Ni(t),void 0!==this._losses&&null!==this._losses&&this.losses.push(...t))}computeOutputShape(t){return t}computeMask(t,e){if(!this.supportsMasking){if(null!=e){if(!Array.isArray(e))throw new TypeError(`Layer ${this.name} does not support masking, but was passed an inputMask.`);e.forEach((t=>{if(null!=t)throw new TypeError(`Layer ${this.name} does not support masking, but was passed an inputMask.`)}))}return null}return e}addInboundNode(t,e,n,s,i,r,a=null){const o=Ni(t);e=Ni(e),n=Ni(n),s=Ni(s),i=ra(i),r=ra(r);const l=[],u=[],h=[];for(const t of o)l.push(t.sourceLayer),u.push(t.nodeIndex),h.push(t.tensorIndex);new ma({outboundLayer:this,inboundLayers:l,nodeIndices:u,tensorIndices:h,inputTensors:o,outputTensors:e,inputMasks:n,outputMasks:s,inputShapes:i,outputShapes:r},a);for(let t=0;t<e.length;t++)e[t].sourceLayer=this,e[t].nodeIndex=this.inboundNodes.length-1,e[t].tensorIndex=t}getConfig(){const t={name:this.name,trainable:this.trainable};return null!=this.batchInputShape&&(t.batchInputShape=this.batchInputShape),null!=this.dtype&&(t.dtype=this.dtype),t}disposeWeights(){return this.weights.forEach((t=>t.dispose())),this.weights.length}assertNotDisposed(){if(0===this._refCount)throw new Error(`Layer '${this.name}' is already disposed.`)}dispose(){if(!this.built)throw new Error(`Cannot dispose Layer ${this.name} because it has not been built yet.`);if(null===this._refCount)throw new Error(`Cannot dispose Layer ${this.name} because it has not been used yet.`);this.assertNotDisposed();let t=0;return 0==--this._refCount&&(t=this.disposeWeights()),{refCountAfterDispose:this._refCount,numDisposedVariables:t}}}function wa(t,e,n){if((null==e||null!=n&&n>0)&&(e=t.sourceLayer,n=t.nodeIndex),0===e.inboundNodes.length)return[t];{const t=e.inboundNodes[n];if(0===t.inboundLayers.length)return t.inputTensors;{const e=[];for(let n=0;n<t.inboundLayers.length;n++){const s=wa(t.inputTensors[n],t.inboundLayers[n],t.nodeIndices[n]);for(const t of s)-1===e.indexOf(t)&&e.push(t)}return e}}}class ka extends ba{constructor(t){if(super({dtype:t.dtype,name:null!=t.name?t.name:sa("input").toString()}),null==t.batchSize&&(t.batchSize=null),null==t.sparse&&(t.sparse=!1),this.trainable=!1,this.built=!0,this.sparse=t.sparse,null!=t.inputShape&&null!=t.batchInputShape)throw new yi("Only provide the inputShape OR batchInputShape argument to inputLayer, not both at the same time.");let e=t.batchInputShape;if(null==e){if(null==t.inputShape)throw new yi("An InputLayer should be passed either a `batchInputShape` or an `inputShape`.");e=[t.batchSize].concat(t.inputShape)}else if(null!=t.batchSize)throw new yi("Cannot specify batchSize if batchInputShape is specified when creating an InputLayer.");const n=t.dtype||"float32";this.batchInputShape=e,this.dtype=n,this.inputSpec=[{shape:e}];const s=new fa(this.dtype,this.batchInputShape,this,[],{},this.name);s.nodeIndex=0,s.tensorIndex=0,new ma({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:[s],outputTensors:[s],inputMasks:[null],outputMasks:[null],inputShapes:[e],outputShapes:[e]})}apply(t,e){throw new yi(`Cannot pass any input to an InputLayer's apply() method. InputLayer name: ${this.name}`)}dispose(){return{refCountAfterDispose:this._refCount,numDisposedVariables:0}}getConfig(){return{batchInputShape:this.batchInputShape,dtype:this.dtype,sparse:this.sparse,name:this.name}}}function va(t){if(null==t.batchShape&&null==t.shape)throw new Error("Please provide to Input either a `shape` or a `batchShape` argument. Note that `shape` does not include the batch dimension.");if(null!=t.batchShape&&null!=t.shape)throw new yi("Please provide either a `shape` or `batchShape` argument to Input, but not both.");let e=t.batchShape;null!=t.shape&&null==e&&(e=[null].concat(t.shape));let n=t.dtype;null==n&&(n="float32");return new ka({batchInputShape:e,name:t.name,dtype:n,sparse:t.sparse}).inboundNodes[0].outputTensors[0]}async function Sa(t){if(null==t)return;const n=[],s=[],i=[];for(const e in t){const r=t[e];if("number"!=typeof r){const t=r;n.push(t.data()),s.push(e),i.push(t)}}if(n.length>0){const r=await Promise.all(n);for(let e=0;e<r.length;++e)t[s[e]]=r[e][0];e.dispose(i)}}function xa(t){if(null!=t)for(const e in t){const n=t[e];"number"!=typeof n&&n.dispose()}}var Na;ka.className="InputLayer",e.serialization.registerClass(ka),function(t){t[t.SILENT=0]="SILENT",t[t.VERBOSE=1]="VERBOSE"}(Na||(Na={}));class za{constructor(){this.validationData=null}setParams(t){this.params=t}async onEpochBegin(t,e){}async onEpochEnd(t,e){}async onBatchBegin(t,e){}async onBatchEnd(t,e){}async onTrainBegin(t){}async onTrainEnd(t){}setModel(t){}}class Ia{constructor(t,e=10){null==t&&(t=[]),this.callbacks=t,this.queueLength=e}append(t){this.callbacks.push(t)}setParams(t){for(const e of this.callbacks)e.setParams(t)}setModel(t){for(const e of this.callbacks)e.setModel(t)}async onEpochBegin(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onEpochBegin(t,e)}async onEpochEnd(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onEpochEnd(t,e)}async onBatchBegin(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onBatchBegin(t,e)}async onBatchEnd(t,e){null==e&&(e={});for(const n of this.callbacks)await n.onBatchEnd(t,e)}async onTrainBegin(t){null==t&&(t={});for(const e of this.callbacks)await e.onTrainBegin(t)}async onTrainEnd(t){null==t&&(t={});for(const e of this.callbacks)await e.onTrainEnd(t)}}class Aa extends za{constructor(){super()}async onEpochBegin(t){this.seen=0,this.totals={}}async onBatchEnd(t,n){null==n&&(n={});const s=null==n.size?0:n.size;this.seen+=s;for(const t in n){const i=n[t];if("number"==typeof i)this.totals.hasOwnProperty(t)||(this.totals[t]=0),this.totals[t]=this.totals[t]+i*s;else{let n;t in this.totals?n=this.totals[t]:this.totals[t]=0;const r=e.tidy((()=>e.add(this.totals[t],e.mul(i,s))));this.totals[t]=r,null!=n&&n.dispose()}}}async onEpochEnd(t,n){if(null!=n)for(const t of this.params.metrics)null!=this.totals[t]&&("number"==typeof this.totals[t]?n[t]=this.totals[t]/this.seen:e.tidy((()=>{const s=e.mul(e.div(1,this.seen),this.totals[t]);n[t]=s,this.totals[t].dispose(),e.keep(n[t])})))}}class Ca extends za{async onTrainBegin(t){this.epoch=[],this.history={}}async onEpochEnd(t,e){null==e&&(e={}),this.epoch.push(t);for(const t in e)null==this.history[t]&&(this.history[t]=[]),this.history[t].push(e[t])}async syncData(){const t=[],e=[],n=[];for(const s in this.history){const i=this.history[s];for(let r=0;r<i.length;++r)if("number"!=typeof i[r]){const a=i[r];t.push(a.data()),e.push(s),n.push(r)}}const s=await Promise.all(t);for(let t=0;t<s.length;++t){this.history[e[t]][n[t]].dispose(),this.history[e[t]][n[t]]=s[t][0]}}}class Ta extends za{constructor(t,n){if(super(),this.currentEpoch=0,this.yieldEvery=n||"auto","auto"===this.yieldEvery&&(this.yieldEvery=125),"never"===this.yieldEvery&&null!=t.onYield)throw new Error("yieldEvery is `never` but you provided an `onYield` callback. Either change `yieldEvery` or remove the callback");e.util.isNumber(this.yieldEvery)&&(this.maybeWait=function(t,n){let s,i=e.util.now();return(...r)=>{const a=e.util.now();return a-i<n||(i=a,s=t(...r)),s}}(this.maybeWait.bind(this),this.yieldEvery)),this.trainBegin=t.onTrainBegin,this.trainEnd=t.onTrainEnd,this.epochBegin=t.onEpochBegin,this.epochEnd=t.onEpochEnd,this.batchBegin=t.onBatchBegin,this.batchEnd=t.onBatchEnd,this.yield=t.onYield}async maybeWait(t,n,s){const i=[];null!=this.yield&&(await Sa(s),i.push(this.yield(t,n,s))),i.push(e.nextFrame()),await Promise.all(i)}async onEpochBegin(t,e){this.currentEpoch=t,null!=this.epochBegin&&(await Sa(e),await this.epochBegin(t,e))}async onEpochEnd(t,n){const s=[];null!=this.epochEnd&&(await Sa(n),s.push(this.epochEnd(t,n))),"epoch"===this.yieldEvery&&s.push(e.nextFrame()),await Promise.all(s)}async onBatchBegin(t,e){null!=this.batchBegin&&(await Sa(e),await this.batchBegin(t,e))}async onBatchEnd(t,n){const s=[];null!=this.batchEnd&&(await Sa(n),s.push(this.batchEnd(t,n))),"batch"===this.yieldEvery?s.push(e.nextFrame()):e.util.isNumber(this.yieldEvery)&&s.push(this.maybeWait(this.currentEpoch,t,n)),await Promise.all(s)}async onTrainBegin(t){null!=this.trainBegin&&(await Sa(t),await this.trainBegin(t))}async onTrainEnd(t){null!=this.trainEnd&&(await Sa(t),await this.trainEnd(t))}}function $a(t,e){if(null==t&&(t={}),t instanceof za)return[t];if(Array.isArray(t)&&t[0]instanceof za)return t;return Ni(t).map((t=>new Ta(t,e)))}class Ea{constructor(){}static registerCallbackConstructor(t,n){e.util.assert(t>=0&&Number.isInteger(t),(()=>`Verbosity level is expected to be an integer >= 0, but got ${t}`)),Ea.checkForDuplicate(n),null==Ea.constructors[t]&&(Ea.constructors[t]=[]),Ea.constructors[t].push(n)}static checkForDuplicate(t){for(const e in Ea.constructors){Ea.constructors[+e].forEach((e=>{if(e===t)throw new yi("Duplicate callback constructor.")}))}}static clear(){Ea.constructors={}}static createCallbacks(t){const e=[];for(const n in Ea.constructors){const s=+n;t>=s&&e.push(...Ea.constructors[s])}return e.map((t=>new t))}}function Fa(t,e,n,s,i,r,a,o,l){const u=new Ca,h=[new Aa,...Ea.createCallbacks(e)];null!=t&&h.push(...t),h.push(u);const c=new Ia(h);return c.setParams({epochs:n,initialEpoch:s,samples:i,steps:r,batchSize:a,verbose:e,doValidation:o,metrics:l}),{callbackList:c,history:u}}function Da(t,n={},s=!1){return $i(t,e.serialization.SerializationMap.getMap().classNameMap,n,"layer",s)}function La(t,n){return e.tidy((()=>{"float32"!==t.dtype&&(t=e.cast(t,"float32"));const s=e.sum(Ir(t),n,!0),i=e.fill(s.shape,fi()),r=e.sqrt(e.maximum(s,i));return e.div(t,r)}))}function _a(t,n){return e.tidy((()=>e.mean(Ir(e.sub(n,t)),-1)))}function Ra(t,n){return e.tidy((()=>e.mean(e.abs(e.sub(n,t)),-1)))}function Ma(t,n){return e.tidy((()=>{const s=e.sub(t,n),i=e.clipByValue(e.abs(t),fi(),Number.MAX_VALUE),r=e.abs(e.div(s,i));return e.mul(100,e.mean(r,-1))}))}function Oa(t,n,s=!1){return e.tidy((()=>{if(s)n=e.softmax(n);else{const t=e.sum(n,n.shape.length-1,!0);n=e.div(n,t)}return n=e.clipByValue(n,fi(),1-fi()),e.neg(e.sum(e.mul(e.cast(t,"float32"),e.log(n)),n.shape.length-1))}))}function Ba(t,n,s=!1){return e.tidy((()=>{const i=e.cast(e.floor(function(t){const n=[cr(t.shape)];return e.reshape(t,n)}(t)),"int32"),r=(n=e.clipByValue(n,fi(),1-fi())).shape;return Oa(e.reshape(e.oneHot(i,r[r.length-1]),r),n,s)}))}function Pa(t,n){return e.tidy((()=>{let s;return s=e.clipByValue(n,fi(),1-fi()),s=e.log(e.div(s,e.sub(1,s))),e.mean(function(t,n){if(!e.util.arraysEqual(t.shape,n.shape))throw new yi(`logits and labels must have the same shape, but got shapes ${JSON.stringify(t.shape)} and ${JSON.stringify(n.shape)}`);return e.tidy((()=>{const s=e.relu(n),i=e.neg(e.abs(n));return e.add(e.sub(s,e.mul(n,t)),e.log1p(e.exp(i)))}))}(t,s),-1)}))}function Wa(t,n){return e.tidy((()=>{const s=La(t,-1),i=La(n,-1),r=e.mul(s,i);return e.neg(e.sum(r,-1))}))}Ea.constructors={};const Ua={meanSquaredError:_a,meanAbsoluteError:Ra,meanAbsolutePercentageError:Ma,meanSquaredLogarithmicError:function(t,n){return e.tidy((()=>{const s=e.clipByValue(n,fi(),Number.MAX_VALUE),i=e.log(e.add(1,s)),r=e.clipByValue(t,fi(),Number.MAX_VALUE),a=e.log(e.add(1,r));return e.mean(Ir(e.sub(i,a)),-1)}))},squaredHinge:function(t,n){return e.tidy((()=>{const s=e.maximum(0,e.sub(1,e.mul(t,n)));return e.mean(Ir(s),-1)}))},hinge:function(t,n){return e.tidy((()=>{const s=e.maximum(0,e.sub(1,e.mul(t,n)));return e.mean(s,-1)}))},categoricalHinge:function(t,n){return e.tidy((()=>{const s=e.sum(e.mul(t,n),-1),i=e.max(e.mul(e.sub(1,t),n),-1);return e.maximum(0,e.add(1,e.sub(i,s)))}))},logcosh:function(t,n){return e.tidy((()=>{const s=Math.log(2),i=e.sub(n,t),r=e.sub(e.add(i,e.softplus(e.mul(-2,i))),s);return e.mean(r,-1)}))},categoricalCrossentropy:Oa,sparseCategoricalCrossentropy:Ba,binaryCrossentropy:Pa,kullbackLeiblerDivergence:function(t,n){return e.tidy((()=>{const s=e.clipByValue(t,fi(),1),i=e.clipByValue(n,fi(),1);return e.sum(e.mul(t,e.log(e.div(s,i))),-1)}))},poisson:function(t,n){return e.tidy((()=>{const s=e.log(e.add(fi(),n));return e.mean(e.sub(n,e.mul(t,s)),-1)}))},cosineProximity:Wa};function ja(t){if("string"==typeof t){if(t in Ua)return Ua[t];let e=`Unknown loss ${t}`;throw t.toLowerCase().includes("softmaxcrossentropy")&&(e=`Unknown loss ${t}. Use "categoricalCrossentropy" as the string name for tf.losses.softmaxCrossEntropy`),new yi(e)}return t}function Va(t,n){return e.tidy((()=>{const s=e.mul(.5,e.onesLike(n)),i=gr(e.greater(n,s),t.dtype);return e.mean(e.equal(t,i),-1)}))}function Ka(t,n){return e.tidy((()=>gr(e.equal(e.argMax(t,-1),e.argMax(n,-1)),"float32")))}function qa(t,n){return e.tidy((()=>e.cast(e.sum(e.logicalAnd(e.equal(t,1),e.equal(n,1))),"float32")))}function Ga(t,n){return e.tidy((()=>{const s=qa(t,n),i=function(t,n){return e.tidy((()=>e.cast(e.sum(e.logicalAnd(e.equal(t,0),e.equal(n,1))),"float32")))}(t,n),r=e.add(s,i);return e.cast(e.where(e.greater(r,0),e.div(s,r),0),"float32")}))}function Ha(t,n){return e.tidy((()=>{const s=qa(t,n),i=function(t,n){return e.tidy((()=>e.cast(e.sum(e.logicalAnd(e.equal(t,1),e.equal(n,0))),"float32")))}(t,n),r=e.add(s,i);return e.cast(e.where(e.greater(r,0),e.div(s,r),0),"float32")}))}function Ja(t,e){return Pa(t,e)}function Za(t,n){return t.rank===n.rank&&(t=e.squeeze(t,[t.rank-1])),(n=e.argMax(n,-1)).dtype!==t.dtype&&(n=e.cast(n,t.dtype)),e.cast(e.equal(t,n),"float32")}const Ya=Oa,Xa=Ba,Qa={binaryAccuracy:Va,categoricalAccuracy:Ka,precision:Ga,categoricalCrossentropy:Ya,sparseCategoricalCrossentropy:Xa,mse:_a,MSE:_a,mae:Ra,MAE:Ra,mape:Ma,MAPE:Ma,cosine:Wa};function to(t){if("string"==typeof t&&t in Qa)return Qa[t];if("string"!=typeof t&&null!=t)return t;throw new yi(`Unknown metric ${t}`)}function eo(t){if(vi(null!==t,`Unknown LossOrMetricFn ${t}`),"string"==typeof t)return t;{let e;for(const n of Object.keys(Ua))if(Ua[n]===t){e=n;break}if(void 0!==e)return e;for(const n of Object.keys(Qa))if(Qa[n]===t){e=n;break}return void 0!==e?e:t.name}}const no=1048576;function so(t,e,n=!1){if(null==t||"object"!=typeof t||Object.getPrototypeOf(t)!==Object.prototype||!io(t))throw new Error("User-defined metadata is expected to be a JSON object, but is not.");if(n){const n=JSON.stringify(t);n.length>no&&console.warn(`User-defined metadata of model "${e}" is too large in size (length=${n.length} when serialized). It is not recommended to store such large objects in user-defined metadata. Please make sure its serialized length is <= 1048576.`)}}function io(t){if(null===t)return!0;if("object"==typeof t){if(Object.getPrototypeOf(t)===Object.prototype){const e=Object.keys(t);for(const n of e){if("string"!=typeof n)return!1;if(!io(t[n]))return!1}return!0}if(Array.isArray(t)){for(const e of t)if(!io(e))return!1;return!0}return!1}{const e=typeof t;return"string"===e||"number"===e||"boolean"===e}}function ro(t,e,n,s=console.log){const i=function(t){let e=!0;const n=[],s=[];for(const e in t.nodesByDepth)n.push(t.nodesByDepth[e]);for(const t of n){if(t.length>1||1===t.length&&t[0].inboundLayers.length>1){e=!1;break}s.push(...t)}if(e)for(const n of t.layers){let t=!1;for(const i of n.inboundNodes)if(-1!==s.indexOf(i)){if(t){e=!1;break}t=!0}if(!e)break}return e}(t),r=["Layer (type)","Output shape","Param #"];let a;if(i?(e=e||65,n=n||[.45,.85,1]):(e=e||98,n=n||[.33,.55,.67,1]),n[n.length-1]<=1&&(n=n.map((t=>Math.floor(e*t)))),!i){r.push("Receives inputs"),a=[];for(const e in t.nodesByDepth)a.push(...t.nodesByDepth[e])}s("_".repeat(e)),ao(r,n,s),s("=".repeat(e));const o=t.layers;for(let t=0;t<o.length;++t)i?oo(o[t],n,s):lo(o[t],n,a,s),s((t===o.length-1?"=":"_").repeat(e));t.checkTrainableWeightsConsistency();const l=function(t){let e;e=null!=t.collectedTrainableWeights?la(t.collectedTrainableWeights):la(t.trainableWeights);return e}(t),u=la(t.nonTrainableWeights);s(`Total params: ${l+u}`),s(`Trainable params: ${l}`),s(`Non-trainable params: ${u}`),s("_".repeat(e))}function ao(t,e,n=console.log){let s="";for(let n=0;n<t.length;++n)n>0&&(s=s.slice(0,s.length-1)+" "),s+=t[n],s=s.slice(0,e[n]),s+=" ".repeat(e[n]-s.length);n(s)}function oo(t,e,n){let s;try{s=JSON.stringify(t.outputShape)}catch(t){s="multiple"}ao([`${t.name} (${t.getClassName()})`,s,t.countParams().toString()],e,n)}function lo(t,e,n,s){let i;try{i=JSON.stringify(t.outputShape)}catch(t){i="multiple"}const r=[];for(const e of t.inboundNodes)if(!(null!=n&&n.length>0&&-1===n.indexOf(e)))for(let t=0;t<e.inboundLayers.length;++t){const n=e.inboundLayers[t].name,s=e.nodeIndices[t],i=e.tensorIndices[t];r.push(`${n}[${s}][${i}]`)}const a=t.name,o=t.getClassName(),l=0===r.length?"":r[0];ao([`${a} (${o})`,i,t.countParams().toString(),l],e,s);for(let t=1;t<r.length;++t)ao(["","","",r[t]],e,s)}function uo(t,e,n){return("inboundNodes"===t||"outputLayers"===t||"inputLayers"===t)&&0===e&&"string"==typeof n}function ho(t,e){if(null===t)return null;if("string"==typeof t)return Ii(t);if("number"==typeof t||"boolean"==typeof t)return t;if(t instanceof Array){const n=[],s=t.length;for(let i=0;i<s;++i){const s=t[i];uo(e,i,s)?n.push(s):n.push(ho(s,e))}return n}{const e={};for(const n of Object.keys(t)){const s=t[n];if("name"===n&&"string"==typeof s)e[n]=s;else{const t=Ii(n);e[t]=ho(s,t)}}return e}}function co(t,e){if(null==t)return null;if("string"==typeof t)return zi(t);if("number"==typeof t||"boolean"==typeof t)return t;if(t instanceof Array){const n=[],s=t.length;for(let i=0;i<s;++i){const s=t[i];uo(e,i,s)?n.push(s):n.push(co(s,e))}return n}{const e={};for(const n of Object.keys(t)){const s=t[n],i=zi(n);e[i]="name"!==n&&"className"!==n||"string"!=typeof s?co(s,n):s}return e}}const po="3.9.0";class fo{constructor(t){if(this.id2Value={},this.id2Mask={},this.name2Id={},t instanceof fo)for(const e in t.id2Value)this.id2Value[e]=t.id2Value[e],e in t.id2Mask&&(this.id2Mask[e]=t.id2Mask[e]);else{if(null==t)return;for(const e of t)this.add(e.key,e.value)}}add(t,n,s){if(null!=this.id2Value[t.id])throw new yi(`Duplicate key: name=${t.name}, id=${t.id}`);return this.id2Value[t.id]=function(t,n){if(null==t.dtype||t.dtype===n.dtype)return n;try{return e.cast(n,t.dtype)}catch(e){throw new yi(`The dtype of the feed (${n.dtype}) can not be cast to the dtype of the key '${t.name}' (${t.dtype}).`)}}(t,n),this.name2Id[t.name]=t.id,null!=s&&(this.id2Mask[t.id]=s),this}addFeed(t){this.add(t.key,t.value)}hasKey(t){return null!=this.id2Value[t.id]}names(){return Object.keys(this.name2Id)}getValue(t){if(t instanceof fa){if(null==this.id2Value[t.id])throw new yi(`Nonexistent key: ${t.name}`);return this.id2Value[t.id]}{const e=this.name2Id[t];if(null==e)throw new yi(`Feed dict has no SymbolicTensor name: ${t}`);return this.id2Value[e]}}getMask(t){if(t instanceof fa){if(null==this.id2Value[t.id])throw new yi(`Nonexistent key: ${t.name}`);return this.id2Mask[t.id]}{const e=this.name2Id[t];if(null==e)throw new yi(`Feed dict has no SymbolicTensor name: ${t}`);return this.id2Mask[e]}}disposeMasks(){null!=this.id2Mask&&e.dispose(this.id2Mask)}}const go={},mo={};function yo(t,n,s,i){const r=null!=s&&s.training,a=Array.isArray(t),o=a?t:[t],l=o.map((t=>t.name)),u=[],h=n.names();for(const t of l)-1!==h.indexOf(t)?u.push(n.getValue(t)):u.push(null);null!=i&&(i.maxNumTensors=-1/0,i.minNumTensors=1/0);const c=l.join(",")+"|"+n.names().join(",");let p,d;if(null==go[c]){const t=function(t,n){e.util.assert(null!=t&&t.length>0,(()=>"Expected at least one fetch, got none"));let s=[],i={};if(1===t.length){const e=wo(t[0],n);s=e.sorted,i=e.recipientMap}else{const e=new Set;for(const r of t){const{sorted:t,recipientMap:a}=wo(r,n);for(const n of t)e.has(n.name)||(s.push(n),e.add(n.name));for(const t in a)null==i[t]&&(i[t]=new Set),a[t].forEach((e=>i[t].add(e)))}}return{sorted:s,recipientCounts:bo(i)}}(o,n);p=t.sorted,d=t.recipientCounts,go[c]=p,mo[c]=d}p=go[c],d={},r||Object.assign(d,mo[c]);const f=new fo(n);for(let t=0;t<p.length;++t){if(null!=i){const t=e.memory().numTensors;t>i.maxNumTensors&&(i.maxNumTensors=t),t<i.minNumTensors&&(i.minNumTensors=t)}const a=p[t],o=a.sourceLayer;if(o instanceof ka)continue;const h=[],c=[],g=[];let m=!1;for(const t of a.inputs){const e=f.getValue(t),s=f.getMask(t);h.push(e),c.push(s),null!=s&&(m=!0),r||(d[t.name]--,0!==d[t.name]||n.hasKey(t)||-1!==l.indexOf(t.name)||e.isDisposed||!0===t.sourceLayer.stateful||g.push(e))}m&&((s=s||{}).mask=c[0]);const y=Ni(o.apply(h,s));let b=null;o.supportsMasking&&(b=o.computeMask(h,c));const w=ko(a),k=Array.isArray(w)?w:[w];for(let t=0;t<k.length;++t){f.hasKey(k[t])||f.add(k[t],y[t],Array.isArray(b)?b[0]:b);const e=l.indexOf(k[t].name);-1!==e&&(u[e]=y[t])}r||e.dispose(g)}return f.disposeMasks(),a?u:u[0]}function bo(t){const e={};for(const n in t)e[n]=t[n].size;return e}function wo(t,e){const n=new Set,s=[],i={};for(const t of e.names())n.add(t);const r=[],a=[];for(r.push(t);r.length>0;){const t=r[r.length-1];if(n.has(t.name)){r.pop();continue}const e=a[a.length-1]===r.length-1;if(0===t.inputs.length||e)r.pop(),s.push(t),n.add(t.name),e&&a.pop();else{a.push(r.length-1);for(const e of t.inputs)null==i[e.name]&&(i[e.name]=new Set),i[e.name].add(t.name),n.has(e.name)||r.push(e)}}return{sorted:s,recipientMap:i}}function ko(t){let e;if(1===t.sourceLayer.inboundNodes.length)e=t.sourceLayer.output;else{let n=null;for(let e=0;e<t.sourceLayer.inboundNodes.length;++e)for(const s of t.sourceLayer.inboundNodes[e].outputTensors)if(s.id===t.id){n=e;break}e=t.sourceLayer.getOutputAt(n)}return e}class vo extends ba{constructor(t){if(super({}),this.containerNodes=new Set,this.name=t.name,null==this.name){const t=this.getClassName().toLowerCase();this.name=sa(t)}if(this.supportsMasking=!1,this.trainable_=!0,Array.isArray(t.inputs)?this.inputs=t.inputs.slice():this.inputs=[t.inputs],Array.isArray(t.outputs)?this.outputs=t.outputs.slice():this.outputs=[t.outputs],Fi(this.inputs).length!==this.inputs.length)throw new yi(`The list of inputs passed to the model is redundant. All inputs should only appear once. Found: ${this.inputs.map((t=>t.name))}`);Fi(this.outputs).length!==this.outputs.length&&console.warn(`The list of outputs passed to the model is redundant. All outputs should only appear once. Found: ${this.outputs.map((t=>t.name))}`),this.inputLayers=[],this.inputLayersNodeIndices=[],this.inputLayersTensorIndices=[],this.outputLayers=[],this.outputLayersNodeIndices=[],this.outputLayersTensorIndices=[],this.layers=[],this.internalContainerRefs=[];for(const t of this.outputs){const e=t.sourceLayer,n=t.nodeIndex,s=t.tensorIndex;this.outputLayers.push(e),this.outputLayersNodeIndices.push(n),this.outputLayersTensorIndices.push(s)}for(const t of this.inputs){const e=t.sourceLayer,n=t.nodeIndex,s=t.tensorIndex;vi(0===n,"input layer has >1 nodes"),vi(0===s,"input layer has >1 tensors"),this.inputLayers.push(e),this.inputLayersNodeIndices.push(n),this.inputLayersTensorIndices.push(s)}this.inputNames=[],this.outputNames=[],this.feedInputShapes=[],this.feedInputNames=[],this.feedOutputNames=[];for(let e=0;e<this.inputLayers.length;e++){const n=this.inputLayers[e];if(!(n instanceof ka))throw new TypeError(`Input layers to a LayersModel must be InputLayer objects. Received inputs: ${t.inputs}. Input ${e} (0-based) originates from layer type ${n.getClassName()}.`);this.inputNames.push(n.name),this.feedInputShapes.push(n.batchInputShape),this.feedInputNames.push(n.name)}for(const t of this.outputLayers)this.outputNames.push(t.name);this.internalInputShapes=this.inputs.map((t=>t.shape)),this.internalOutputShapes=this.outputs.map((t=>t.shape));const e={},n={},s={},i={},r={},a=[],o=(t,e,n,s,i,l)=>{null!=s&&null!=i&&null!=l||(s=t.sourceLayer,i=t.nodeIndex,l=t.tensorIndex);const u=s.inboundNodes[i];if(-1!==n.indexOf(u))throw new mi(`The tensor ${t.name} at layer "${s.name}" is part of a cycle.`);if(-1!==e.indexOf(u))return;this.containerNodes.add(vo.nodeKey(s,i)),s.id in r||(r[s.id]=Object.keys(r).length),-1===n.indexOf(u)&&n.push(u);const h=u.inboundLayers.length;for(let t=0;t<h;t++){const s=u.inputTensors[t],i=u.inboundLayers[t],r=u.nodeIndices[t],a=u.tensorIndices[t];o(s,e,n,i,r,a)}for(e.push(u);n.indexOf(u)>=0;)n.splice(n.indexOf(u),1);a.push(u)},l=[],u=[];for(const t of this.outputs)o(t,l,u);const h=a.slice().reverse();for(const t of h){n[t.id]=t,t.id in e||(e[t.id]=0);let r=e[t.id];const a=null==s[t.outboundLayer.id]?0:s[t.outboundLayer.id];r=Math.max(r,a),s[t.outboundLayer.id]=r,i[t.outboundLayer.id]=t.outboundLayer,e[t.id]=r;for(let s=0;s<t.inboundLayers.length;s++){const i=t.inboundLayers[s],a=t.nodeIndices[s],o=i.inboundNodes[a],l=null==e[o.id]?0:e[o.id];e[o.id]=Math.max(r+1,l),n[o.id]=o}}const c={};for(const t in e){const s=e[t];s in c||(c[s]=[]),c[s].push(n[t])}const p={};for(const t in s){const e=s[t];e in p||(p[e]=[]),p[e].push(i[t])}let d=Object.keys(p).map((t=>parseInt(t,10))).sort(Ei);this.layers=[];for(const t of d){const e=p[t];e.sort(((t,e)=>{const n=r[t.id],s=r[e.id];return n<s?-1:n>s?1:0}));for(const t of e)t instanceof vo&&this.internalContainerRefs.push(t),this.layers.push(t)}this.layersByDepth=p,d=Object.keys(c).map((t=>parseInt(t,10))).sort(Ei);const f=this.inputs.slice(),g=[];for(const t of d)for(const e of c[t]){const t=e.outboundLayer;if(null!=t){for(const n of e.inputTensors)if(-1===f.indexOf(n))throw new mi(`Graph disconnected: cannot obtain value for tensor ${n} at layer "${t.name}". The following previous layers were accessed without issue: ${g}`);for(const t of e.outputTensors)f.push(t);g.push(t.name)}}this.nodesByDepth=c;const m=this.layers.map((t=>t.name));for(const t of m){const e=m.filter((e=>e===t)).length;if(1!==e)throw new mi(`The name "${t}" is used ${e} times in the model. All layer names should be unique. Layer names: `+JSON.stringify(m))}this.outboundNodes=[],this.inboundNodes=[],new ma({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:this.inputs,outputTensors:this.outputs,inputMasks:this.inputs.map((t=>null)),outputMasks:this.outputs.map((t=>null)),inputShapes:this.inputs.map((t=>t.shape)),outputShapes:this.outputs.map((t=>t.shape))}),this.built=!0,this._refCount=1}assertNotDisposed(){if(0===this._refCount)throw new Error(`Container '${this.name}' is already disposed.`)}dispose(){this.assertNotDisposed();const t={refCountAfterDispose:null,numDisposedVariables:0};if(0==--this._refCount){for(const e of this.layers)t.numDisposedVariables+=e.dispose().numDisposedVariables;for(const e of this.internalContainerRefs)t.numDisposedVariables+=e.dispose().numDisposedVariables}return t.refCountAfterDispose=this._refCount,t}get trainable(){return this.trainable_}set trainable(t){this.layers.forEach((e=>{e._trainableWeights.forEach((e=>e.trainable=t))})),this.trainable_=t}get trainableWeights(){if(this._trainableWeights.length>0)throw new yi("Container instance unexpectedly contains _trainableWeights.The trainable weights of a Container are a union of the trainable weights of its consituent Layers. Its own _trainableWeights must remain an empty Array.");if(!this.trainable)return[];let t=[];for(const e of this.layers)t=t.concat(e.trainableWeights);return t}get nonTrainableWeights(){const t=[];for(const e of this.layers)t.push(...e.nonTrainableWeights);if(!this.trainable){const e=[];for(const t of this.layers)e.push(...t.trainableWeights);return e.concat(t)}return t}get weights(){return this.trainableWeights.concat(this.nonTrainableWeights)}loadWeights(t,e=!0){const n={};let s=0;for(const t of this.layers)for(const e of t.weights){if(null!=n[e.originalName])throw new yi(`Duplicate weight name: ${e.originalName}`);n[e.originalName]=e,s++}const i=[];for(const s in t){let r=s;if(null==n[s]){const t=s.split("/");r=t.slice(0,-2).concat([t[t.length-1]]).join("/")}if(null!=n[r])i.push([n[r],t[s]]);else if(e)throw new yi(`Provided weight data has no target variable: ${s}`);delete n[r]}if(e){const t=[];for(const e in n)t.push(e);if(t.length>0)throw new yi(`${t.length} of ${s} weights are not set: ${t}`)}pa(i)}updatedConfig(){const t=this.getConfig(),e={};return e.className=this.getClassName(),e.config=t,e.kerasVersion="tfjs-layers 3.9.0",e.backend="TensorFlow.js",e}toJSON(t,e=!0){const n=co(this.updatedConfig());return e?JSON.stringify(n):n}call(t,n){return e.tidy((()=>{t=Ni(t);const e=new fo;for(let n=0;n<this.inputs.length;++n)e.add(this.inputs[n],t[n]);return yo(this.outputs,e,n)}))}computeMask(t,n){return e.tidy((()=>{let e;return t=Ni(t),e=null==n?ki(null,t.length):Ni(n),this.runInternalGraph(t,e)[1]}))}computeOutputShape(t){const e=ra(t);if(e.length!==this.inputLayers.length)throw new yi(`Invalid inputShape argument ${t}: model has ${this.inputLayers.length} tensor inputs.`);const n={};for(let t=0;t<e.length;t++){const s=this.inputLayers[t],i=e[t];n[s.name+"_0_0"]=i}const s=Object.keys(this.nodesByDepth).map((t=>parseInt(t,10))).sort(Ei);if(s.length>1)for(const t of s){const e=this.nodesByDepth[t];for(const t of e){const e=t.outboundLayer;if(-1!==this.inputLayers.map((t=>t.id)).indexOf(e.id))continue;const s=[];for(let e=0;e<t.inboundLayers.length;e++){const i=t.inboundLayers[e],r=t.nodeIndices[e],a=t.tensorIndices[e],o=n[`${i.name}_${r}_${a}`];s.push(o)}const i=ra(e.computeOutputShape(xi(s))),r=e.inboundNodes.indexOf(t);for(let t=0;t<i.length;t++){n[`${e.name}_${r}_${t}`]=i[t]}}}const i=[],r=[];for(let t=0;t<this.outputLayers.length;t++){const e=this.outputLayers[t],n=this.outputLayersNodeIndices[t],s=this.outputLayersTensorIndices[t],i=`${e.name}_${n}_${s}`;r.push(i)}for(let t=0;t<r.length;t++){const e=r[t];vi(e in n),i.push(n[e])}return xi(i)}runInternalGraph(t,e){null==e&&(e=ki(null,t.length));const n={};for(let s=0;s<this.inputs.length;++s){const i=this.inputs[s],r=t[s],a=e[s];n[i.id]=[r,a]}const s=Object.keys(this.nodesByDepth).map((t=>parseInt(t,10))).sort(Ei);for(const t of s){const e=this.nodesByDepth[t];for(const t of e){const e=t.outboundLayer,s=t.inputTensors,i=t.outputTensors,r=new Array;for(const t of s)t.id in n&&r.push(n[t.id]);if(r.length===s.length){let s,a,o,l,u={};if(null!=t.callArgs&&(u=t.callArgs),1===r.length){const[t,n]=r[0];null==u.mask&&(u.mask=n),o=Ni(e.call(t,u)),l=Ni(e.computeMask(t,n)),s=[t],a=[n]}else s=r.map((t=>t[0])),a=r.map((t=>t[1])),null==u.mask&&(u.mask=a),o=Ni(e.call(s,u)),l=Ni(e.computeMask(s,a));if(e.activityRegularizer)throw new bi("LayersModel invocation with concrete Tensor value(s) in the presence of activity regularizer(s) is not supported yet.");for(let t=0;t<i.length;++t){const e=i[t],s=o[t],r=l[t];n[e.id]=[s,r]}}}}const i=[],r=[],a=[];for(const t of this.outputs){vi(t.id in n,`Could not compute output ${t.name} : ${t.id}`);const[e,s]=n[t.id];a.push(e.shape),i.push(e),r.push(s)}return[i,r,a]}buildNodeConversionMap(t){const e={};let n;for(const t of this.layers){n=t instanceof vo?1:0;for(let s=0;s<t.inboundNodes.length;s++){const i=vo.nodeKey(t,s);this.containerNodes.has(i)&&(e[i]=n,n+=1)}}return e}getLayer(t,e){if(null!=e){if(this.layers.length<=e)throw new yi(`Was asked to retrieve layer at index ${e}, but model only has ${this.layers.length} layer(s).`);return this.layers[e]}if(null==t)throw new yi("Provide either a layer name or layer index");for(const e of this.layers)if(e.name===t)return e;throw new yi(`No such layer: ${t}`)}calculateLosses(){return e.tidy((()=>{const t=[];for(const e of this.layers)for(let n=0;n<e.inboundNodes.length;++n){const s=vo.nodeKey(e,n);this.containerNodes.has(s)&&t.push(...e.calculateLosses())}return t}))}getConfig(){const t={name:this.name},e=this.buildNodeConversionMap(this.layers),n=[];for(const t of this.layers){const s=t.getClassName(),i=t.getConfig(),r=[];for(let n=0;n<t.inboundNodes.length;n++){const s=t.inboundNodes[n],i=vo.nodeKey(t,n);let a={};if(this.containerNodes.has(i)){if(s.callArgs)try{JSON.stringify(s.callArgs),a=s.callArgs}catch(e){console.warn(`Layer ${t.name} was passed non-serializable keyword arguments: ${s.callArgs}. They will not be included in the serialized model (and thus will be missing at deserialization time).`),a={}}if(s.inboundLayers.length>0){const t=[];for(let n=0;n<s.inboundLayers.length;n++){const i=s.inboundLayers[n],r=s.nodeIndices[n],o=s.tensorIndices[n];let l=e[vo.nodeKey(i,r)];null==l&&(l=0),t.push([i.name,l,o,a])}r.push(t)}}}const a={};a.name=t.name,a.className=s,a.config=i,a.inboundNodes=r,n.push(a)}t.layers=n;const s=[];for(let t=0;t<this.inputLayers.length;t++){const n=this.inputLayers[t],i=this.inputLayersNodeIndices[t],r=vo.nodeKey(n,i);if(!this.containerNodes.has(r))continue;let a=e[r];null==a&&(a=0);const o=this.inputLayersTensorIndices[t];s.push([n.name,a,o])}t.inputLayers=s;const i=[];for(let t=0;t<this.outputLayers.length;t++){const n=this.outputLayers[t],s=this.outputLayersNodeIndices[t],r=vo.nodeKey(n,s);if(!this.containerNodes.has(r))continue;let a=e[r];null==a&&(a=0);const o=this.outputLayersTensorIndices[t];i.push([n.name,a,o])}return t.outputLayers=i,t}static fromConfig(t,e,n={},s=!1){const i={},r={};function a(t,e){t.name in r?r[t.name].push(e):r[t.name]=[e]}function o(t,e){const n=[];let s;for(const r of e){const o=r[0],l=r[1],u=r[2];if(s=null==r[3]?{}:r[3],!(o in i))return void a(t,e);const h=i[o];if(h.inboundNodes.length<=l)return void a(t,e);const c=h.inboundNodes[l];n.push(c.outputTensors[u])}n.length>0&&t.apply(xi(n),s)}function l(t){const n=t.name,r=Da(t,null!=e.customObjects?e.customObjects:{});r.setFastWeightInitDuringBuild(s),i[n]=r;t.inboundNodes.forEach((t=>{if(!(t instanceof Array))throw new yi(`Corrupted configuration, expected array for nodeData: ${t}`);a(r,t)}))}const u=e.name,h=e.layers;for(const t of h)l(t);for(;!Di(r);)for(const t of h){const e=i[t.name];if(e.name in r){const t=r[e.name];delete r[e.name];for(const n of t)o(e,n)}}const c=[],p=[],d=e.inputLayers;for(const t of d){const e=t[0],n=t[1],s=t[2];vi(e in i);const r=i[e].inboundNodes[n].outputTensors;c.push(r[s])}const f=e.outputLayers;for(const t of f){const e=t[0],n=t[1],s=t[2];vi(e in i);const r=i[e].inboundNodes[n].outputTensors;p.push(r[s])}return new t({inputs:c,outputs:p,name:u})}get stateful(){if(this._stateful)throw new yi("Container instance unexpectedly has _stateful = true. The statefulness of a Container is determined by the Layers it contains. Its _stateful property must remain the default false.");for(const t of this.layers)if(t.stateful)return!0;return!1}resetStates(){e.tidy((()=>{this.layers.forEach((t=>{t.stateful&&t.resetStates()}))}))}}function So(t,e){return function(t,e,n){const s=e.length;if(null==t||Array.isArray(t)&&0===t.length)return e.map((t=>null));if(1===s)return Array.isArray(t)&&1===t.length?t:"object"==typeof t&&e[0]in t?[t[e[0]]]:[t];if(Array.isArray(t)){if(t.length!==s)throw new Error(`Provided ${n} is an array of ${t.length} element(s), but the model has ${s} outputs. Make sure a set of weights is provided for each model output.`);return t}if("object"==typeof t&&Object.keys(t).length>0&&"object"==typeof t[Object.keys(t)[0]]){const n=[];return e.forEach((e=>{e in t?n.push(t[e]):n.push(null)})),n}throw new Error(`The model has multiple (${s}) outputs, so ${n} must be either an array with ${s} elements or an object with ${e} keys. Provided ${n} not understood: ${JSON.stringify(t)}`)}(t,e,"classWeight")}async function xo(t,n,s,i){if(null!=n||null!=i)throw new Error("Support sampleWeight is not implemented yet");if(null!=s){const n=e.tidy((()=>{if(1===t.shape.length)return e.clone(t);if(2===t.shape.length){if(t.shape[1]>1){const n=1;return e.argMax(t,n)}if(1===t.shape[1])return e.reshape(t,[t.shape[0]]);throw new Error(`Encountered unexpected last-dimension size (${t.shape[1]}) during handling of class weights. The size is expected to be >= 1.`)}throw new Error(`Unexpected rank of target (y) tensor (${t.rank}) during handling of class weights. The rank is expected to be 1 or 2.`)})),i=Array.from(await n.data());e.dispose(n);const r=[];return i.forEach((t=>{if(null==s[t])throw new Error(`classWeight must contain all classes in the training data. The class ${t} exists in the data but not in classWeight`);r.push(s[t])})),e.tensor1d(r,"float32")}return null}function No(t,n){return e.mul(t,n)}function zo(t,n){let s,i;const r=n;s=r.xs,i=r.ys,e.util.assert(null!=s&&null!=i,(()=>`A Dataset iterator for fitDataset() is expected to generate objects of the form \`{xs: xVal, ys: yVal}\`, where the two values may be \`tf.Tensor\`, an array of Tensors, or a map of string to Tensor.  The provided Dataset instead generates ${n}`));const a=Io("input",t.inputNames,s),o=Io("output",t.outputNames,i),l=a[0].shape[0];e.util.assert(a.length===t.inputs.length,(()=>`LayersModel has ${t.inputs.length} inputs, but the dataset provides ${a.length} inputs.  (Expected input keys: ${JSON.stringify(t.inputNames)})`)),e.util.assert(o.length===t.outputs.length,(()=>`LayersModel has ${t.outputs.length} outputs, but the dataset provides ${o.length} outputs.  (Expected output keys: ${JSON.stringify(t.outputNames)})`));for(let n=0;n<a.length;n++)e.util.assert(a[n].shape[0]===l,(()=>`Batch size mismatch: input ${t.inputNames[n]} has ${a[n].shape[0]}; expected  ${l} based on input ${t.inputNames[0]}.`));for(let n=0;n<o.length;n++)e.util.assert(o[n].shape[0]===l,(()=>`Batch size mismatch: output ${t.outputNames[n]} has ${o[n].shape[0]}; expected  ${l} based on input ${t.inputNames[0]}.`));return{xs:a,ys:o}}function Io(t,n,s){if(s instanceof e.Tensor)return[s];if(Array.isArray(s))return e.util.assert(s.length===n.length,(()=>`Received an array of ${s.length} Tensors, but expected ${n.length} to match the ${t} keys ${n}.`)),s;{const e=[];for(const i of n){if(null==s[i])throw new yi(`The feature data generated by the dataset lacks the required ${t} key '${i}'.`);e.push(s[i])}return e}}async function Ao(t,n,s){const i=null!=s.batchesPerEpoch;if(e.util.assert(null!=t.optimizer,(()=>"You must compile a model before training/testing. Use LayersModel.compile(modelCompileConfig).")),e.util.assert(null!=s,(()=>"For fitDataset(), the 2nd argument (config) is required, but it is not provided in this call.")),e.util.assert(null!=s.epochs&&s.epochs>0&&Number.isInteger(s.epochs),(()=>`For fitDataset(), config.epochs is expected to be a positive integer, but got ${s.epochs}`)),e.util.assert(!i||s.batchesPerEpoch>0&&Number.isInteger(s.batchesPerEpoch),(()=>`For fitDataset(), config.batchesPerEpoch is expected to be a positive integer if specified, but got ${s.batchesPerEpoch}`)),e.util.assert(null==s.validationSplit,(()=>"`validationSplit` is not supported by `fitDataset()`. Use validationData instead.")),t.isTraining)throw new Error("Cannot start training because another fit() call is ongoing.");t.isTraining=!0;try{const r=null!=s.validationData;let a,o;if(r)if(Co(s.validationData))e.util.assert(null==s.validationBatches||s.validationBatches>0&&Number.isInteger(s.validationBatches),(()=>`For fitDataset() with dataset-based validation, config.validationBatches is expected not to be provided, or to be a positive integer, but got ${s.validationBatches}`));else{const t=function(t){if(3===t.length)throw new bi("Validation with sample weights is not implemented yet.");return{xs:t[0],ys:t[1]}}(s.validationData);a=t.xs,o=t.ys}const l=t.makeTrainFunction(),u=t.getDedupedMetricsNames();let h;h=r?u.slice().concat(u.map((t=>"val_"+t))):u.slice();const c=$a(s.callbacks,s.yieldEvery),p=null==s.verbose?1:s.verbose,{callbackList:d,history:f}=Fa(c,p,s.epochs,null,null,function(t,e){let n=null;null!=e.batchesPerEpoch?n=e.batchesPerEpoch:Number.isFinite(t.size)&&(n=t.size);return n}(n,s),null,r,h);d.setModel(t),t.history=f,await d.onTrainBegin(),t.stopTraining_=!1;let g=null==s.initialEpoch?0:s.initialEpoch,m=await n.iterator();for(;g<s.epochs;){const h={};await d.onEpochBegin(g);let c=0,p=0;for(i||(m=await n.iterator());!i||c<s.batchesPerEpoch;){const n=await m.next();if(i&&n.done){console.warn(`You provided \`batchesPerEpoch\` as ${s.batchesPerEpoch}, but your dataset iterator ran out of data after ${c} batches; interrupting training. Make sure that your dataset can generate at least \`batchesPerEpoch * epochs\` batches (in this case, `+s.batchesPerEpoch*s.epochs+" batches). You may need to use the repeat() function when building your dataset.");break}if(null!=n.value){const{xs:i,ys:r}=zo(t,n.value),a={};a.batch=p,a.size=i[0].shape[0],await d.onBatchBegin(p,a);const o=[];if(null!=s.classWeight){const e=So(s.classWeight,t.outputNames);for(let t=0;t<e.length;++t)o.push(await xo(r[t],null,e[t]))}const h=i.concat(r).concat(o),f=l(h);e.dispose(h);for(let t=0;t<u.length;++t){const n=u[t],s=f[t];a[n]=s,e.keep(s)}await d.onBatchEnd(p,a),xa(a),p++,c++}if(i?c>=s.batchesPerEpoch:n.done){if(r){let e;e=Co(s.validationData)?Ni(await t.evaluateDataset(s.validationData,{batches:s.validationBatches})):Ni(t.evaluate(a,o,{batchSize:null==s.validationBatchSize?32:s.validationBatchSize,verbose:0}));for(let n=0;n<t.metricsNames.length;++n)h[`val_${t.metricsNames[n]}`]=e[n]}break}if(t.stopTraining_)break}if(await d.onEpochEnd(g,h),g++,t.stopTraining_)break}return await d.onTrainEnd(),await t.history.syncData(),t.history}finally{t.isTraining=!1}}function Co(t){return"function"==typeof t.iterator}function To(t){e.util.assert(t>0&&Number.isInteger(t),(()=>`batchSize is required to be a positive integer, but got ${t}`))}function $o(t,e,n){return null==t?[null]:Array.isArray(t)?t.map((t=>yr(t,e,n-e))):yr(t,e,n-e)}function Eo(t,n){return e.tidy((()=>null==t?null:Array.isArray(t)?t.map((t=>Eo(t,n))):zr(t,"int32"===n.dtype?n:e.cast(n,"int32"))))}function Fo(t,e){const n=[];let s=0,i=null;for(;s<t;)i=s+e,i>=t&&(i=t),n.push([s,i]),s=i;return n}async function Do(t,n,s,i={}){if(t.isTraining)throw new Error("Cannot start training because another fit() call is ongoing.");let r,a,o,l,u,h,c;t.isTraining=!0;try{const p=null==i.batchSize?32:i.batchSize;To(p);const d=!1,f=await t.standardizeUserData(n,s,i.sampleWeight,i.classWeight,d,p);r=f[0],a=f[1],c=f[2];let g,m=!1;if(null!=i.validationData&&i.validationData.length>0){if(m=!0,2!==i.validationData.length)throw 3===i.validationData.length?new bi("validationData including sample weights is not supported yet."):new yi(`When passing validation data, it must contain 2 (valX, valY) or 3 (valX, valY, valSampleWeight) items; ${i.validationData} is invalid.`);o=i.validationData[0],l=i.validationData[1];const e=!0,n=await t.standardizeUserData(o,l,null,null,e,p);u=n[0],h=n[1],g=u.concat(h)}else if(null!=i.validationSplit&&i.validationSplit>0&&i.validationSplit<1){m=!0;const t=Math.floor(r[0].shape[0]*(1-i.validationSplit)),e=r[0].shape[0];u=$o(r,t,e),r=$o(r,0,t),h=$o(a,t,e),a=$o(a,0,t),g=u.concat(h)}else null!=i.validationSteps&&(m=!0);const y=r.concat(a).concat(c);t.checkTrainableWeightsConsistency();const b=t.makeTrainFunction(),w=t.getDedupedMetricsNames();let k,v;m?(t.makeTestFunction(),k=t.testFunction,v=w.slice().concat(w.map((t=>"val_"+t)))):(k=null,g=[],v=w.slice());const S=$a(i.callbacks,i.yieldEvery);return await async function(t,n,s,i,r,a,o,l,u,h,c,p,d,f,g){null==r&&(r=32),null==a&&(a=1),null==c&&(c=!0),null==d&&(d=0);let m=!1;if(null!=u&&null!=h&&(m=!0),null!=g&&(m=!0,null==f))throw new yi("Can only use `validationSteps` when doing step-wise training, i.e., `stepsPerEpoch` must be set.");const y=t.checkNumSamples(s,r,f,"steps_per_epoch");let b;null!=y&&(b=fr(0,y)),null==o&&(o=1);const{callbackList:w,history:k}=Fa(l,o,a,d,y,f,r,m,p);w.setModel(t),t.history=k,await w.onTrainBegin(),t.stopTraining_=!1;for(let o=d;o<a;++o){await w.onEpochBegin(o);const a={};if(null!=f)throw new bi("stepsPerEpoch mode is not implemented yet.");{if("batch"===c)throw new bi("batch shuffling is not implemneted yet");c&&e.util.shuffle(b);const o=e.tensor1d(b),l=Fo(y,r);for(let c=0;c<l.length;++c){const p={};if(await w.onBatchBegin(c,p),e.tidy((()=>{const d=l[c][0],f=l[c][1],g=yr(o,d,f-d);p.batch=c,p.size=f-d;const y=Eo(s,g),b=n(y);for(let t=0;t<i.length;++t){const n=i[t],s=b[t];p[n]=s,e.keep(s)}if(c===l.length-1&&m){const n=t.testLoop(u,h,r);for(let t=0;t<i.length;++t){const s=i[t],r=n[t];e.keep(r),a["val_"+s]=r}}})),await w.onBatchEnd(c,p),xa(p),t.stopTraining_)break}o.dispose()}if(await w.onEpochEnd(o,a),t.stopTraining_)break}return await w.onTrainEnd(),await t.history.syncData(),t.history}(t,b,y,w,p,i.epochs,i.verbose,S,k,g,i.shuffle,v,i.initialEpoch,null,null)}finally{t.isTraining=!1,_o(r,n),_o(a,s),_o(u,o),_o(h,l),null!=c&&e.dispose(c)}}function Lo(t){const n=[];t instanceof e.Tensor&&(t=[t]);for(let e=0;e<t.length;++e){const s=t[e];if(1===s.rank)n.push(mr(s,1));else{if(0===s.rank)throw new Error("Expected tensor to be at least 1D, but received a 0D tensor (scalar).");n.push(s)}}return n}function _o(t,n){if(null==t)return;const s=[];if(n instanceof e.Tensor)s.push(n.id);else if(Array.isArray(n))n.forEach((t=>s.push(t.id)));else if(null!=n)for(const t in n){const e=n[t];s.push(e.id)}const i=[];if(t instanceof e.Tensor)-1===s.indexOf(t.id)&&i.push(t);else if(Array.isArray(t))t.forEach((t=>{-1===s.indexOf(t.id)&&i.push(t)}));else if(null!=t)for(const e in t){const n=t[e];-1===s.indexOf(n.id)&&i.push(n)}i.forEach((t=>{t.isDisposed||t.dispose()}))}function Ro(t){return Array.isArray(t)}function Mo(t){return!function(t){return t instanceof e.Tensor}(t)&&!Ro(t)}function Oo(t,e,n,s=!0,i=""){if(null==e||0===e.length){if(null!=t){let e=!1;if(Ro(t)&&t.length>0)e=!0;else if(Mo(t)){for(const n in t)if(t.hasOwnProperty(n)){e=!0;break}}else e=!0;if(e)throw new yi(`Error when checking model ${i} expected no data, but got ${t}`)}return[]}if(null==t)return e.map((t=>null));let r;if(Mo(t)){t=t,r=[];for(const n of e){if(null==t[n])throw new yi(`No data provided for "${n}". Need data for each key in: ${e}`);r.push(t[n])}}else if(Ro(t)){if((t=t).length!==e.length)throw new yi(`Error when checking model ${i}: the Array of Tensors that you are passing to your model is not the size the model expected. Expected to see ${e.length} Tensor(s), but instead got the following list of Tensor(s): ${t}`);r=t}else{if(t=t,e.length>1)throw new yi(`The model ${i} expects ${e.length} Tensor(s), but only received one Tensor. Found: Tensor with shape ${t.shape}`);r=[t]}if(r=Lo(r),null!=n)for(let t=0;t<e.length;++t){if(null==n[t])continue;const a=r[t];if(a.shape.length!==n[t].length)throw new yi(`Error when checking ${i}: expected ${e[t]} to have ${n[t].length} dimension(s). but got array with shape ${a.shape}`);for(let e=0;e<n[t].length;++e){if(0===e&&!s)continue;const r=a.shape[e],o=n[t][e];if(null!=o&&o>=0&&r!==o)throw new yi(`${i} expected a batch of elements where each example has shape [${n[t].slice(1,n[t].length)}] (i.e.,tensor shape [*,${n[t].slice(1,n[t].length)}]) but the ${i} received an input with ${a.shape[0]} examples, each with shape [${a.shape.slice(1,a.shape.length)}] (tensor shape [${a.shape}])`)}}return r}function Bo(t,e,n,s=!0,i=""){let r;if(Array.isArray(t)){if(t.length!==e.length)throw new yi(`Error when checking model ${i}: the Array of Tensors that you are passing to your model is not the size the the model expected. Expected to see ${e.length} Tensor(s), but instead got ${t.length} Tensors(s).`);r=t}else{if(e.length>1)throw new yi(`The model expects ${e.length} ${i} Tensors, but only received one Tensor. Found: array with shape ${JSON.stringify(t.shape)}.`);r=[t]}if(null!=n)for(let t=0;t<e.length;++t){if(null==n[t])continue;const a=r[t];if(a.shape.length!==n[t].length)throw new yi(`Error when checking ${i}: expected ${e[t]} to have ${n[t].length} dimension(s), but got array with shape ${JSON.stringify(a.shape)}`);for(let r=0;r<n[t].length;++r){if(0===r&&!s)continue;const o=a.shape[r],l=n[t][r];if(null!=l&&l!==o)throw new yi(`Error when checking ${i}: expected ${e[t]} to have shape ${JSON.stringify(n[t])} but got array with shape ${JSON.stringify(a.shape)}.`)}}}class Po extends vo{constructor(t){super(t),this.isTraining=!1}summary(t,e,n=console.log){if(!this.built)throw new yi("This model has never been called, thus its weights have not been created yet. So no summary can be displayed. Build the model first (e.g., by calling it on some test data).");ro(this,t,e,n)}compile(t){if(null==t.loss&&(t.loss=[]),this.loss=t.loss,"string"==typeof t.optimizer)this.optimizer_=function(t){const n={Adagrad:()=>e.train.adagrad(.01),Adadelta:()=>e.train.adadelta(1,.95,fi()),Adam:()=>e.train.adam(.001,.9,.999,fi()),Adamax:()=>e.train.adamax(.002,.9,.999,fi(),0),RMSProp:()=>e.train.rmsprop(.001,.9,0,fi()),SGD:()=>e.train.sgd(.01)};if(n.adagrad=n.Adagrad,n.adadelta=n.Adadelta,n.adam=n.Adam,n.adamax=n.Adamax,n.rmsprop=n.RMSProp,n.sgd=n.SGD,t in n)return n[t]();throw new yi(`Unknown Optimizer ${t}`)}(t.optimizer),this.isOptimizerOwned=!0;else{if(!(t.optimizer instanceof e.Optimizer))throw new yi("User-defined optimizer must be an instance of tf.Optimizer.");this.optimizer_=t.optimizer,this.isOptimizerOwned=!1}let n=[];if(Array.isArray(t.loss)||"string"==typeof t.loss||"function"==typeof t.loss)if(Array.isArray(t.loss)){if(t.loss.length!==this.outputs.length)throw new yi(`When passing an Array as loss, it should have one entry per model output. The model has ${this.outputs.length} output(s), but you passed loss=${t.loss}.`);const e=t.loss;n=e.map((t=>ja(t)))}else{const e=ja(t.loss);this.outputs.forEach((t=>{n.push(e)}))}else{t.loss=t.loss;for(const e in t.loss)if(-1===this.outputNames.indexOf(e))throw new yi(`Unknown entry in loss dictionary: "${e}". Only expected the following keys: ${this.outputNames}`);for(const e of this.outputNames)null==t.loss[e]&&console.warn(`Output "${e}" is missing from loss dictionary. We assume this was done on purpose, and we will not be expecting data to be passed to ${e} during training`),n.push(ja(t.loss[e]))}this.lossFunctions=n,this.feedOutputNames=[],this.feedOutputShapes=[],this.feedLossFns=[];for(let t=0;t<this.outputs.length;++t){const e=this.internalOutputShapes[t],n=this.outputNames[t];this.feedOutputNames.push(n),this.feedOutputShapes.push(e),this.feedLossFns.push(this.lossFunctions[t])}const s=[];this.metrics=t.metrics,this.metricsNames=["loss"],this.metricsTensors=[],ar("loss",(()=>{for(let t=0;t<this.outputs.length;++t){if(-1!==s.indexOf(t))continue;const e=this.lossFunctions[t];this.outputs.length>1&&(this.metricsTensors.push([e,t]),this.metricsNames.push(this.outputNames[t]+"_loss"))}}));const i=function(t,e){if(null==t||Array.isArray(t)&&0===t.length)return e.map((t=>[]));let n;if("string"==typeof t||"function"==typeof t)n=[t];else{if(!Array.isArray(t)&&"object"!=typeof t)throw new TypeError(`Type of metrics argument not understood. Expected an string,function, Array, or Object, found: ${t}`);n=t}if(Array.isArray(n))return e.map((t=>n));{const t=[];for(const s of e){let e=n.hasOwnProperty(s)?n[s]:[];Array.isArray(e)||(e=[e]),t.push(e)}return t}}(t.metrics,this.outputNames),r=(t,e,n)=>{this.outputNames.length>1&&(e=this.outputNames[t]+"_"+e),this.metricsNames.push(e),this.metricsTensors.push([n,t])};ar("metric",(()=>{for(let t=0;t<this.outputs.length;++t){if(-1!==s.indexOf(t))continue;(e=>{let n,s,i;for(const a of e){if("string"==typeof a&&-1!==["accuracy","acc","crossentropy","ce"].indexOf(a)){const e=this.internalOutputShapes[t];let r;1===e[e.length-1]||this.lossFunctions[t]===Pa?-1!==["accuracy","acc"].indexOf(a)?s=Va:-1!==["crossentropy","ce"].indexOf(a)&&(s=Ja):this.lossFunctions[t]===Ba?-1!==["accuracy","acc"].indexOf(a)?s=Za:-1!==["crossentropy","ce"].indexOf(a)&&(s=Xa):-1!==["accuracy","acc"].indexOf(a)?s=Ka:-1!==["crossentropy","ce"].indexOf(a)&&(s=Ya),-1!==["accuracy","acc"].indexOf(a)?r="acc":-1!==["crossentropy","ce"].indexOf(a)&&(r="ce"),i=s,n=""+r}else{const t=to(a);i=t,n=""+eo(a)}let e;ar(n,(()=>{e=i})),r(t,n,e)}})(i[t])}})),this.collectedTrainableWeights=this.trainableWeights}checkTrainableWeightsConsistency(){null!=this.collectedTrainableWeights&&this.trainableWeights.length!==this.collectedTrainableWeights.length&&console.warn("Discrepancy between trainableweights and collected trainable weights. Did you set `model.trainable` without calling `model.compile()` afterwards?")}evaluate(t,e,n={}){const s=null==n.batchSize?32:n.batchSize;To(s);const i=this.standardizeUserDataXY(t,e,!0,s);try{const r=i[0].concat(i[1]);this.makeTestFunction();const a=this.testFunction;return xi(this.testLoop(a,r,s,n.verbose,n.steps))}finally{_o(i[0],t),_o(i[1],e)}}async evaluateDataset(t,n){return this.makeTestFunction(),async function(t,n,s){const i=null!=(s=s||{}).batches,r=t.testFunction;let a=[];if(s.verbose>0)throw new bi("Verbose mode is not implemented yet.");e.util.assert(!i||s.batches>0&&Number.isInteger(s.batches),(()=>`Test loop expects \`batches\` to be a positive integer, but received ${JSON.stringify(s.batches)}`));const o="function"==typeof n.next?n:await n.iterator();let l=0,u=0;for(;!i||u<s.batches;){const n=await o.next();if(a=e.tidy((()=>{if(n.value){const{xs:s,ys:i}=zo(t,n.value),o=s.concat(i),h=e.tidy((()=>r(o)));if(e.dispose(o),0===u)for(let t=0;t<h.length;++t)a.push(e.scalar(0));const c=o[0].shape[0];for(let t=0;t<h.length;++t){const n=h[t],s=a[t];a[t]=e.tidy((()=>e.add(a[t],e.mul(c,n)))),u>0&&e.dispose(s)}e.dispose(h),l+=c,++u}return a})),n.done){i&&console.warn(`Your dataset iterator ran out of data during evaluateDataset(). Interrupting evalution. Make sure that your dataset can generate at least \`batches\` batches (in this case, ${s.batches} batches). You may need to use the repeat() function when building your dataset.`);break}}for(let t=0;t<a.length;++t){const n=a[t];a[t]=e.div(a[t],l),e.dispose(n)}return xi(a)}(this,t,n)}checkNumSamples(t,e,n,s="steps"){let i;if(null!=n){if(i=null,null!=e)throw new yi(`If ${s} is set, batchSize must be null or undefined.Got batchSize = ${e}`)}else{if(null==t)throw new yi(`Either the input data should have a defined shape, or ${s} shoud be specified.`);i=Array.isArray(t)?t[0].shape[0]:t.shape[0]}return i}execute(t,n){if(Array.isArray(n)&&0===n.length)throw new yi("`outputs` is an empty Array, which is not allowed.");const s=Array.isArray(n),i=s?n:[n],r=this.retrieveSymbolicTensors(i),a=new fo;if(t instanceof e.Tensor&&(t=[t]),Array.isArray(t)){if(t.length!==this.inputs.length)throw new yi(`The number of inputs provided (${t.length}) does not match the number of inputs of this model (${this.inputs.length}).`);for(let e=0;e<this.inputs.length;++e)a.add(this.inputs[e],t[e])}else for(const e of this.inputs){const n=t[e.name];if(null==n)throw new yi(`No value is provided for the model's input ${e.name}`);a.add(e,n)}const o=yo(r,a);return s?o:o[0]}retrieveSymbolicTensors(t){const e=ki(null,t.length);let n=t.length;for(const s of this.layers){const i=Array.isArray(s.output)?s.output:[s.output],r=i.map((t=>t.name));for(let s=0;s<t.length;++s){const a=r.indexOf(t[s]);if(-1!==a&&(e[s]=i[a],n--),0===n)break}if(0===n)break}if(n>0){const n=[];throw e.forEach(((e,s)=>{null==e&&n.push(t[s])})),new yi(`Cannot find SymbolicTensors for output name(s): ${JSON.stringify(n)}`)}return e}predictLoop(t,n=32,s=!1){return e.tidy((()=>{const i=this.checkNumSamples(t);if(s)throw new bi("Verbose predictLoop() is not implemented yet.");const r=Fo(i,n),a=this.outputs.map((t=>[]));for(let n=0;n<r.length;++n){e.tidy((()=>{const e=r[n][0],s=r[n][1],i=$o(t,e,s),a=[];if(Array.isArray(i))for(let t=0;t<i.length;++t)a.push({key:this.inputs[t],value:i[t]});else a.push({key:this.inputs[0],value:i});const o=new fo(a);return yo(this.outputs,o)})).forEach(((t,e)=>a[e].push(t)))}return xi(a.map((t=>e.concat(t,0))))}))}predict(t,e={}){const n=Lo(t);Bo(n,this.inputNames,this.feedInputShapes,!1);try{const s=null==e.batchSize?32:e.batchSize;return To(s),this.predictLoop(n,s)}finally{_o(n,t)}}predictOnBatch(t){Bo(t,this.inputNames,this.feedInputShapes,!0);const e=(Array.isArray(t)?t[0]:t).shape[0];return this.predictLoop(t,e)}standardizeUserDataXY(t,n,s=!0,i){if(null==this.optimizer_)throw new mi("You must compile a model before training/testing. Use LayersModel.compile(modelCompileArgs).");const r=[];for(let t=0;t<this.feedOutputShapes.length;++t){const e=this.feedOutputShapes[t];this.feedLossFns[t]===Ba?r.push(e.slice(0,e.length-1).concat([1])):r.push(e)}if(function(t,n,s){const i=Fi(t.map((t=>t.shape[0])));i.sort();const r=Fi(n.map((t=>t.shape[0])));if(r.sort(),i.length>1)throw new yi(`All input Tensors (x) should have the same number of samples. Got array shapes: ${JSON.stringify(t.map((t=>t.shape)))}`);if(r.length>1)throw new yi(`All target Tensors (y) should have the same number of samples. Got array shapes: ${JSON.stringify(n.map((t=>t.shape)))}`);if(i.length>0&&r.length>0&&!e.util.arraysEqual(i,r))throw new yi(`Input Tensors should have the same number of samples as target Tensors. Found ${i[0]} input sample(s) and ${r[0]} target sample(s).`)}(t=Oo(t,this.feedInputNames,this.feedInputShapes,!1,"input"),n=Oo(n,this.feedOutputNames,r,!1,"target")),function(t,e,n){const s=[_a,Pa,Oa];for(let i=0;i<t.length;++i){const r=t[i],a=e[i],o=n[i];if(null!=a){if(a===Oa&&1===r.shape[r.shape.length-1])throw new yi(`You are passing a target array of shape ${r.shape} while using a loss 'categorical_crossentropy'. 'categorical_crossentropy'expects targets to be binary matrices (1s and 0s) of shape [samples, classes].`);if(-1!==s.indexOf(a)){const t=r.shape.slice(1),e=o.slice(1);for(let n=0;n<t.length;++n){const s=t[n],i=e[n];if(null!=i&&s!==i)throw new yi(`A target Tensor with shape ${r.shape} was passed for an output of shape ${o}, while using a loss function that expects targets to have the same shape as the output.`)}}}}}(n,this.feedLossFns,this.feedOutputShapes),this.stateful&&null!=i&&i>0&&t[0].shape[0]%i!=0)throw new yi(`In a stateful network, you should only pass inputs with a number of samples that is divisible by the batch size ${i}. Found: ${t[0].shape[0]} sample(s).`);return[t,n]}async standardizeUserData(t,e,n,s,i=!0,r){const[a,o]=this.standardizeUserDataXY(t,e,i,r);if(null!=n)throw new Error("sample weight is not supported yet.");let l=null;if(null!=s){const t=So(s,this.outputNames);l=[];for(let e=0;e<t.length;++e)l.push(await xo(o[e],null,t[e]))}return[a,o,l]}testLoop(t,n,s,i=0,r){return e.tidy((()=>{const a=this.checkNumSamples(n,s,r,"steps"),o=[];if(i>0)throw new bi("Verbose mode is not implemented yet.");if(null!=r)throw new bi("steps mode in testLoop() is not implemented yet");{const i=Fo(a,s),r=e.tensor1d(fr(0,a));for(let s=0;s<i.length;++s){const a=i[s][0],l=i[s][1],u=yr(r,a,l-a),h=Eo(n,u),c=t(h);if(0===s)for(let t=0;t<c.length;++t)o.push(e.scalar(0));for(let t=0;t<c.length;++t){const n=c[t];o[t]=e.add(o[t],e.mul(l-a,n))}}for(let t=0;t<o.length;++t)o[t]=e.div(o[t],a)}return o}))}getDedupedMetricsNames(){const t=this.metricsNames,e=[];for(let n=0;n<t.length;++n){const s=t[n];let i=s;if(Si(t,s)>1){i+=`_${Si(t.slice(0,n),s)}`}e.push(i)}return e}makeTrainFunction(){return t=>{const n=[],s=t.slice(0,this.inputs.length),i=t.slice(this.inputs.length,this.inputs.length+this.outputs.length),r=t.slice(this.inputs.length+this.outputs.length,this.inputs.length+2*this.outputs.length),a=[],o=this.collectedTrainableWeights.map((t=>t.read()));return[this.optimizer_.minimize((()=>{const t=[];for(let e=0;e<this.inputs.length;++e)t.push({key:this.inputs[e],value:s[e]});const o=new fo(t),l=yo(this.outputs,o,{training:!0});let u;for(let t=0;t<this.lossFunctions.length;++t){let s=(0,this.lossFunctions[t])(i[t],l[t]);null!=r[t]&&(s=No(s,r[t]));const a=e.mean(s);n.push(a),u=0===t?s:e.add(u,s)}for(let t=0;t<this.metricsTensors.length;++t){let s;if(this.outputs.length>1&&t<this.outputs.length)s=n[t];else{const n=this.metricsTensors[t][0],r=this.metricsTensors[t][1];s=e.mean(n(i[r],l[r]))}e.keep(s),a.push(s)}return u=e.mean(u),this.calculateLosses().forEach((t=>{u=e.add(u,t)})),u}),!0,o)].concat(a)}}makeTestFunction(){this.testFunction=t=>e.tidy((()=>{const n=[];let s;const i=t.slice(0,this.inputs.length),r=t.slice(this.inputs.length,this.inputs.length+this.outputs.length),a=[];for(let t=0;t<this.inputs.length;++t)a.push({key:this.inputs[t],value:i[t]});const o=new fo(a),l=yo(this.outputs,o);for(let t=0;t<this.lossFunctions.length;++t){const i=this.lossFunctions[t],a=e.mean(i(r[t],l[t]));s=0===t?a:e.add(s,a),n.push(s)}for(let t=0;t<this.metricsTensors.length;++t){const s=this.metricsTensors[t][0],i=this.metricsTensors[t][1],a=e.mean(s(r[i],l[i]));n.push(a)}return n}))}async fit(t,e,n={}){return Do(this,t,e,n)}async fitDataset(t,e){return Ao(this,t,e)}async trainOnBatch(t,n){const s=await this.standardizeUserData(t,n),i=s[0],r=s[1],a=this.makeTrainFunction()(i.concat(r)),o=[];for(const t of a){const e=await t.data();o.push(e[0])}return e.dispose(a),xi(o)}getNamedWeights(t){const e=[],n=null!=t&&t.trainableOnly,s=n?this.trainableWeights:this.weights,i=this.getWeights(n);for(let t=0;t<s.length;++t)n&&!s[t].trainable||e.push({name:s[t].originalName,tensor:i[t]});return e}set stopTraining(t){this.stopTraining_=t}get stopTraining(){return this.stopTraining_}get optimizer(){return this.optimizer_}set optimizer(t){this.optimizer_!==t&&(this.optimizer_=t,this.isOptimizerOwned=!1)}dispose(){const t=super.dispose();if(0===t.refCountAfterDispose&&null!=this.optimizer&&this.isOptimizerOwned){const n=e.memory().numTensors;this.optimizer_.dispose(),t.numDisposedVariables+=n-e.memory().numTensors}return t}getLossIdentifiers(){let t;if("string"==typeof this.loss)t=zi(this.loss);else if(Array.isArray(this.loss)){for(const t of this.loss)if("string"!=typeof t)throw new Error("Serialization of non-string loss is not supported.");t=this.loss.map((t=>zi(t)))}else{const e=Object.keys(this.loss);t={};const n=this.loss;for(const s of e){if("string"!=typeof n[s])throw new Error("Serialization of non-string loss is not supported.");t[s]=zi(n[s])}}return t}getMetricIdentifiers(){if("string"==typeof this.metrics||"function"==typeof this.metrics)return[zi(eo(this.metrics))];if(Array.isArray(this.metrics))return this.metrics.map((t=>zi(eo(t))));{const t={};for(const e in this.metrics)t[e]=zi(eo(this.metrics[e]));return t}}getTrainingConfig(){return{loss:this.getLossIdentifiers(),metrics:this.getMetricIdentifiers(),optimizer_config:{class_name:this.optimizer.getClassName(),config:this.optimizer.getConfig()}}}loadTrainingConfig(t){if(null!=t.weighted_metrics)throw new Error("Loading weight_metrics is not supported yet.");if(null!=t.loss_weights)throw new Error("Loading loss_weights is not supported yet.");if(null!=t.sample_weight_mode)throw new Error("Loading sample_weight_mode is not supported yet.");const e=Da(ho(t.optimizer_config));let n,s;if("string"==typeof t.loss)n=Ii(t.loss);else if(Array.isArray(t.loss))n=t.loss.map((t=>Ii(t)));else if(null!=t.loss){n={};for(const e in t.loss)n[e]=Ii(t.loss[e])}if(Array.isArray(t.metrics))s=t.metrics.map((t=>Ii(t)));else if(null!=t.metrics){s={};for(const e in t.metrics)s[e]=Ii(t.metrics[e])}this.compile({loss:n,metrics:s,optimizer:e})}async save(t,n){if("string"==typeof t){const n=e.io.getSaveHandlers(t);if(0===n.length)throw new yi(`Cannot find any save handlers for URL '${t}'`);if(n.length>1)throw new yi(`Found more than one (${n.length}) save handlers for URL '${t}'`);t=n[0]}if(null==t.save)throw new yi("LayersModel.save() cannot proceed because the IOHandler provided does not have the `save` attribute defined.");const s=await e.io.encodeWeights(this.getNamedWeights(n)),i={modelTopology:this.toJSON(null,!1),format:"layers-model",generatedBy:"TensorFlow.js tfjs-layers v3.9.0",convertedBy:null};if(null!=n&&n.includeOptimizer&&null!=this.optimizer){i.trainingConfig=this.getTrainingConfig();const t="optimizer",{data:n,specs:r}=await e.io.encodeWeights(await this.optimizer.getWeights(),t);s.specs.push(...r),s.data=e.io.concatenateArrayBuffers([s.data,n])}if(null!=this.userDefinedMetadata){const t=!0;so(this.userDefinedMetadata,this.name,t),i.userDefinedMetadata=this.userDefinedMetadata}return i.weightData=s.data,i.weightSpecs=s.specs,t.save(i)}setUserDefinedMetadata(t){so(t,this.name),this.userDefinedMetadata=t}getUserDefinedMetadata(){return this.userDefinedMetadata}}Po.className="Model",e.serialization.registerClass(Po);class Wo extends Po{}async function Uo(t,n){if(null==n&&(n={}),"string"==typeof t){const s=e.io.getLoadHandlers(t,n);if(0===s.length)s.push(e.io.browserHTTPRequest(t,n));else if(s.length>1)throw new yi(`Found more than one (${s.length}) load handlers for URL '${t}'`);t=s[0]}return async function(t,n,s){null==s&&(s={});if(null==t.load)throw new yi("Cannot proceed with model loading because the IOHandler provided does not have the `load` method implemented.");const i=await t.load();let r=i.modelTopology;null!=r.model_config&&(r=r.model_config);const a=null==s.strict||s.strict,o=null!=i.weightData&&null!=i.weightSpecs&&a,l=Da(ho(r),n,o),u=i.trainingConfig;null!=u&&l.loadTrainingConfig(u);null!=i.userDefinedMetadata&&l.setUserDefinedMetadata(i.userDefinedMetadata);if(null!=i.weightData){if(null==i.weightSpecs)throw new yi("LayersModel artifacts contains weight data, but not weight specs. Therefore loading of weights cannot proceed.");const{modelWeights:t,optimizerWeights:n}=function(t,n){const s=e.io.decodeWeights(t,n),i={},r=[];return n.forEach((t=>{"optimizer"===t.group?r.push({name:t.name,tensor:s[t.name]}):i[t.name]=s[t.name]})),{modelWeights:i,optimizerWeights:r}}(i.weightData,i.weightSpecs);l.loadWeights(t,a),null!=l.optimizer&&n.length>0&&await l.optimizer.setWeights(n),e.dispose(t),e.dispose(n.map((t=>t.tensor)))}return l}(t,void 0,n)}Wo.className="Functional",e.serialization.registerClass(Wo);class jo extends Po{constructor(t){if(super({inputs:[],outputs:[]}),t=t||{},this.trainable=!0,this.built=!1,this.name=null!=t.name?t.name:sa("sequential_"),null!=t.layers)for(const e of t.layers)this.add(e)}checkShape(t){if(t.inboundNodes[0].outputTensors[0].shape.some((t=>t<0)))throw new yi(`Negative dimension size caused by adding layer ${t.name} with input shape [${t.inboundNodes[0].inputTensors[0].shape}]`)}add(t){const e=t instanceof jo||t instanceof Po;let n;if(e){if(n=t,1!==n.outputs.length)throw new yi("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");if(1!==n.inputs.length)throw new yi("All layers in a Sequential model should have a single input tensor. For multi-input layers, use the functional API.")}if(0===this.outputs.length){if(0===t.inboundNodes.length){if(null==t.batchInputShape)throw new yi("The first layer in a Sequential model must get an `inputShape` or `batchInputShape` argument.");const e=va({batchShape:t.batchInputShape,dtype:t.dtype,name:t.name+"_input"});t.apply(e)}if(e)this.outputs=n.outputs,this.inputs=n.inputs;else{if(1!==t.inboundNodes.length)throw new yi(`A layer added to a Sequential model must not already be connected somewhere else. LayersModel received layer ${t.name} which has ${t.inboundNodes.length} pre-existing inbound connections.`);if(1!==t.inboundNodes[0].outputTensors.length)throw new yi("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");this.checkShape(t),this.outputs=[t.inboundNodes[0].outputTensors[0]],this.inputs=wa(this.outputs[0])}this.inboundNodes=[],new ma({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:this.inputs,outputTensors:this.outputs,inputMasks:ki(null,this.inputs.length),outputMasks:[null],inputShapes:this.inputs.map((t=>t.shape)),outputShapes:this.outputs[0].shape})}else{const e=t.apply(this.outputs[0]);if(Array.isArray(e))throw new TypeError("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");this.checkShape(t),this.outputs=[e],this.inboundNodes[0].outputTensors=this.outputs,this.inboundNodes[0].outputShapes=[this.outputs[0].shape]}this.layers.push(t),this.built=!1}pop(){if(0===this.layers.length)throw new TypeError("There are no layers in the model.");if(this.layers.pop(),0===this.layers.length)this.outputs=[],this.inboundNodes=[],this.outboundNodes=[];else{const t=this.layers.length-1;this.layers[t].outboundNodes=[],this.outputs=[this.layers[t].output],this.inboundNodes[0].outputTensors=this.outputs,this.inboundNodes[0].outputShapes=[this.outputs[0].shape]}}call(t,e){return null==this.model&&this.build(),this.model.call(t,e)}build(t){if(oa(t),0===this.inputs.length||0===this.outputs.length)throw new TypeError("Sequential model cannot be built: model is empty. Add some layers first.");this.model=new Po({inputs:this.inputs,outputs:this.outputs[0],name:this.name+"_model"}),this.model.trainable=this.trainable,this.supportsMasking=this.model.supportsMasking,this.inputLayers=this.model.inputLayers,this.inputLayersNodeIndices=this.model.inputLayersNodeIndices,this.inputLayersTensorIndices=this.model.inputLayersTensorIndices,this.outputLayers=this.model.outputLayers,this.outputLayersNodeIndices=this.model.outputLayersNodeIndices,this.outputLayersTensorIndices=this.model.outputLayersTensorIndices,this.nodesByDepth=this.model.nodesByDepth,this.containerNodes=this.model.containerNodes,this.outputNames=this.model.outputNames,this.inputNames=this.model.inputNames,this.built=!0}countParams(){return this.built||this.build(),super.countParams()}summary(t,e,n=console.log){this.built||this.build(),super.summary(t,e,n)}setWeights(t){null==this.model&&this.build(),this.model.setWeights(t)}evaluate(t,e,n={}){if(!this.built)throw new mi("The model needs to be compiled before being used.");return this.model.evaluate(t,e,n)}async evaluateDataset(t,e){if(!this.built)throw new mi("The model needs to be compiled before being used.");return this.model.evaluateDataset(t,e)}predict(t,e={}){return null==this.model&&this.build(),this.model.predict(t,e)}predictOnBatch(t){return null==this.model&&this.build(),this.model.predictOnBatch(t)}compile(t){this.build(),this.model.compile(t),this.optimizer_=this.model.optimizer,this.isOptimizerOwned=this.model.isOptimizerOwned,this.loss=this.model.loss,this.metrics=this.model.metrics,this.metricsTensors=this.model.metricsTensors,this.metricsNames=this.model.metricsNames}get optimizer(){return null==this.model?void 0:this.model.optimizer}set optimizer(t){this.model.optimizer=t}async fit(t,e,n={}){if(!this.built)throw new mi("The model needs to be compiled before being used.");return this.model.fit(t,e,n)}async fitDataset(t,e){if(!this.built)throw new mi("The model needs to be compiled before being used.");return this.model.fitDataset(t,e)}async trainOnBatch(t,e){return this.model.trainOnBatch(t,e)}static fromConfig(t,n,s={},i=!1){let r,a={};if(n instanceof Array){if(null==n[0].className||"Merge"===n[0].className)throw new yi("Legacy serialization format not supported yet.");r=n}else e.util.assert(null!=n.layers,(()=>"When the config data for a Sequential model is not an Array, it must be an Object that contains the 'layers' field.")),r=n.layers,delete n.layers,a=n;const o=new t(a);if(!(o instanceof jo))throw new bi(`Sequential.fromConfig called on non-Sequential input: ${o}`);for(const t of r){const e=Da(t,void 0,i);i&&e.setFastWeightInitDuringBuild(!0),o.add(e)}return o}set stopTraining(t){if(null==this.model)throw new yi("Cannot set the stopTraining property of a sequential model before it is compiled.");this.model.stopTraining=t}get stopTraining(){if(null==this.model)throw new yi("Cannot get the stopTraining property of a sequential model before it is compiled.");return this.model.stopTraining}getConfig(){const t=[];for(const e of this.layers){const n={};n.className=e.getClassName(),n.config=e.getConfig(),t.push(n)}return{name:this.name,layers:t}}}function Vo(t){return va(t)}jo.className="Sequential",e.serialization.registerClass(jo);class Ko extends e.serialization.Serializable{getConfig(){return{}}}class qo extends Ko{apply(t,n=1){return function(t,n=1){if(1!==n)throw new bi(`Support for alpha values other than 1 (${n}) is not implemented yet.`);return e.elu(t)}(t,n)}}qo.className="elu",e.serialization.registerClass(qo);class Go extends Ko{apply(t){return e.selu(t)}}Go.className="selu",e.serialization.registerClass(Go);class Ho extends Ko{apply(t){return e.relu(t)}}Ho.className="relu",e.serialization.registerClass(Ho);class Jo extends Ko{apply(t){return e.tidy((()=>e.minimum(6,e.relu(t))))}}Jo.className="relu6",e.serialization.registerClass(Jo);class Zo extends Ko{apply(t){return t}}Zo.className="linear",e.serialization.registerClass(Zo);class Yo extends Ko{apply(t){return e.sigmoid(t)}}Yo.className="sigmoid",e.serialization.registerClass(Yo);class Xo extends Ko{apply(t){return function(t){return e.tidy((()=>{const n=e.add(.5,e.mul(.2,t));return e.clipByValue(n,0,1)}))}(t)}}Xo.className="hardSigmoid",e.serialization.registerClass(Xo);class Qo extends Ko{apply(t){return e.softplus(t)}}Qo.className="softplus",e.serialization.registerClass(Qo);class tl extends Ko{apply(t){return function(t){return e.tidy((()=>e.div(t,e.add(e.abs(t),1))))}(t)}}tl.className="softsign",e.serialization.registerClass(tl);class el extends Ko{apply(t){return e.tanh(t)}}el.className="tanh",e.serialization.registerClass(el);class nl extends Ko{apply(t,n=-1){return e.softmax(t,n)}}nl.className="softmax",e.serialization.registerClass(nl);class sl extends Ko{apply(t,n=-1){return e.logSoftmax(t,n)}}sl.className="logSoftmax",e.serialization.registerClass(sl);class il extends Ko{apply(t,n=1){return e.tidy((()=>e.mul(e.sigmoid(e.mul(t,n)),t)))}}il.className="swish",e.serialization.registerClass(il);class rl extends Ko{apply(t){return e.tidy((()=>e.mul(t,e.tanh(e.softplus(t)))))}}function al(t){return t.getClassName()}function ol(t,n={}){return $i(t,e.serialization.SerializationMap.getMap().classNameMap,n,"activation")}function ll(t){if(null==t){const t={className:"linear",config:{}};return ol(t)}if("string"==typeof t){const e={};return e.className=t,e.config={},ol(e)}return t instanceof Ko?t:ol(t)}function ul(t){if(null!=t&&"object"!=typeof t)throw new Error(`Argument to L1L2 regularizer's constructor is expected to be an object, but received: ${t}`)}rl.className="mish",e.serialization.registerClass(rl);class hl extends e.serialization.Serializable{}class cl extends hl{constructor(t){super(),ul(t),this.l1=null==t||null==t.l1?.01:t.l1,this.l2=null==t||null==t.l2?.01:t.l2,this.hasL1=0!==this.l1,this.hasL2=0!==this.l2}apply(t){return e.tidy((()=>{let n=e.zeros([1]);return this.hasL1&&(n=e.add(n,e.sum(e.mul(this.l1,e.abs(t))))),this.hasL2&&(n=e.add(n,e.sum(e.mul(this.l2,Ir(t))))),e.reshape(n,[])}))}getConfig(){return{l1:this.l1,l2:this.l2}}static fromConfig(t,e){return new t({l1:e.l1,l2:e.l2})}}cl.className="L1L2",e.serialization.registerClass(cl);const pl={l1l2:"L1L2"};function dl(t){return Ci(t)}function fl(t,n={}){return $i(t,e.serialization.SerializationMap.getMap().classNameMap,n,"regularizer")}function gl(t){if(null==t)return null;if("string"==typeof t){return fl({className:t in pl?pl[t]:t,config:{}})}return t instanceof hl?t:fl(t)}class ml extends ba{constructor(t){super(null==t?{}:t),this.supportsMasking=!0,null!=t&&(this.maxValue=t.maxValue)}call(t,n){t=aa(t);let s=e.relu(t);return null!=this.maxValue&&(s=e.clipByValue(s,0,this.maxValue)),s}computeOutputShape(t){return t}getConfig(){const t={maxValue:this.maxValue},e=super.getConfig();return Object.assign(t,e),t}}ml.className="ReLU",e.serialization.registerClass(ml);class yl extends ba{constructor(t){super(null==t?{}:t),this.DEFAULT_ALPHA=.3,null==t&&(t={}),this.alpha=null==t.alpha?this.DEFAULT_ALPHA:t.alpha}call(t,n){const s=aa(t);return e.leakyRelu(s,this.alpha)}computeOutputShape(t){return t}getConfig(){const t={alpha:this.alpha},e=super.getConfig();return Object.assign(t,e),t}}yl.className="LeakyReLU",e.serialization.registerClass(yl);class bl extends ba{constructor(t){if(super(null==t?{}:t),this.DEFAULT_ALPHA_INITIALIZER="zeros",null==t&&(t={}),this.supportsMasking=!0,this.alphaInitializer=Xr(t.alphaInitializer||this.DEFAULT_ALPHA_INITIALIZER),this.alphaRegularizer=gl(t.alphaRegularizer),this.alphaConstraint=Hi(t.alphaConstraint),null==t.sharedAxes)this.sharedAxes=null;else if(Array.isArray(t.sharedAxes))this.sharedAxes=t.sharedAxes;else{if("number"!=typeof t.sharedAxes)throw new yi(`Expected sharedAxes to be a number or an array of numbers, but got ${t.sharedAxes}`);this.sharedAxes=[t.sharedAxes]}}build(t){const e=(t=oa(t)).slice(1);if(null!=this.sharedAxes)for(const t of this.sharedAxes)e[t-1]=1;this.alpha=this.addWeight("alpha",e,"float32",this.alphaInitializer,this.alphaRegularizer,!0,this.alphaConstraint);const n={};if(null!=this.sharedAxes)for(let e=1;e<t.length;++e)n[e]=t[e];this.inputSpec=[new da({ndim:t.length,axes:n})],this.built=!0}call(t,n){return t=aa(t),e.prelu(t,this.alpha.read())}getConfig(){const t={alphaInitializer:Yr(this.alphaInitializer),alphaRegularizer:dl(this.alphaRegularizer),alphaConstraint:qi(this.alphaConstraint),sharedAxes:this.sharedAxes},e=super.getConfig();return Object.assign(t,e),t}}bl.className="PReLU",e.serialization.registerClass(bl);class wl extends ba{constructor(t){if(super(null==t?{}:t),this.DEFAULT_ALPHA=1,null==t&&(t={}),null!=t.alpha&&t.alpha!==this.DEFAULT_ALPHA)throw new bi(`Non-default alpha value (${t.alpha}) is not supported by the ELU layer yet.`);this.alpha=null==t.alpha?this.DEFAULT_ALPHA:t.alpha}call(t,n){const s=aa(t);return e.elu(s)}computeOutputShape(t){return t}getConfig(){const t={alpha:this.alpha},e=super.getConfig();return Object.assign(t,e),t}}wl.className="ELU",e.serialization.registerClass(wl);class kl extends ba{constructor(t){super(null==t?{}:t),this.DEFAULT_THETA=1,null==t&&(t={}),this.theta=null==t.theta?this.DEFAULT_THETA:t.theta}call(t,n){const s=aa(t);return e.mul(s,e.cast(e.greater(s,this.theta),"float32"))}computeOutputShape(t){return t}getConfig(){const t={theta:this.theta},e=super.getConfig();return Object.assign(t,e),t}}kl.className="ThresholdedReLU",e.serialization.registerClass(kl);class vl extends ba{constructor(t){super(null==t?{}:t),this.DEFAULT_AXIS=1,null==t&&(t={}),this.softmax=(new nl).apply,this.axis=null==t.axis?this.DEFAULT_AXIS:t.axis}call(t,e){const n=aa(t);return this.softmax(n,this.axis)}computeOutputShape(t){return t}getConfig(){const t={axis:this.axis},e=super.getConfig();return Object.assign(t,e),t}}function Sl(t,e,n){if("number"==typeof t)return ki(t,e);if(t.length!==e)throw new yi(`The ${n} argument must be an integer or tuple of ${e} integers. Received: ${t.length} elements.`);for(let i=0;i<e;++i){const r=t[i];if((s=r)!==parseInt(s.toString(),10))throw new yi(`The ${n} argument must be an integer or tuple of ${e} integers. Received: ${JSON.stringify(t)} including a non-integer number ${r}`)}return t;var s}function xl(t,e,n,s,i=1){if(null==t)return t;let r;return r="same"===n?t:t-(e+(e-1)*(i-1))+1,Math.floor((r+s-1)/s)}function Nl(t,e,n,s){if(null==t)return null;if("valid"===s)t=t*e+dr([n-e,0]);else{if("same"!==s)throw new yi(`Unsupport padding mode: ${s}.`);t*=e}return t}function zl(t,n){return e.tidy((()=>(nr(n),"channelsFirst"===n?e.transpose(t,[0,2,3,1]):t)))}function Il(t,n){return e.tidy((()=>(nr(n),"channelsFirst"===n?e.transpose(t,[0,2,3,4,1]):t)))}function Al(t,n,s,i=[1,1],r="valid",a,o,l=null){return e.tidy((()=>{if(null==a&&(a="channelsLast"),nr(a),3!==t.rank&&4!==t.rank)throw new yi(`conv2dWithBiasActivation expects input to be of rank 3 or 4, but received ${t.rank}.`);if(3!==n.rank&&4!==n.rank)throw new yi(`conv2dWithBiasActivation expects kernel to be of rank 3 or 4, but received ${t.rank}.`);let u=zl(t,a);if("causal"===r)throw new bi("The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.");return u=e.fused.conv2d({x:u,filter:n,strides:i,pad:"same"===r?"same":"valid",dilations:o,dataFormat:"NHWC",bias:s,activation:l}),"channelsFirst"===a&&(u=e.transpose(u,[0,3,1,2])),u}))}vl.className="Softmax",e.serialization.registerClass(vl);class Cl extends ba{constructor(t,e){if(super(e),this.bias=null,this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_BIAS_INITIALIZER="zeros",Cl.verifyArgs(e),this.rank=t,Ri(this.rank,"rank"),1!==this.rank&&2!==this.rank&&3!==this.rank)throw new bi(`Convolution layer for rank other than 1, 2, or 3 (${this.rank}) is not implemented yet.`);if(this.kernelSize=Sl(e.kernelSize,t,"kernelSize"),this.strides=Sl(null==e.strides?1:e.strides,t,"strides"),this.padding=null==e.padding?"valid":e.padding,sr(this.padding),this.dataFormat=null==e.dataFormat?"channelsLast":e.dataFormat,nr(this.dataFormat),this.activation=ll(e.activation),this.useBias=null==e.useBias||e.useBias,this.biasInitializer=Xr(e.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.biasConstraint=Hi(e.biasConstraint),this.biasRegularizer=gl(e.biasRegularizer),this.activityRegularizer=gl(e.activityRegularizer),this.dilationRate=Sl(null==e.dilationRate?1:e.dilationRate,t,"dilationRate"),1===this.rank&&Array.isArray(this.dilationRate)&&1!==this.dilationRate.length)throw new yi(`dilationRate must be a number or an array of a single number for 1D convolution, but received ${JSON.stringify(this.dilationRate)}`);if(2===this.rank){if("number"==typeof this.dilationRate)this.dilationRate=[this.dilationRate,this.dilationRate];else if(2!==this.dilationRate.length)throw new yi(`dilationRate must be a number or array of two numbers for 2D convolution, but received ${JSON.stringify(this.dilationRate)}`)}else if(3===this.rank)if("number"==typeof this.dilationRate)this.dilationRate=[this.dilationRate,this.dilationRate,this.dilationRate];else if(3!==this.dilationRate.length)throw new yi(`dilationRate must be a number or array of three numbers for 3D convolution, but received ${JSON.stringify(this.dilationRate)}`)}static verifyArgs(t){if(vi("kernelSize"in t,"required key 'kernelSize' not in config"),"number"!=typeof t.kernelSize&&!_i(t.kernelSize,"number",1,3))throw new yi(`BaseConv expects config.kernelSize to be number or number[] with length 1, 2, or 3, but received ${JSON.stringify(t.kernelSize)}.`)}getConfig(){const t={kernelSize:this.kernelSize,strides:this.strides,padding:this.padding,dataFormat:this.dataFormat,dilationRate:this.dilationRate,activation:al(this.activation),useBias:this.useBias,biasInitializer:Yr(this.biasInitializer),biasRegularizer:dl(this.biasRegularizer),activityRegularizer:dl(this.activityRegularizer),biasConstraint:qi(this.biasConstraint)},e=super.getConfig();return Object.assign(t,e),t}}class Tl extends Cl{constructor(t,e){super(t,e),this.kernel=null,Tl.verifyArgs(e),this.filters=e.filters,Ri(this.filters,"filters"),this.kernelInitializer=Xr(e.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.kernelConstraint=Hi(e.kernelConstraint),this.kernelRegularizer=gl(e.kernelRegularizer)}build(t){t=oa(t);const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new yi(`The channel dimension of the input should be defined. Found ${t[e]}`);const n=t[e],s=this.kernelSize.concat([n,this.filters]);this.kernel=this.addWeight("kernel",s,null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[{ndim:this.rank+2,axes:{[e]:n}}],this.built=!0}call(t,n){return e.tidy((()=>{let n;t=aa(t);const s=null==this.bias?null:this.bias.read(),i=Oi(this.activation.getClassName());if(null!=i&&2===this.rank)n=Al(t,this.kernel.read(),s,this.strides,this.padding,this.dataFormat,this.dilationRate,i);else{if(1===this.rank)n=function(t,n,s,i=1,r="valid",a,o=1){return e.tidy((()=>{if(null==a&&(a="channelsLast"),nr(a),3!==t.shape.length)throw new yi(`The input of a conv1dWithBias operation should be 3, but is ${t.shape.length} instead.`);if(3!==n.shape.length)throw new yi(`The kernel for a conv1dWithBias operation should be 3, but is ${n.shape.length} instead`);if(null!=s&&1!==s.shape.length)throw new yi(`The bias for a conv1dWithBias operation should be 1, but is ${n.shape.length} instead`);if("channelsFirst"===a&&(t=e.transpose(t,[0,2,1])),"causal"===r)throw new bi("The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.");let l=e.conv1d(t,n,i,"same"===r?"same":"valid","NWC",o);return null!=s&&(l=Cr(l,s)),l}))}(t,this.kernel.read(),s,this.strides[0],this.padding,this.dataFormat,this.dilationRate[0]);else if(2===this.rank)n=Al(t,this.kernel.read(),s,this.strides,this.padding,this.dataFormat,this.dilationRate);else{if(3!==this.rank)throw new bi("convolutions greater than 3D are not implemented yet.");n=function(t,n,s,i=[1,1,1],r="valid",a,o){return e.tidy((()=>{if(null==a&&(a="channelsLast"),nr(a),4!==t.rank&&5!==t.rank)throw new yi(`conv3dWithBias expects input to be of rank 4 or 5, but received ${t.rank}.`);if(4!==n.rank&&5!==n.rank)throw new yi(`conv3dWithBias expects kernel to be of rank 4 or 5, but received ${t.rank}.`);let l=Il(t,a);if("causal"===r)throw new bi("The support for CAUSAL padding mode in conv3dWithBias is not implemented yet.");return l=e.conv3d(l,n,i,"same"===r?"same":"valid","NDHWC",o),null!=s&&(l=Cr(l,s)),"channelsFirst"===a&&(l=e.transpose(l,[0,4,1,2,3])),l}))}(t,this.kernel.read(),s,this.strides,this.padding,this.dataFormat,this.dilationRate)}null!=this.activation&&(n=this.activation.apply(n))}return n}))}computeOutputShape(t){t=oa(t);const e=[],n="channelsLast"===this.dataFormat?t.slice(1,t.length-1):t.slice(2);for(let t=0;t<n.length;++t){const s=xl(n[t],this.kernelSize[t],this.padding,this.strides[t],"number"==typeof this.dilationRate?this.dilationRate:this.dilationRate[t]);e.push(s)}let s=[t[0]];return"channelsLast"===this.dataFormat?(s=s.concat(e),s.push(this.filters)):(s.push(this.filters),s=s.concat(e)),s}getConfig(){const t={filters:this.filters,kernelInitializer:Yr(this.kernelInitializer),kernelRegularizer:dl(this.kernelRegularizer),kernelConstraint:qi(this.kernelConstraint)},e=super.getConfig();return Object.assign(t,e),t}static verifyArgs(t){if(!("filters"in t)||"number"!=typeof t.filters||t.filters<1)throw new yi(`Convolution layer expected config.filters to be a 'number' > 0 but got ${JSON.stringify(t.filters)}`)}}class $l extends Tl{constructor(t){super(2,t),$l.verifyArgs(t)}getConfig(){const t=super.getConfig();return delete t.rank,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&!_i(t.kernelSize,"number",1,2))throw new yi(`Conv2D expects config.kernelSize to be number or number[] with length 1 or 2, but received ${JSON.stringify(t.kernelSize)}.`)}}$l.className="Conv2D",e.serialization.registerClass($l);class El extends Tl{constructor(t){super(3,t),El.verifyArgs(t)}getConfig(){const t=super.getConfig();return delete t.rank,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&(!Array.isArray(t.kernelSize)||1!==t.kernelSize.length&&3!==t.kernelSize.length))throw new yi(`Conv3D expects config.kernelSize to be number or [number, number, number], but received ${JSON.stringify(t.kernelSize)}.`)}}El.className="Conv3D",e.serialization.registerClass(El);class Fl extends $l{constructor(t){if(super(t),this.inputSpec=[new da({ndim:4})],"same"!==this.padding&&"valid"!==this.padding)throw new yi(`Conv2DTranspose currently supports only padding modes 'same' and 'valid', but received padding mode ${this.padding}`)}build(t){if(4!==(t=oa(t)).length)throw new yi("Input should have rank 4; Received input shape: "+JSON.stringify(t));const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new yi("The channel dimension of the inputs should be defined. Found `None`.");const n=t[e],s=this.kernelSize.concat([this.filters,n]);this.kernel=this.addWeight("kernel",s,"float32",this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[new da({ndim:4,axes:{[e]:n}})],this.built=!0}call(t,n){return e.tidy((()=>{let n=aa(t);if(4!==n.shape.length)throw new yi(`Conv2DTranspose.call() expects input tensor to be rank-4, but received a tensor of rank-${n.shape.length}`);const s=n.shape,i=s[0];let r,a;"channelsFirst"===this.dataFormat?(r=2,a=3):(r=1,a=2);const o=s[r],l=s[a],u=this.kernelSize[0],h=this.kernelSize[1],c=this.strides[0],p=this.strides[1],d=[i,Nl(o,c,u,this.padding),Nl(l,p,h,this.padding),this.filters];"channelsLast"!==this.dataFormat&&(n=e.transpose(n,[0,2,3,1]));let f=e.conv2dTranspose(n,this.kernel.read(),d,this.strides,this.padding);return"channelsLast"!==this.dataFormat&&(f=e.transpose(f,[0,3,1,2])),null!=this.bias&&(f=Cr(f,this.bias.read(),this.dataFormat)),null!=this.activation&&(f=this.activation.apply(f)),f}))}computeOutputShape(t){const e=(t=oa(t)).slice();let n,s,i;"channelsFirst"===this.dataFormat?(n=1,s=2,i=3):(n=3,s=1,i=2);const r=this.kernelSize[0],a=this.kernelSize[1],o=this.strides[0],l=this.strides[1];return e[n]=this.filters,e[s]=Nl(e[s],o,r,this.padding),e[i]=Nl(e[i],l,a,this.padding),e}getConfig(){const t=super.getConfig();return delete t.dilationRate,t}}Fl.className="Conv2DTranspose",e.serialization.registerClass(Fl);class Dl extends El{constructor(t){if(super(t),this.inputSpec=[new da({ndim:5})],"same"!==this.padding&&"valid"!==this.padding)throw new yi(`Conv3DTranspose currently supports only padding modes 'same' and 'valid', but received padding mode ${this.padding}`)}build(t){if(5!==(t=oa(t)).length)throw new yi("Input should have rank 5; Received input shape: "+JSON.stringify(t));const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new yi("The channel dimension of the inputs should be defined. Found `None`.");const n=t[e],s=this.kernelSize.concat([this.filters,n]);this.kernel=this.addWeight("kernel",s,"float32",this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[new da({ndim:5,axes:{[e]:n}})],this.built=!0}call(t,n){return e.tidy((()=>{let n=aa(t);if(5!==n.shape.length)throw new yi(`Conv3DTranspose.call() expects input tensor to be rank-4, but received a tensor of rank-${n.shape.length}`);const s=n.shape,i=s[0];let r,a,o;"channelsFirst"===this.dataFormat?(o=2,r=3,a=4):(o=1,r=2,a=3);const l=s[o],u=s[r],h=s[a],c=this.kernelSize[0],p=this.kernelSize[1],d=this.kernelSize[2],f=this.strides[0],g=this.strides[1],m=this.strides[2],y=[i,Nl(l,f,c,this.padding),Nl(u,g,p,this.padding),Nl(h,m,d,this.padding),this.filters];"channelsLast"!==this.dataFormat&&(n=e.transpose(n,[0,2,3,4,1]));let b=e.conv3dTranspose(n,this.kernel.read(),y,this.strides,this.padding);return"channelsLast"!==this.dataFormat&&(b=e.transpose(b,[0,4,1,2,3])),null!==this.bias&&(b=Cr(b,this.bias.read(),this.dataFormat)),null!==this.activation&&(b=this.activation.apply(b)),b}))}computeOutputShape(t){const e=(t=oa(t)).slice();let n,s,i,r;"channelsFirst"===this.dataFormat?(n=1,s=2,i=3,r=4):(n=4,s=1,i=2,r=3);const a=this.kernelSize[0],o=this.kernelSize[1],l=this.kernelSize[2],u=this.strides[0],h=this.strides[1],c=this.strides[2];return e[n]=this.filters,e[s]=Nl(e[s],u,a,this.padding),e[i]=Nl(e[i],h,o,this.padding),e[r]=Nl(e[r],c,l,this.padding),e}getConfig(){const t=super.getConfig();return delete t.dilationRate,t}}Dl.className="Conv3DTranspose",e.serialization.registerClass(Dl);class Ll extends Tl{constructor(t,e){if(super(t,e),this.DEFAULT_DEPTHWISE_INITIALIZER="glorotUniform",this.DEFAULT_POINTWISE_INITIALIZER="glorotUniform",this.depthwiseKernel=null,this.pointwiseKernel=null,null==e.filters)throw new yi("The `filters` configuration field is required by SeparableConv, but is unspecified.");if(null!=e.kernelInitializer||null!=e.kernelRegularizer||null!=e.kernelConstraint)throw new yi("Fields kernelInitializer, kernelRegularizer and kernelConstraint are invalid for SeparableConv2D. Use depthwiseInitializer, depthwiseRegularizer, depthwiseConstraint, pointwiseInitializer, pointwiseRegularizer and pointwiseConstraint instead.");if(null!=e.padding&&"same"!==e.padding&&"valid"!==e.padding)throw new yi(`SeparableConv${this.rank}D supports only padding modes: 'same' and 'valid', but received ${JSON.stringify(e.padding)}`);this.depthMultiplier=null==e.depthMultiplier?1:e.depthMultiplier,this.depthwiseInitializer=Xr(e.depthwiseInitializer||this.DEFAULT_DEPTHWISE_INITIALIZER),this.depthwiseRegularizer=gl(e.depthwiseRegularizer),this.depthwiseConstraint=Hi(e.depthwiseConstraint),this.pointwiseInitializer=Xr(e.depthwiseInitializer||this.DEFAULT_POINTWISE_INITIALIZER),this.pointwiseRegularizer=gl(e.pointwiseRegularizer),this.pointwiseConstraint=Hi(e.pointwiseConstraint)}build(t){if((t=oa(t)).length<this.rank+2)throw new yi(`Inputs to SeparableConv${this.rank}D should have rank ${this.rank+2}, but received input shape: ${JSON.stringify(t)}`);const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e]||t[e]<0)throw new yi(`The channel dimension of the inputs should be defined, but found ${JSON.stringify(t[e])}`);const n=t[e],s=this.kernelSize.concat([n,this.depthMultiplier]),i=[];for(let t=0;t<this.rank;++t)i.push(1);i.push(n*this.depthMultiplier,this.filters);const r=!0;this.depthwiseKernel=this.addWeight("depthwise_kernel",s,"float32",this.depthwiseInitializer,this.depthwiseRegularizer,r,this.depthwiseConstraint),this.pointwiseKernel=this.addWeight("pointwise_kernel",i,"float32",this.pointwiseInitializer,this.pointwiseRegularizer,r,this.pointwiseConstraint),this.useBias?this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,r,this.biasConstraint):this.bias=null,this.inputSpec=[new da({ndim:this.rank+2,axes:{[e]:n}})],this.built=!0}call(t,n){return e.tidy((()=>{let n;if(t=aa(t),1===this.rank)throw new bi("1D separable convolution is not implemented yet.");return 2===this.rank&&("channelsFirst"===this.dataFormat&&(t=e.transpose(t,[0,2,3,1])),n=e.separableConv2d(t,this.depthwiseKernel.read(),this.pointwiseKernel.read(),this.strides,this.padding,this.dilationRate,"NHWC")),this.useBias&&(n=Cr(n,this.bias.read(),this.dataFormat)),null!=this.activation&&(n=this.activation.apply(n)),"channelsFirst"===this.dataFormat&&(n=e.transpose(n,[0,3,1,2])),n}))}getConfig(){const t=super.getConfig();return delete t.rank,delete t.kernelInitializer,delete t.kernelRegularizer,delete t.kernelConstraint,t.depthwiseInitializer=Yr(this.depthwiseInitializer),t.pointwiseInitializer=Yr(this.pointwiseInitializer),t.depthwiseRegularizer=dl(this.depthwiseRegularizer),t.pointwiseRegularizer=dl(this.pointwiseRegularizer),t.depthwiseConstraint=qi(this.depthwiseConstraint),t.pointwiseConstraint=qi(this.pointwiseConstraint),t}}Ll.className="SeparableConv";class _l extends Ll{constructor(t){super(2,t)}}_l.className="SeparableConv2D",e.serialization.registerClass(_l);class Rl extends Tl{constructor(t){super(1,t),Rl.verifyArgs(t),this.inputSpec=[{ndim:3}]}getConfig(){const t=super.getConfig();return delete t.rank,delete t.dataFormat,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&!_i(t.kernelSize,"number",1,1))throw new yi(`Conv1D expects config.kernelSize to be number or number[] with length 1, but received ${JSON.stringify(t.kernelSize)}.`)}}Rl.className="Conv1D",e.serialization.registerClass(Rl);class Ml extends ba{constructor(t){super(t),"number"==typeof t.cropping?this.cropping=[[t.cropping,t.cropping],[t.cropping,t.cropping]]:"number"==typeof t.cropping[0]?this.cropping=[[t.cropping[0],t.cropping[0]],[t.cropping[1],t.cropping[1]]]:this.cropping=t.cropping,this.dataFormat=void 0===t.dataFormat?"channelsLast":t.dataFormat,this.inputSpec=[{ndim:4}]}computeOutputShape(t){return"channelsFirst"===this.dataFormat?[t[0],t[1],t[2]-this.cropping[0][0]-this.cropping[0][1],t[3]-this.cropping[1][0]-this.cropping[1][1]]:[t[0],t[1]-this.cropping[0][0]-this.cropping[0][1],t[2]-this.cropping[1][0]-this.cropping[1][1],t[3]]}call(t,n){return e.tidy((()=>{if(t=aa(t),"channelsLast"===this.dataFormat){const e=wr(t,this.cropping[0][0],t.shape[1]-this.cropping[0][0]-this.cropping[0][1],2);return wr(e,this.cropping[1][0],t.shape[2]-this.cropping[1][1]-this.cropping[1][0],3)}{const e=wr(t,this.cropping[0][0],t.shape[2]-this.cropping[0][0]-this.cropping[0][1],3);return wr(e,this.cropping[1][0],t.shape[3]-this.cropping[1][1]-this.cropping[1][0],4)}}))}getConfig(){const t={cropping:this.cropping,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}Ml.className="Cropping2D",e.serialization.registerClass(Ml);class Ol extends ba{constructor(t){var e;super(t),this.DEFAULT_SIZE=[2,2],this.inputSpec=[{ndim:4}],this.size=null==t.size?this.DEFAULT_SIZE:t.size,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,nr(this.dataFormat),this.interpolation=null==t.interpolation?"nearest":t.interpolation,e=this.interpolation,Li(Yi,"InterpolationFormat",e)}computeOutputShape(t){if("channelsFirst"===this.dataFormat){const e=null==t[2]?null:this.size[0]*t[2],n=null==t[3]?null:this.size[1]*t[3];return[t[0],t[1],e,n]}{const e=null==t[1]?null:this.size[0]*t[1],n=null==t[2]?null:this.size[1]*t[2];return[t[0],e,n,t[3]]}}call(t,n){return e.tidy((()=>{let n=aa(t);const s=n.shape;if("channelsFirst"===this.dataFormat){n=e.transpose(n,[0,2,3,1]);const t=this.size[0]*s[2],i=this.size[1]*s[3],r="nearest"===this.interpolation?e.image.resizeNearestNeighbor(n,[t,i]):e.image.resizeBilinear(n,[t,i]);return e.transpose(r,[0,3,1,2])}{const t=this.size[0]*s[1],i=this.size[1]*s[2];return"nearest"===this.interpolation?e.image.resizeNearestNeighbor(n,[t,i]):e.image.resizeBilinear(n,[t,i])}}))}getConfig(){const t={size:this.size,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}Ol.className="UpSampling2D",e.serialization.registerClass(Ol);class Bl extends Cl{constructor(t){super(2,t),this.depthwiseKernel=null,this.depthMultiplier=null==t.depthMultiplier?1:t.depthMultiplier,this.depthwiseInitializer=Xr(t.depthwiseInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.depthwiseConstraint=Hi(t.depthwiseConstraint),this.depthwiseRegularizer=gl(t.depthwiseRegularizer)}build(t){if((t=oa(t)).length<4)throw new yi(`Inputs to DepthwiseConv2D should have rank 4. Received input shape: ${JSON.stringify(t)}.`);const e="channelsFirst"===this.dataFormat?1:3;if(null==t[e]||t[e]<0)throw new yi(`The channel dimension of the inputs to DepthwiseConv2D should be defined, but is not (${t[e]}).`);const n=t[e],s=[this.kernelSize[0],this.kernelSize[1],n,this.depthMultiplier];this.depthwiseKernel=this.addWeight("depthwise_kernel",s,null,this.depthwiseInitializer,this.depthwiseRegularizer,!0,this.depthwiseConstraint),this.useBias?this.bias=this.addWeight("bias",[n*this.depthMultiplier],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,n){return e.tidy((()=>{let n=function(t,n,s=[1,1],i="valid",r,a){return e.tidy((()=>{null==r&&(r="channelsLast"),nr(r);let o=zl(t,r);if(4!==t.rank)throw new yi(`Input for depthwiseConv2d is required to be 4-D, but is instead ${t.rank}-D`);if(4!==n.rank)throw new yi(`depthwiseKernel is required to be 4-D, but is instead ${n.rank}-D`);return o=e.depthwiseConv2d(o,n,s,"same"===i?"same":"valid","NHWC",a),"channelsFirst"===r&&(o=e.transpose(o,[0,3,1,2])),o}))}(t=aa(t),this.depthwiseKernel.read(),this.strides,this.padding,this.dataFormat,null);return this.useBias&&(n=Cr(n,this.bias.read(),this.dataFormat)),null!=this.activation&&(n=this.activation.apply(n)),n}))}computeOutputShape(t){t=oa(t);const e="channelsFirst"===this.dataFormat?t[2]:t[1],n="channelsFirst"===this.dataFormat?t[3]:t[2],s="channelsFirst"===this.dataFormat?t[1]*this.depthMultiplier:t[3]*this.depthMultiplier,i=xl(e,this.kernelSize[0],this.padding,this.strides[0]),r=xl(n,this.kernelSize[1],this.padding,this.strides[1]);return"channelsFirst"===this.dataFormat?[t[0],s,i,r]:[t[0],i,r,s]}getConfig(){const t=super.getConfig();return t.depthMultiplier=this.depthMultiplier,t.depthwiseInitializer=Yr(this.depthwiseInitializer),t.depthwiseRegularizer=dl(this.depthwiseRegularizer),t.depthwiseConstraint=qi(this.depthwiseRegularizer),t}}function Pl(t,e,n,s){if(Array.isArray(t)){if(null!=e||null!=n)throw new yi("When inputs is an array, neither initialState or constants should be provided");null!=s&&(n=t.slice(t.length-s,t.length),t=t.slice(0,t.length-s)),t.length>1&&(e=t.slice(1,t.length)),t=t[0]}function i(t){return null==t||Array.isArray(t)?t:[t]}return{inputs:t,initialState:e=i(e),constants:n=i(n)}}function Wl(t,n,s,i=!1,r,a,o=!1,l=!1){return e.tidy((()=>{const u=n.shape.length;if(u<3)throw new yi(`Input should be at least 3D, but is ${u}D.`);const h=[1,0].concat(fr(2,u));if(n=e.transpose(n,h),null!=a)throw new bi("The rnn() functoin of the deeplearn.js backend does not support constants yet.");o&&console.warn("Backend rnn(): the unroll = true option is not applicable to the imperative deeplearn.js backend."),null!=r&&((r=e.cast(e.cast(r,"bool"),"float32")).rank===u-1&&(r=e.expandDims(r,-1)),r=e.transpose(r,h)),i&&(n=e.reverse(n,0),null!=r&&(r=e.reverse(r,0)));const c=[];let p,d=s;const f=n.shape[0],g=e.unstack(n);let m,y;null!=r&&(m=e.unstack(r));for(let n=0;n<f;++n){const s=g[n],i=e.tidy((()=>t(s,d)));if(null==r)p=i[0],d=i[1];else{const t=e.tidy((()=>{const t=m[n],s=e.sub(e.onesLike(t),t);return{output:e.add(e.mul(i[0],t),e.mul(d[0],s)),newStates:d.map(((n,r)=>e.add(e.mul(i[1][r],t),e.mul(n,s))))}}));p=t.output,d=t.newStates}l&&c.push(p)}if(l){const t=1;y=e.stack(c,t)}return[p,y,d]}))}Bl.className="DepthwiseConv2D",e.serialization.registerClass(Bl);class Ul extends ba{constructor(t){let e;if(super(t),null==t.cell)throw new yi("cell property is missing for the constructor of RNN.");if(e=Array.isArray(t.cell)?new Zl({cells:t.cell}):t.cell,null==e.stateSize)throw new yi("The RNN cell should have an attribute `stateSize` (tuple of integers, one integer per RNN state).");this.cell=e,this.returnSequences=null!=t.returnSequences&&t.returnSequences,this.returnState=null!=t.returnState&&t.returnState,this.goBackwards=null!=t.goBackwards&&t.goBackwards,this._stateful=null!=t.stateful&&t.stateful,this.unroll=null!=t.unroll&&t.unroll,this.supportsMasking=!0,this.inputSpec=[new da({ndim:3})],this.stateSpec=null,this.states_=null,this.numConstants=null,this.keptStates=[]}getStates(){if(null==this.states_){return fr(0,Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1).map((t=>null))}return this.states_}setStates(t){this.states_=t}computeOutputShape(t){ia(t)&&(t=t[0]),t=t;let e=this.cell.stateSize;Array.isArray(e)||(e=[e]);const n=e[0];let s;if(s=this.returnSequences?[t[0],t[1],n]:[t[0],n],this.returnState){const n=[];for(const s of e)n.push([t[0],s]);return[s].concat(n)}return s}computeMask(t,n){return e.tidy((()=>{Array.isArray(n)&&(n=n[0]);const t=this.returnSequences?n:null;if(this.returnState){const e=this.states.map((t=>null));return[t].concat(e)}return t}))}get states(){if(null==this.states_){const t=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1,e=[];for(let n=0;n<t;++n)e.push(null);return e}return this.states_}set states(t){this.states_=t}build(t){if(null!=this.numConstants)throw new bi("Constants support is not implemented in RNN yet.");ia(t)&&(t=t[0]),t=t;const n=this.stateful?t[0]:null,s=t.slice(2);this.inputSpec[0]=new da({shape:[n,null,...s]});const i=[t[0]].concat(t.slice(2));let r;if(this.cell.build(i),r=Array.isArray(this.cell.stateSize)?this.cell.stateSize:[this.cell.stateSize],null!=this.stateSpec){if(!e.util.arraysEqual(this.stateSpec.map((t=>t.shape[t.shape.length-1])),r))throw new yi(`An initialState was passed that is not compatible with cell.stateSize. Received stateSpec=${this.stateSpec}; However cell.stateSize is ${this.cell.stateSize}`)}else this.stateSpec=r.map((t=>new da({shape:[null,t]})));this.stateful&&this.resetStates()}resetStates(t,n=!1){e.tidy((()=>{if(!this.stateful)throw new gi("Cannot call resetStates() on an RNN Layer that is not stateful.");const s=this.inputSpec[0].shape[0];if(null==s)throw new yi("If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.");if(null==this.states_)Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((t=>e.zeros([s,t]))):this.states_=[e.zeros([s,this.cell.stateSize])];else if(null==t)e.dispose(this.states_),null!=this.keptStates&&(e.dispose(this.keptStates),this.keptStates=[]),Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((t=>e.zeros([s,t]))):this.states_[0]=e.zeros([s,this.cell.stateSize]);else{if(Array.isArray(t)||(t=[t]),t.length!==this.states_.length)throw new yi(`Layer ${this.name} expects ${this.states_.length} state(s), but it received ${t.length} state value(s). Input received: ${t}`);!0===n?this.keptStates.push(this.states_.slice()):e.dispose(this.states_);for(let n=0;n<this.states_.length;++n){const i=t[n],r=Array.isArray(this.cell.stateSize)?this.cell.stateSize[n]:this.cell.stateSize,a=[s,r];if(!e.util.arraysEqual(i.shape,a))throw new yi(`State ${n} is incompatible with layer ${this.name}: expected shape=${a}, received shape=${i.shape}`);this.states_[n]=i}}this.states_=this.states_.map((t=>e.keep(t.clone())))}))}apply(t,e){let n=null==e?null:e.initialState,s=null==e?null:e.constants;null==e&&(e={});const i=Pl(t,n,s,this.numConstants);t=i.inputs,n=i.initialState,s=i.constants;let r=[],a=[];if(null!=n){e.initialState=n,r=r.concat(n),this.stateSpec=[];for(const t of n)this.stateSpec.push(new da({shape:t.shape}));a=a.concat(this.stateSpec)}null!=s&&(e.constants=s,r=r.concat(s),this.numConstants=s.length);if(r[0]instanceof fa){const n=[t].concat(r),s=this.inputSpec.concat(a),i=this.inputSpec;this.inputSpec=s;const o=super.apply(n,e);return this.inputSpec=i,o}return super.apply(t,e)}call(t,n){return e.tidy((()=>{const e=null==n?null:n.mask,s=null==n?null:n.training;let i=null==n?null:n.initialState;t=aa(t),null==i&&(i=this.stateful?this.states_:this.getInitialState(t));const r=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1;if(i.length!==r)throw new yi(`RNN Layer has ${r} state(s) but was passed ${i.length} initial state(s).`);this.unroll&&console.warn("Ignoring unroll = true for RNN layer, due to imperative backend.");const a={training:s},o=Wl(((t,e)=>{const n=this.cell.call([t].concat(e),a);return[n[0],n.slice(1)]}),t,i,this.goBackwards,e,null,this.unroll,this.returnSequences),l=o[0],u=o[1],h=o[2];this.stateful&&this.resetStates(h,s);const c=this.returnSequences?u:l;return this.returnState?[c].concat(h):c}))}getInitialState(t){return e.tidy((()=>{let n=e.zeros(t.shape);return n=e.sum(n,[1,2]),n=mr(n),Array.isArray(this.cell.stateSize)?this.cell.stateSize.map((t=>t>1?Sr(n,[1,t]):n)):this.cell.stateSize>1?[Sr(n,[1,this.cell.stateSize])]:[n]}))}get trainableWeights(){return this.trainable?this.cell.trainableWeights:[]}get nonTrainableWeights(){return this.trainable?this.cell.nonTrainableWeights:this.cell.weights}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.cell&&this.cell.setFastWeightInitDuringBuild(t)}getConfig(){const t=super.getConfig(),e={returnSequences:this.returnSequences,returnState:this.returnState,goBackwards:this.goBackwards,stateful:this.stateful,unroll:this.unroll};null!=this.numConstants&&(e.numConstants=this.numConstants);const n=this.cell.getConfig();return this.getClassName()===Ul.className&&(e.cell={className:this.cell.getClassName(),config:n}),Object.assign({},n,t,e)}static fromConfig(t,e,n={}){const s=Da(e.cell,n);return new t(Object.assign(e,{cell:s}))}}Ul.className="RNN",e.serialization.registerClass(Ul);class jl extends ba{}class Vl extends jl{constructor(t){super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",this.units=t.units,Ri(this.units,"units"),this.activation=ll(null==t.activation?this.DEFAULT_ACTIVATION:t.activation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=Xr(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=Xr(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=Xr(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelRegularizer=gl(t.kernelRegularizer),this.recurrentRegularizer=gl(t.recurrentRegularizer),this.biasRegularizer=gl(t.biasRegularizer),this.kernelConstraint=Hi(t.kernelConstraint),this.recurrentConstraint=Hi(t.recurrentConstraint),this.biasConstraint=Hi(t.biasConstraint),this.dropout=pr([1,dr([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=pr([1,dr([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.stateSize=this.units,this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){t=oa(t),this.kernel=this.addWeight("kernel",[t[t.length-1],this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias?this.bias=this.addWeight("bias",[this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,n){return e.tidy((()=>{if(2!==(t=t).length)throw new yi(`SimpleRNNCell expects 2 input Tensors, got ${t.length}.`);let s=t[1];t=t[0];const i=null!=n.training&&n.training;let r;0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=Yl({ones:()=>e.onesLike(t),rate:this.dropout,training:i})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=Yl({ones:()=>e.onesLike(s),rate:this.recurrentDropout,training:i}));const a=this.dropoutMask,o=this.recurrentDropoutMask;r=Nr(null!=a?e.mul(t,a):t,this.kernel.read()),null!=this.bias&&(r=Cr(r,this.bias.read())),null!=o&&(s=e.mul(s,o));let l=e.add(r,Nr(s,this.recurrentKernel.read()));return null!=this.activation&&(l=this.activation.apply(l)),[l,l]}))}getConfig(){const t=super.getConfig(),e={units:this.units,activation:al(this.activation),useBias:this.useBias,kernelInitializer:Yr(this.kernelInitializer),recurrentInitializer:Yr(this.recurrentInitializer),biasInitializer:Yr(this.biasInitializer),kernelRegularizer:dl(this.kernelRegularizer),recurrentRegularizer:dl(this.recurrentRegularizer),biasRegularizer:dl(this.biasRegularizer),activityRegularizer:dl(this.activityRegularizer),kernelConstraint:qi(this.kernelConstraint),recurrentConstraint:qi(this.recurrentConstraint),biasConstraint:qi(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout};return Object.assign({},t,e)}}Vl.className="SimpleRNNCell",e.serialization.registerClass(Vl);class Kl extends Ul{constructor(t){t.cell=new Vl(t),super(t)}call(t,n){return e.tidy((()=>{null!=this.cell.dropoutMask&&(e.dispose(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(e.dispose(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const s=null==n?null:n.mask,i=null==n?null:n.training,r=null==n?null:n.initialState;return super.call(t,{mask:s,training:i,initialState:r})}))}static fromConfig(t,e){return new t(e)}}Kl.className="SimpleRNN",e.serialization.registerClass(Kl);class ql extends jl{constructor(t){if(super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_RECURRENT_ACTIVATION="hardSigmoid",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",t.resetAfter)throw new yi("GRUCell does not support reset_after parameter set to true.");this.units=t.units,Ri(this.units,"units"),this.activation=ll(void 0===t.activation?this.DEFAULT_ACTIVATION:t.activation),this.recurrentActivation=ll(void 0===t.recurrentActivation?this.DEFAULT_RECURRENT_ACTIVATION:t.recurrentActivation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=Xr(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=Xr(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=Xr(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelRegularizer=gl(t.kernelRegularizer),this.recurrentRegularizer=gl(t.recurrentRegularizer),this.biasRegularizer=gl(t.biasRegularizer),this.kernelConstraint=Hi(t.kernelConstraint),this.recurrentConstraint=Hi(t.recurrentConstraint),this.biasConstraint=Hi(t.biasConstraint),this.dropout=pr([1,dr([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=pr([1,dr([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.implementation=t.implementation,this.stateSize=this.units,this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){const e=(t=oa(t))[t.length-1];this.kernel=this.addWeight("kernel",[e,3*this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,3*this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias?this.bias=this.addWeight("bias",[3*this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,n){return e.tidy((()=>{if(2!==(t=t).length)throw new yi(`GRUCell expects 2 input Tensors (inputs, h, c), got ${t.length}.`);const s=null!=n.training&&n.training;let i=t[1];t=t[0],0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=Yl({ones:()=>e.onesLike(t),rate:this.dropout,training:s,count:3})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=Yl({ones:()=>e.onesLike(i),rate:this.recurrentDropout,training:s,count:3}));const r=this.dropoutMask,a=this.recurrentDropoutMask;let o,l,u;0<this.dropout&&this.dropout<1&&(t=e.mul(t,r[0]));let h=Nr(t,this.kernel.read());this.useBias&&(h=Cr(h,this.bias.read())),0<this.recurrentDropout&&this.recurrentDropout<1&&(i=e.mul(i,a[0]));const c=this.recurrentKernel.read(),[p,d]=e.split(c,[2*this.units,this.units],c.rank-1),f=Nr(i,p),[g,m,y]=e.split(h,3,h.rank-1),[b,w]=e.split(f,2,f.rank-1);o=this.recurrentActivation.apply(e.add(g,b)),l=this.recurrentActivation.apply(e.add(m,w));const k=Nr(e.mul(l,i),d);u=this.activation.apply(e.add(y,k));const v=e.add(e.mul(o,i),e.mul(e.add(1,e.neg(o)),u));return[v,v]}))}getConfig(){const t=super.getConfig(),e={units:this.units,activation:al(this.activation),recurrentActivation:al(this.recurrentActivation),useBias:this.useBias,kernelInitializer:Yr(this.kernelInitializer),recurrentInitializer:Yr(this.recurrentInitializer),biasInitializer:Yr(this.biasInitializer),kernelRegularizer:dl(this.kernelRegularizer),recurrentRegularizer:dl(this.recurrentRegularizer),biasRegularizer:dl(this.biasRegularizer),activityRegularizer:dl(this.activityRegularizer),kernelConstraint:qi(this.kernelConstraint),recurrentConstraint:qi(this.recurrentConstraint),biasConstraint:qi(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout,implementation:this.implementation,resetAfter:!1};return Object.assign({},t,e)}}ql.className="GRUCell",e.serialization.registerClass(ql);class Gl extends Ul{constructor(t){0===t.implementation&&console.warn("`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call."),t.cell=new ql(t),super(t)}call(t,n){return e.tidy((()=>{null!=this.cell.dropoutMask&&(e.dispose(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(e.dispose(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const s=null==n?null:n.mask,i=null==n?null:n.training,r=null==n?null:n.initialState;return super.call(t,{mask:s,training:i,initialState:r})}))}static fromConfig(t,e){return 0===e.implmentation&&(e.implementation=1),new t(e)}}Gl.className="GRU",e.serialization.registerClass(Gl);class Hl extends jl{constructor(t){super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_RECURRENT_ACTIVATION="hardSigmoid",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",this.units=t.units,Ri(this.units,"units"),this.activation=ll(void 0===t.activation?this.DEFAULT_ACTIVATION:t.activation),this.recurrentActivation=ll(void 0===t.recurrentActivation?this.DEFAULT_RECURRENT_ACTIVATION:t.recurrentActivation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=Xr(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=Xr(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=Xr(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.unitForgetBias=t.unitForgetBias,this.kernelRegularizer=gl(t.kernelRegularizer),this.recurrentRegularizer=gl(t.recurrentRegularizer),this.biasRegularizer=gl(t.biasRegularizer),this.kernelConstraint=Hi(t.kernelConstraint),this.recurrentConstraint=Hi(t.recurrentConstraint),this.biasConstraint=Hi(t.biasConstraint),this.dropout=pr([1,dr([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=pr([1,dr([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.implementation=t.implementation,this.stateSize=[this.units,this.units],this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){var e;const n=(t=oa(t))[t.length-1];let s;if(this.kernel=this.addWeight("kernel",[n,4*this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,4*this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias){if(this.unitForgetBias){const t=this.biasInitializer,n=this.units;s=new((e=class extends Dr{apply(e,s){const i=t.apply([n]),r=(new _r).apply([n]),a=t.apply([2*n]);return vr(vr(i,r),a)}}).className="CustomInit",e)}else s=this.biasInitializer;this.bias=this.addWeight("bias",[4*this.units],null,s,this.biasRegularizer,!0,this.biasConstraint)}else this.bias=null;this.built=!0}call(t,n){return e.tidy((()=>{const s=null!=n.training&&n.training;if(3!==(t=t).length)throw new yi(`LSTMCell expects 3 input Tensors (inputs, h, c), got ${t.length}.`);let i=t[1];const r=t[2];t=t[0],0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=Yl({ones:()=>e.onesLike(t),rate:this.dropout,training:s,count:4})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=Yl({ones:()=>e.onesLike(i),rate:this.recurrentDropout,training:s,count:4}));const a=this.dropoutMask,o=this.recurrentDropoutMask;let l,u,h,c;0<this.dropout&&this.dropout<1&&(t=e.mul(t,a[0]));let p=Nr(t,this.kernel.read());0<this.recurrentDropout&&this.recurrentDropout<1&&(i=e.mul(i,o[0])),p=e.add(p,Nr(i,this.recurrentKernel.read())),this.useBias&&(p=Cr(p,this.bias.read()));const[d,f,g,m]=e.split(p,4,p.rank-1);l=this.recurrentActivation.apply(d),u=this.recurrentActivation.apply(f),h=e.add(e.mul(u,r),e.mul(l,this.activation.apply(g))),c=this.recurrentActivation.apply(m);const y=e.mul(c,this.activation.apply(h));return[y,y,h]}))}getConfig(){const t=super.getConfig(),e={units:this.units,activation:al(this.activation),recurrentActivation:al(this.recurrentActivation),useBias:this.useBias,kernelInitializer:Yr(this.kernelInitializer),recurrentInitializer:Yr(this.recurrentInitializer),biasInitializer:Yr(this.biasInitializer),unitForgetBias:this.unitForgetBias,kernelRegularizer:dl(this.kernelRegularizer),recurrentRegularizer:dl(this.recurrentRegularizer),biasRegularizer:dl(this.biasRegularizer),activityRegularizer:dl(this.activityRegularizer),kernelConstraint:qi(this.kernelConstraint),recurrentConstraint:qi(this.recurrentConstraint),biasConstraint:qi(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout,implementation:this.implementation};return Object.assign({},t,e)}}Hl.className="LSTMCell",e.serialization.registerClass(Hl);class Jl extends Ul{constructor(t){0===t.implementation&&console.warn("`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call."),t.cell=new Hl(t),super(t)}call(t,n){return e.tidy((()=>{null!=this.cell.dropoutMask&&(e.dispose(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(e.dispose(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const s=null==n?null:n.mask,i=null==n?null:n.training,r=null==n?null:n.initialState;return super.call(t,{mask:s,training:i,initialState:r})}))}static fromConfig(t,e){return 0===e.implmentation&&(e.implementation=1),new t(e)}}Jl.className="LSTM",e.serialization.registerClass(Jl);class Zl extends jl{constructor(t){super(t),this.cells=t.cells}get stateSize(){const t=[];for(const e of this.cells.slice().reverse())Array.isArray(e.stateSize)?t.push(...e.stateSize):t.push(e.stateSize);return t}call(t,n){return e.tidy((()=>{let e=(t=t).slice(1);const s=[];for(const t of this.cells.slice().reverse())Array.isArray(t.stateSize)?s.push(e.splice(0,t.stateSize.length)):s.push(e.splice(0,1));s.reverse();const i=[];let r;for(let a=0;a<this.cells.length;++a){const o=this.cells[a];e=s[a],r=0===a?[t[0]].concat(e):[r[0]].concat(e),r=o.call(r,n),i.push(r.slice(1))}e=[];for(const t of i.slice().reverse())e.push(...t);return[r[0]].concat(e)}))}build(t){let e;ia(t)&&(t=t[0]),t=t,this.cells.forEach(((n,s)=>{ar(`RNNCell_${s}`,(()=>{n.build(t),e=Array.isArray(n.stateSize)?n.stateSize[0]:n.stateSize,t=[t[0],e]}))})),this.built=!0}getConfig(){const t=super.getConfig(),e={cells:this.cells.map((t=>({className:t.getClassName(),config:t.getConfig()})))};return Object.assign({},t,e)}static fromConfig(t,e,n={}){const s=[];for(const t of e.cells)s.push(Da(t,n));return new t({cells:s})}get trainableWeights(){if(!this.trainable)return[];const t=[];for(const e of this.cells)t.push(...e.trainableWeights);return t}get nonTrainableWeights(){const t=[];for(const e of this.cells)t.push(...e.nonTrainableWeights);if(!this.trainable){const e=[];for(const t of this.cells)e.push(...t.trainableWeights);return e.concat(t)}return t}getWeights(){const t=[];for(const e of this.cells)t.push(...e.weights);return ca(t)}setWeights(t){const e=[];for(const n of this.cells){const s=n.weights.length,i=t.splice(s);for(let t=0;t<n.weights.length;++t)e.push([n.weights[t],i[t]])}pa(e)}}function Yl(t){const{ones:n,rate:s,training:i=!1,count:r=1}=t,a=()=>Tr(n(),s),o=()=>$r(a,n,i);if(!r||r<=1)return e.keep(o().clone());return Array(r).fill(void 0).map(o).map((t=>e.keep(t.clone())))}Zl.className="StackedRNNCells",e.serialization.registerClass(Zl);class Xl extends Ul{constructor(t){if(t.unroll)throw new bi("Unrolling is not possible with convolutional RNNs.");if(Array.isArray(t.cell))throw new bi("It is not possible at the moment to stack convolutional cells.");super(t),this.inputSpec=[new da({ndim:5})]}call(t,n){return e.tidy((()=>{if(null!=this.cell.dropoutMask&&(e.dispose(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(e.dispose(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null),n&&n.constants)throw new yi("ConvRNN2D cell does not support constants");const s=null==n?null:n.mask,i=null==n?null:n.training,r=null==n?null:n.initialState;return super.call(t,{mask:s,training:i,initialState:r})}))}computeOutputShape(t){let e=this.computeSingleOutputShape(t);return this.returnSequences||(e=[e[0],...e.slice(2)]),this.returnState&&(e=[e,...Array(2).fill([t[0],...e.slice(-3)])]),e}getInitialState(t){return e.tidy((()=>{const{stateSize:n}=this.cell,s=t.shape,i=this.computeSingleOutputShape(s),r=[i[0],...i.slice(2)],a=e.zeros(r);return Array.isArray(n)?Array(n.length).fill(a):[a]}))}resetStates(t,n=!1){e.tidy((()=>{if(!this.stateful)throw new gi("Cannot call resetStates() on an RNN Layer that is not stateful.");const s=this.inputSpec[0].shape,i=this.computeSingleOutputShape(s),r=[i[0],...i.slice(2)];if(null==s[0])throw new yi("If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.");if(null==this.getStates())Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((()=>e.zeros(r))):this.states_=[e.zeros(r)];else if(null==t)e.dispose(this.states_),null!=this.keptStates&&(e.dispose(this.keptStates),this.keptStates=[]),Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((()=>e.zeros(r))):this.states_[0]=e.zeros(r);else{if(Array.isArray(t)||(t=[t]),t.length!==this.states_.length)throw new yi(`Layer ${this.name} expects ${this.states_.length} state(s), but it received ${t.length} state value(s). Input received: ${t}`);n?this.keptStates.push(this.states_.slice()):e.dispose(this.states_);for(let n=0;n<this.states_.length;++n){const s=t[n],i=r;if(!e.util.arraysEqual(s.shape,i))throw new yi(`State ${n} is incompatible with layer ${this.name}: expected shape=${i}, received shape=${s.shape}`);this.states_[n]=s}}this.states_=this.states_.map((t=>e.keep(t.clone())))}))}computeSingleOutputShape(t){const{dataFormat:e,filters:n,kernelSize:s,padding:i,strides:r,dilationRate:a}=this.cell,o="channelsFirst"===e,l=t[o?3:2],u=t[o?4:3],h=xl(l,s[0],i,r[0],a[0]),c=xl(u,s[1],i,r[1],a[1]);return[...t.slice(0,2),...o?[n,h,c]:[h,c,n]]}}Xl.className="ConvRNN2D";class Ql extends Hl{constructor(t){const{filters:e,kernelSize:n,strides:s,padding:i,dataFormat:r,dilationRate:a}=t;super(Object.assign({},t,{units:e})),this.filters=e,Ri(this.filters,"filters"),this.kernelSize=Sl(n,2,"kernelSize"),this.kernelSize.forEach((t=>Ri(t,"kernelSize"))),this.strides=Sl(s||1,2,"strides"),this.strides.forEach((t=>Ri(t,"strides"))),this.padding=i||"valid",sr(this.padding),this.dataFormat=r||"channelsLast",nr(this.dataFormat),this.dilationRate=Sl(a||1,2,"dilationRate"),this.dilationRate.forEach((t=>Ri(t,"dilationRate")))}build(t){var n;t=oa(t);const s="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[s])throw new yi(`The channel dimension of the input should be defined. Found ${t[s]}`);const i=t[s],r=this.kernelSize.concat([i,4*this.filters]);this.kernel=this.addWeight("kernel",r,null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint);const a=this.kernelSize.concat([this.filters,4*this.filters]);if(this.recurrentKernel=this.addWeight("recurrent_kernel",a,null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias){let t;if(this.unitForgetBias){const s=this.biasInitializer,i=this.filters;t=new((n=class extends Dr{apply(t,n){return kr([s.apply([i]),e.ones([i]),s.apply([2*i])])}}).className="CustomInit",n)}else t=this.biasInitializer;this.bias=this.addWeight("bias",[4*this.filters],null,t,this.biasRegularizer,!0,this.biasConstraint)}this.built=!0}call(t,n){return e.tidy((()=>{if(3!==t.length)throw new yi(`ConvLSTM2DCell expects 3 input Tensors (inputs, h, c), got ${t.length}.`);const s=n.training||!1,i=t[0],r=t[1],a=t[2];0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=Yl({ones:()=>e.onesLike(i),rate:this.dropout,training:s,count:4}));const o=this.dropoutMask,l=(t,n,s)=>n&&n[s]?e.mul(n[s],t):t;let u=l(i,o,0),h=l(i,o,1),c=l(i,o,2),p=l(i,o,3);0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=Yl({ones:()=>e.onesLike(r),rate:this.recurrentDropout,training:s,count:4}));const d=this.recurrentDropoutMask;let f=l(r,d,0),g=l(r,d,1),m=l(r,d,2),y=l(r,d,3);const[b,w,k,v]=e.split(this.kernel.read(),4,3),[S,x,N,z]=this.useBias?e.split(this.bias.read(),4):[null,null,null,null];u=this.inputConv(u,b,S,this.padding),h=this.inputConv(h,w,x,this.padding),c=this.inputConv(c,k,N,this.padding),p=this.inputConv(p,v,z,this.padding);const[I,A,C,T]=e.split(this.recurrentKernel.read(),4,3);f=this.recurrentConv(f,I),g=this.recurrentConv(g,A),m=this.recurrentConv(m,C),y=this.recurrentConv(y,T);const $=this.recurrentActivation.apply(e.add(u,f)),E=this.recurrentActivation.apply(e.add(h,g)),F=e.add(e.mul(E,a),e.mul($,this.activation.apply(e.add(c,m)))),D=e.mul(this.recurrentActivation.apply(e.add(p,y)),this.activation.apply(F));return[D,D,F]}))}getConfig(){const t=function(t,e){var n={};for(var s in t)Object.prototype.hasOwnProperty.call(t,s)&&e.indexOf(s)<0&&(n[s]=t[s]);if(null!=t&&"function"==typeof Object.getOwnPropertySymbols){var i=0;for(s=Object.getOwnPropertySymbols(t);i<s.length;i++)e.indexOf(s[i])<0&&Object.prototype.propertyIsEnumerable.call(t,s[i])&&(n[s[i]]=t[s[i]])}return n}(super.getConfig(),["units"]),e={filters:this.filters,kernelSize:this.kernelSize,padding:this.padding,dataFormat:this.dataFormat,dilationRate:this.dilationRate,strides:this.strides};return Object.assign({},t,e)}inputConv(t,n,s,i){const r=e.conv2d(t,n,this.strides,i||"valid","channelsFirst"===this.dataFormat?"NCHW":"NHWC",this.dilationRate);return s?Cr(r,s,this.dataFormat):r}recurrentConv(t,n){return e.conv2d(t,n,1,"same","channelsFirst"===this.dataFormat?"NCHW":"NHWC")}}Ql.className="ConvLSTM2DCell",e.serialization.registerClass(Ql);class tu extends Xl{constructor(t){const e=new Ql(t);super(Object.assign({},t,{cell:e}))}static fromConfig(t,e){return new t(e)}}tu.className="ConvLSTM2D",e.serialization.registerClass(tu);class eu extends ba{constructor(t){super(t),this.rate=Math.max(Math.min(t.rate,1),0),this.noiseShape=t.noiseShape,this.seed=t.seed,this.supportsMasking=!0}getNoiseShape(t){if(null==this.noiseShape)return this.noiseShape;const e=t.shape,n=[];for(let t=0;t<this.noiseShape.length;++t)n.push(null==this.noiseShape[t]?e[t]:this.noiseShape[t]);return n}call(t,n){return e.tidy((()=>{this.invokeCallHook(t,n);const e=aa(t);if(0<this.rate&&this.rate<1){const t=null!=n.training&&n.training,s=this.getNoiseShape(e);return $r((()=>Tr(e,this.rate,s,this.seed)),(()=>e),t)}return t}))}getConfig(){const t={rate:this.rate,noiseShape:this.noiseShape,seed:this.seed},e=super.getConfig();return Object.assign(t,e),t}dispose(){return super.dispose()}}eu.className="Dropout",e.serialization.registerClass(eu);class nu extends eu{constructor(t){super(t),this.inputSpec=[{ndim:3}]}getNoiseShape(t){const e=t.shape;return[e[0],1,e[2]]}}nu.className="SpatialDropout1D",e.serialization.registerClass(nu);class su extends ba{constructor(t){if(super(t),this.activation=null,this.useBias=!0,this.kernel=null,this.bias=null,this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_BIAS_INITIALIZER="zeros",null==t.batchInputShape&&null==t.inputShape&&null!=t.inputDim){let e=null;null!=t.batchSize&&(e=t.batchSize),this.batchInputShape=[e,t.inputDim]}this.units=t.units,Ri(this.units,"units"),this.activation=ll(t.activation),null!=t.useBias&&(this.useBias=t.useBias),this.kernelInitializer=Xr(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.biasInitializer=Xr(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelConstraint=Hi(t.kernelConstraint),this.biasConstraint=Hi(t.biasConstraint),this.kernelRegularizer=gl(t.kernelRegularizer),this.biasRegularizer=gl(t.biasRegularizer),this.activityRegularizer=gl(t.activityRegularizer),this.supportsMasking=!0,this.inputSpec=[{minNDim:2}]}build(t){const e=(t=oa(t))[t.length-1];null==this.kernel&&(this.kernel=this.addWeight("kernel",[e,this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint))),this.inputSpec=[{minNDim:2,axes:{[-1]:e}}],this.built=!0}computeOutputShape(t){const e=(t=oa(t)).slice();return e[e.length-1]=this.units,e}call(t,n){return e.tidy((()=>{this.invokeCallHook(t,n);const e=aa(t),s=Oi(this.activation.getClassName());let i;return null!=s?i=Nr(e,this.kernel.read(),s,this.bias?this.bias.read():null):(i=Nr(e,this.kernel.read()),null!=this.bias&&(i=Cr(i,this.bias.read())),null!=this.activation&&(i=this.activation.apply(i))),i}))}getConfig(){const t={units:this.units,activation:al(this.activation),useBias:this.useBias,kernelInitializer:Yr(this.kernelInitializer),biasInitializer:Yr(this.biasInitializer),kernelRegularizer:dl(this.kernelRegularizer),biasRegularizer:dl(this.biasRegularizer),activityRegularizer:dl(this.activityRegularizer),kernelConstraint:qi(this.kernelConstraint),biasConstraint:qi(this.biasConstraint)},e=super.getConfig();return Object.assign(t,e),t}}su.className="Dense",e.serialization.registerClass(su);class iu extends ba{constructor(t){super(t=t||{}),this.inputSpec=[{minNDim:3}],this.dataFormat=t.dataFormat}computeOutputShape(t){t=oa(t);for(const e of t.slice(1))if(null==e)throw new yi(`The shape of the input to "Flatten" is not fully defined (got ${t.slice(1)}). Make sure to pass a complete "input_shape" or "batch_input_shape" argument to the first layer in your model.`);return[t[0],cr(t,1)]}call(t,n){return e.tidy((()=>{this.invokeCallHook(t,n);let s=aa(t);if("channelsFirst"===this.dataFormat&&s.rank>1){const t=[0];for(let e=2;e<s.rank;++e)t.push(e);t.push(1),s=e.transpose(s,t)}return function(t){if(t.rank<=1)throw new yi(`batchFlatten requires a minimum rank of 2. Got rank: ${t.rank}.`);const n=[t.shape[0],cr(t.shape,1)];return e.reshape(t,n)}(s)}))}getConfig(){const t={};null!=this.dataFormat&&(t.dataFormat=this.dataFormat);const e=super.getConfig();return Object.assign(t,e),t}}iu.className="Flatten",e.serialization.registerClass(iu);class ru extends ba{constructor(t){super(t),this.supportsMasking=!0,this.activation=ll(t.activation)}call(t,n){return e.tidy((()=>{this.invokeCallHook(t,n);const e=aa(t);return this.activation.apply(e)}))}getConfig(){const t={activation:al(this.activation)},e=super.getConfig();return Object.assign(t,e),t}}ru.className="Activation",e.serialization.registerClass(ru);class au extends ba{constructor(t){super(t),this.n=t.n,this.inputSpec=[{ndim:2}]}computeOutputShape(t){return[t[0],this.n,t[1]]}call(t,n){return e.tidy((()=>{return t=aa(t),n=t,s=this.n,e.tidy((()=>{if(2!==n.shape.length)throw new yi(`repeat() expects a rank-2 tensor, but received a rank-${n.shape.length} tensor.`);return Sr(mr(n,1),[1,s,1])}));var n,s}))}getConfig(){const t={n:this.n},e=super.getConfig();return Object.assign(t,e),t}}au.className="RepeatVector",e.serialization.registerClass(au);class ou extends ba{constructor(t){super(t),this.targetShape=t.targetShape;for(let t=0;t<this.targetShape.length;++t)this.isUnknown(this.targetShape[t])&&(this.targetShape[t]=null)}isUnknown(t){return t<0||null==t}fixUnknownDimension(t,e){const n="Total size of new array must be unchanged.",s=e.slice();let i=1,r=null;for(let t=0;t<s.length;++t){const e=s[t];if(this.isUnknown(e)){if(null!==r)throw new yi("Can only specifiy one unknown dimension.");r=t}else i*=e}const a=cr(t);if(null!==r){if(0===i||a%i!=0)throw new yi(n);s[r]=a/i}else if(a!==i)throw new yi(n);return s}computeOutputShape(t){let e=!1;for(let n=0;n<t.length;++n)if(this.isUnknown(t[n])){e=!0;break}return e?t.slice(0,1).concat(this.targetShape):t.slice(0,1).concat(this.fixUnknownDimension(t.slice(1),this.targetShape))}call(t,n){return e.tidy((()=>{this.invokeCallHook(t,n);const s=aa(t),i=s.shape,r=i.slice(0,1).concat(this.fixUnknownDimension(i.slice(1),this.targetShape));return e.reshape(s,r)}))}getConfig(){const t={targetShape:this.targetShape},e=super.getConfig();return Object.assign(t,e),t}}ou.className="Reshape",e.serialization.registerClass(ou);class lu extends ba{constructor(t){if(super(t),null==t.dims)throw new Error("Required configuration field `dims` is missing during Permute constructor call.");if(!Array.isArray(t.dims))throw new Error(`Permute constructor requires \`dims\` to be an Array, but received ${t.dims} instead.`);const n=fr(1,t.dims.length+1);if(!e.util.arraysEqual(t.dims.slice().sort(),n))throw new Error("Invalid permutation `dims`: "+JSON.stringify(t.dims)+" `dims` must contain consecutive integers starting from 1.");this.dims=t.dims,this.dimsIncludingBatch=[0].concat(this.dims),this.inputSpec=[new da({ndim:this.dims.length+1})]}computeOutputShape(t){const e=(t=oa(t)).slice();return this.dims.forEach(((n,s)=>{e[s+1]=t[n]})),e}call(t,n){return e.transpose(aa(t),this.dimsIncludingBatch)}getConfig(){const t={dims:this.dims},e=super.getConfig();return Object.assign(t,e),t}}lu.className="Permute",e.serialization.registerClass(lu);class uu extends ba{constructor(t){super(null==t?{}:t),this.supportsMasking=!0,this.maskValue=null!=t?null==t.maskValue?0:t.maskValue:0}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={maskValue:this.maskValue};return Object.assign(e,t),e}computeMask(t,n){const s=aa(t);return e.any(e.notEqual(s,this.maskValue),-1)}call(t,n){return e.tidy((()=>{this.invokeCallHook(t,n);const s=aa(t),i=e.any(e.notEqual(s,this.maskValue),-1,!0);return e.mul(s,e.cast(i,s.dtype))}))}}uu.className="Masking",e.serialization.registerClass(uu);class hu extends ba{constructor(t){if(super(t),this.embeddings=null,this.DEFAULT_EMBEDDINGS_INITIALIZER="randomUniform",null==t.batchInputShape&&null==t.inputShape){let e=null;null!=t.batchSize&&(e=t.batchSize),null==t.inputLength?this.batchInputShape=[e,null]:this.batchInputShape=[e].concat(Ni(t.inputLength))}this.inputDim=t.inputDim,Ri(this.inputDim,"inputDim"),this.outputDim=t.outputDim,Ri(this.outputDim,"outputDim"),this.embeddingsInitializer=Xr(t.embeddingsInitializer||this.DEFAULT_EMBEDDINGS_INITIALIZER),this.embeddingsRegularizer=gl(t.embeddingsRegularizer),this.activityRegularizer=gl(t.activityRegularizer),this.embeddingsConstraint=Hi(t.embeddingsConstraint),this.maskZero=t.maskZero,this.supportsMasking=t.maskZero,this.inputLength=t.inputLength}build(t){this.embeddings=this.addWeight("embeddings",[this.inputDim,this.outputDim],this.dtype,this.embeddingsInitializer,this.embeddingsRegularizer,!0,this.embeddingsConstraint),this.built=!0}warnOnIncompatibleInputShape(t){}computeMask(t,n){return e.tidy((()=>this.maskZero?(t=aa(t),e.notEqual(t,e.zerosLike(t))):null))}computeOutputShape(t){if(t=oa(t),null==this.inputLength)return[...t,this.outputDim];const e=Ni(this.inputLength);if(e.length!==t.length-1)throw new yi(`"inputLength" is ${this.inputLength}, but received input shape has shape ${t}`);{let n=0;for(let s=0;s<e.length;++s){const i=e[s],r=t[s+1];if(null!=i&&null!=r&&i!==r)throw new yi(`"inputLength" is ${this.inputLength}, but received input shape has shape ${t}`);null==i&&(e[n]=r),n++}}return[t[0],...e,this.outputDim]}call(t,n){return e.tidy((()=>{this.invokeCallHook(t,n);let s=aa(t);"int32"!==s.dtype&&(s=gr(s,"int32"));const i=zr(this.embeddings.read(),e.reshape(s,[s.size]));return e.reshape(i,oa(this.computeOutputShape(s.shape)))}))}getConfig(){const t={inputDim:this.inputDim,outputDim:this.outputDim,embeddingsInitializer:Yr(this.embeddingsInitializer),embeddingsRegularizer:dl(this.embeddingsRegularizer),activityRegularizer:dl(this.activityRegularizer),embeddingsConstraint:qi(this.embeddingsConstraint),maskZero:this.maskZero,inputLength:this.inputLength},e=super.getConfig();return Object.assign(t,e),t}}hu.className="Embedding",e.serialization.registerClass(hu);class cu extends ba{constructor(t){super(t||{}),this.supportsMasking=!0}mergeFunction(t){throw new bi}computeElementwiseOpOutputShape(t,e){if(null==t||null==e)return null;if(t.length<e.length)return this.computeElementwiseOpOutputShape(e,t);if(0===e.length)return t;const n=t.slice(0,t.length-e.length);for(let s=0;s<e.length;++s){const i=t[t.length-e.length+s],r=e[s];if(null==i||null==r||i<0||r<0)n.push(null);else if(1===i)n.push(r);else if(1===r)n.push(i);else{if(i!==r)throw new yi("Operands could not be broadcast together with shapes "+JSON.stringify(t)+" "+JSON.stringify(e));n.push(i)}}return n}build(t){if(Array.isArray(t)&&!Array.isArray(t[0])&&(t=[oa(t)]),(t=t).length<2)throw new yi(`A merge layer should be called on an Array of at least 2 inputs. Got ${t.length} input(s).`);let e=[];for(const n of t)null!=n&&null!==n[0]&&e.push(n[0]);if(e=Fi(e),e.length>1)throw new yi(`Can not merge tensors with different batch sizes. Got tensors with shapes: ${JSON.stringify(t)}.`);let n=null==t[0]?null:t[0].slice(1);for(let e=1;e<t.length;++e){const s=null==t[e]?null:t[e].slice(1);n=this.computeElementwiseOpOutputShape(n,s)}const s=t.map((t=>t.length));-1===t.indexOf(null)&&1===Fi(s).length?this.reshapeRequired=!1:this.reshapeRequired=!0}call(t,n){return e.tidy((()=>{if(t=t,this.reshapeRequired){const n=[],s=t.map((t=>t.rank));if(-1===s.indexOf(null)){const e=dr(s);for(let s of t){const t=s.rank;for(let n=0;n<e-t;++n)s=mr(s,1);n.push(s)}return this.mergeFunction(n)}{let s=!1;for(const i of t){const t=i.rank;if(null==t){const t=i.shape,r=t[0],a=t.slice(1).concat([r]);let o=e.reshape(i,[r].concat(cr(t.slice(1))));o=e.transpose(o,[1,0]),o=e.reshape(o,a),n.push(o),s=!0}else if(t>1){const r=fr(1,t).concat([0]);n.push(e.transpose(i,r)),s=!0}else n.push(i)}let i=this.mergeFunction(n);const r=i.rank;if(s)if(null==r){const t=i.shape,n=t[t.length-1],s=[n].concat(t.slice(0,t.length-1));i=e.reshape(e.transpose(e.reshape(i,[-1,n]),[1,0]),s)}else if(r>1){const t=[r-1].concat(fr(0,r-1));i=e.transpose(i,t)}return i}}return this.mergeFunction(t)}))}computeOutputShape(t){let e;e=null==(t=t)[0]?null:t[0].slice(1);for(let n=1;n<t.length;++n){const s=null==t[n]?null:t[n].slice(1);e=this.computeElementwiseOpOutputShape(e,s)}let n=[];for(const e of t)null!=e&&null!==e[0]&&n.push(e[0]);return n=Fi(n),e=1===n.length?n.concat(e):[null].concat(e),e}computeMask(t,n){return e.tidy((()=>{if(null==n)return null;if(!Array.isArray(n))throw new yi("`mask` should be an Array");if(!Array.isArray(t))throw new yi("`inputs` should be an Array");if(n.length!==t.length)throw new yi(`The Array 'inputs' and 'mask' are expected to have the same length, but have different lengths (${t.length} vs ${n.length})`);if(n.every((t=>null==t)))return null;let s=(n=n.map((t=>null==t?t:e.expandDims(t,0))))[0];for(let t=1;t<n.length-1;++t)s=e.logicalAnd(s,n[t]);return s}))}}class pu extends cu{constructor(t){super(t)}mergeFunction(t){return e.tidy((()=>{let n=t[0].clone();for(let s=1;s<t.length;++s)n=e.add(n,t[s]);return n}))}}pu.className="Add",e.serialization.registerClass(pu);class du extends cu{constructor(t){super(t)}mergeFunction(t){return e.tidy((()=>{let n=t[0].clone();for(let s=1;s<t.length;++s)n=e.mul(n,t[s]);return n}))}}du.className="Multiply",e.serialization.registerClass(du);class fu extends cu{constructor(t){super(t)}mergeFunction(t){return e.tidy((()=>{let n=t[0].clone();for(let s=1;s<t.length;++s)n=e.add(n,t[s]);return e.mul(1/t.length,n)}))}}fu.className="Average",e.serialization.registerClass(fu);class gu extends cu{constructor(t){super(t)}mergeFunction(t){return e.tidy((()=>{let n=t[0];for(let s=1;s<t.length;++s)n=e.maximum(n,t[s]);return n}))}}gu.className="Maximum",e.serialization.registerClass(gu);class mu extends cu{constructor(t){super(t)}mergeFunction(t){return e.tidy((()=>{let n=t[0];for(let s=1;s<t.length;++s)n=e.minimum(n,t[s]);return n}))}}mu.className="Minimum",e.serialization.registerClass(mu);class yu extends cu{constructor(t){super(t),this.DEFAULT_AXIS=-1,null==t&&(t={}),this.axis=null==t.axis?this.DEFAULT_AXIS:t.axis,this.supportsMasking=!0,this.reshapeRequired=!1}build(t){if(!Array.isArray(t)||!Array.isArray(t[0])||1===t.length)throw new yi("A `Concatenate` layer should be called on a list of at least 2 inputs");t=t;let n=!0;for(const e of t)if(null!=e){n=!1;break}if(n)return;const s=[];for(let n=0;n<t.length;++n){const i=t[n].slice();i.splice(this.axis,1);let r=!1;for(const t of s)if(e.util.arraysEqual(t,i)){r=!0;break}r||s.push(i)}if(s.length>1)throw new yi("A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got input shapes: "+JSON.stringify(t))}mergeFunction(t){return e.tidy((()=>kr(t,this.axis)))}computeOutputShape(t){if(!Array.isArray(t)||!Array.isArray(t[0]))throw new yi("A `Concatenate` layer should be called on a list of inputs.");const e=t,n=e[0].slice(),s=this.axis<0?n.length+this.axis:this.axis;for(const t of e.slice(1)){if(null==n[s]||null==t[s]){n[s]=null;break}n[s]+=t[s]}return n}computeMask(t,n){if(null==n)return null;if(!Array.isArray(n))throw new yi("`mask` should be an array for Concatenate");if(!Array.isArray(t))throw new yi("`inputs` should be an array for Concatenate");if(n.length!==t.length)throw new yi(`Mismatch in the length of mask (${n.length}) and the legnth of inputs (${t.length})`);return e.tidy((()=>{let s=!0;if(n.forEach((t=>{null==t||(s=!1)})),s)return null;const i=[];for(let s=0;s<t.length;++s)null==n[s]?i.push(e.cast(e.onesLike(t[s]),"bool")):n[s].rank<t[s].rank?i.push(e.expandDims(n[s],-1)):i.push(n[s]);const r=e.concat(i,this.axis);return e.all(r,-1,!1)}))}getConfig(){const t={axis:this.axis},e=super.getConfig();return Object.assign(t,e),t}}function bu(t,e){for(;t<0;)t+=e;return t}yu.className="Concatenate",e.serialization.registerClass(yu);class wu extends cu{constructor(t){super(t),this.axes=t.axes,this.normalize=null!=t.normalize&&t.normalize,this.supportsMasking=!0,this.reshapeRequired=!1}build(t){e.util.assert(Array.isArray(t)&&2===t.length&&Array.isArray(t[0])&&Array.isArray(t[1]),(()=>"A `Dot` layer should be called on a list of exactly 2 inputs."));const n=t[0],s=t[1];if(n.length>3||s.length>3)throw new bi("Dot layer does not support tensors of 4D or higher rank yet.");const i=this.interpretAxes(n,s);if(n[i[0]]!==s[i[1]])throw new yi(`Dimension incompatibility: ${n[i[0]]} !== ${s[i[1]]}`)}mergeFunction(t){if(2!==t.length)throw new yi(`A \`Dot\` layer must be called on exactly 2 inputs, but received ${t.length} input(s).`);let n,s=t[0],i=t[1];return n=Array.isArray(this.axes)?this.axes.map(((e,n)=>bu(e,t[n].shape.length))):[bu(this.axes,s.shape.length),bu(this.axes,i.shape.length)],this.normalize&&(s=La(s,n[0]),i=La(i,n[1])),function(t,n,s){if(t.shape.length>3||n.shape.length>3)throw new bi("batchDot is not implemented for tensors of 4D or higher rank yet");if(e.util.assert(t.shape.length>=2,(()=>`batchDot requires the rank of x to be >= 2, but got ${t.shape.length}`)),e.util.assert(t.shape.length>=2,(()=>`batchDot requires the rank of y to be >= 2, but got ${n.shape.length}`)),"number"==typeof s&&(s=[s,s]),"complex64"===t.dtype||"complex64"===n.dtype)throw new bi("batchDot is not implemented for complex64-type Tensors yet.");const i=t.shape.length,r=n.shape.length;null==s&&(s=[i-1,r-2]);const a=s;return e.tidy((()=>{let s,o;if(i>r){s=i-r;const t=[];for(let e=0;e<s;++e)t.push(1);n=e.reshape(n,n.shape.concat(t))}else if(r>i){s=r-i;const n=[];for(let t=0;t<s;++t)n.push(1);t=e.reshape(t,t.shape.concat(n))}else s=0;if(2===t.shape.length&&2===n.shape.length)o=a[0]===a[1]?e.sum(e.mul(t,n),a[0]):e.sum(e.mul(e.transpose(t,[1,0]),n),a[1]);else{const s=a[0]!==t.shape.length-1,i=a[1]===n.shape.length-1;o=e.matMul(t,n,s,i)}if(s>0){let t;t=i>r?i+r-3:i-1;const n=[];for(let e=t;e<t+s;++e)n.push(e);o=e.squeeze(o,n)}return 1===o.shape.length&&(o=e.expandDims(o,1)),o}))}(s,i,n)}interpretAxes(t,e){let n;return n=Array.isArray(this.axes)?this.axes:[bu(this.axes,t.length),bu(this.axes,e.length)],n}computeOutputShape(t){e.util.assert(Array.isArray(t)&&2===t.length&&Array.isArray(t[0])&&Array.isArray(t[1]),(()=>"A `Dot` layer should be called on a list of exactly 2 inputs."));const n=t[0].slice(),s=t[1].slice();if(n.length>3||s.length>3)throw new bi("Dot layer does not support tensors of 4D or higher rank yet.");const i=this.interpretAxes(n,s);n.splice(i[0],1),s.splice(i[1],1),s.splice(0,1);const r=n.concat(s);return 1===r.length&&r.push(1),r}computeMask(t,e){return null}getConfig(){const t={axes:this.axes,normalize:this.normalize},e=super.getConfig();return Object.assign(t,e),t}}wu.className="Dot",e.serialization.registerClass(wu);class ku extends ba{constructor(t){super(t),this.supportsMasking=!0,this.stddev=t.stddev}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={stddev:this.stddev};return Object.assign(e,t),e}call(t,n){return e.tidy((()=>{this.invokeCallHook(t,n);const s=aa(t);return $r((()=>e.add(xr(s.shape,0,this.stddev),s)),(()=>s),n.training||!1)}))}}ku.className="GaussianNoise",e.serialization.registerClass(ku);class vu extends ba{constructor(t){super(t),this.supportsMasking=!0,this.rate=t.rate}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={rate:this.rate};return Object.assign(e,t),e}call(t,n){return e.tidy((()=>{this.invokeCallHook(t,n);const s=aa(t);if(this.rate>0&&this.rate<1){return $r((()=>{const t=Math.sqrt(this.rate/(1-this.rate));return e.mul(s,xr(s.shape,1,t))}),(()=>s),n.training||!1)}return s}))}}vu.className="GaussianDropout",e.serialization.registerClass(vu);class Su extends ba{constructor(t){super(t),this.supportsMasking=!0,this.rate=t.rate,this.noiseShape=t.noiseShape}_getNoiseShape(t){return this.noiseShape||aa(t).shape}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={rate:this.rate};return Object.assign(e,t),e}call(t,n){return e.tidy((()=>{if(this.rate<1&&this.rate>0){const s=this._getNoiseShape(t);return $r((()=>{const n=aa(t),i=-1.7580993408473766;let r=e.greaterEqual(e.randomUniform(s),this.rate);r=gr(r,"float32");const a=((1-this.rate)*(1+this.rate*i**2))**-.5,o=-a*i*this.rate,l=e.add(e.mul(n,r),e.mul(e.add(r,-1),i));return e.add(e.mul(l,a),o)}),(()=>aa(t)),n.training||!1)}return t}))}}function xu(t,n,s,i,r,a=.001){let o;if(2===t.rank)o=e.batchNorm2d(t,n,s,i,r,a);else if(3===t.rank)o=e.batchNorm3d(t,n,s,i,r,a);else{if(4!==t.rank)throw new bi(`batchNormalization is not implemented for array of rank ${t.rank} yet`);o=e.batchNorm4d(t,n,s,i,r,a)}return o}function Nu(t,n,s,i,r=.001){return e.util.arraysEqual(i.slice().sort(),fr(0,t.rank-1))?function(t,n,s,i,r=.001){return e.tidy((()=>{const a=e.moments(t,i),o=a.mean,l=a.variance;return[xu(t,o,l,s,n,r),o,l]}))}(t,n,s,i,r):function(t,n,s,i,r=.001){return e.tidy((()=>{const a=e.moments(t,i),o=a.mean,l=a.variance,u=[];for(const e of fr(0,t.rank))-1!==i.indexOf(e)?u.push(1):u.push(t.shape[e]);const h=e.reshape(o,u),c=e.reshape(l,u),p=null==n?null:e.reshape(n,u),d=null==s?null:e.reshape(s,u);return[xu(t,h,c,d,p,r),o,l]}))}(t,n,s,i,r)}Su.className="AlphaDropout",e.serialization.registerClass(Su);class zu extends ba{constructor(t){null==t&&(t={}),super(t),this.supportsMasking=!0,this.axis=null==t.axis?-1:t.axis,this.momentum=null==t.momentum?.99:t.momentum,this.epsilon=null==t.epsilon?.001:t.epsilon,this.center=null==t.center||t.center,this.scale=null==t.scale||t.scale,this.betaInitializer=Xr(t.betaInitializer||"zeros"),this.gammaInitializer=Xr(t.gammaInitializer||"ones"),this.movingMeanInitializer=Xr(t.movingMeanInitializer||"zeros"),this.movingVarianceInitializer=Xr(t.movingVarianceInitializer||"ones"),this.betaConstraint=Hi(t.betaConstraint),this.gammaConstraint=Hi(t.gammaConstraint),this.betaRegularizer=gl(t.betaRegularizer),this.gammaRegularizer=gl(t.gammaRegularizer)}build(t){t=oa(t);const e=this.axis>=0?this.axis:this.axis+t.length,n=t[e];if(null==n)throw new yi(`Axis ${e} of input tensor should have a defined dimension but the layer received an input with shape ${JSON.stringify(t)}.`);this.inputSpec=[new da({ndim:t.length,axes:{[e]:n}})];const s=[n];this.scale&&(this.gamma=this.addWeight("gamma",s,null,this.gammaInitializer,this.gammaRegularizer,!0,this.gammaConstraint)),this.center&&(this.beta=this.addWeight("beta",s,null,this.betaInitializer,this.betaRegularizer,!0,this.betaConstraint)),this.movingMean=this.addWeight("moving_mean",s,null,this.movingMeanInitializer,null,!1),this.movingVariance=this.addWeight("moving_variance",s,null,this.movingVarianceInitializer,null,!1),this.built=!0}call(t,n){return e.tidy((()=>{const s=null!=n.training&&n.training,i=aa(t),r=i.shape,a=r.length,o=fr(0,a),l=this.axis>=0?this.axis:this.axis+a;o.splice(l,1);const u=ki(1,a);u[l]=r[l];const h=o.slice();h.sort();const c=!e.util.arraysEqual(h,fr(0,a).slice(0,a-1));if(!s)return(()=>{if(c){const t=e.reshape(this.movingMean.read(),u),n=e.reshape(this.movingVariance.read(),u),s=this.center?e.reshape(this.beta.read(),u):null,r=this.scale?e.reshape(this.gamma.read(),u):null;return xu(i,t,n,s,r,this.epsilon)}return xu(i,this.movingMean.read(),this.movingVariance.read(),null==this.beta?null:this.beta.read(),null==this.gamma?null:this.gamma.read(),this.epsilon)})();const[p,d,f]=Nu(i,this.gamma.read(),this.beta.read(),o,this.epsilon),g=(t,n,s)=>{e.tidy((()=>{const i=1-s,r=t.read(),a=e.mul(e.sub(r,n),i);t.write(e.sub(r,a))}))};return(()=>{g(this.movingMean,d,this.momentum),g(this.movingVariance,f,this.momentum)})(),p}))}getConfig(){const t={axis:this.axis,momentum:this.momentum,epsilon:this.epsilon,center:this.center,scale:this.scale,betaInitializer:Yr(this.betaInitializer),gammaInitializer:Yr(this.gammaInitializer),movingMeanInitializer:Yr(this.movingMeanInitializer),movingVarianceInitializer:Yr(this.movingVarianceInitializer),betaRegularizer:dl(this.betaRegularizer),gammaRegularizer:dl(this.gammaRegularizer),betaConstraint:qi(this.betaConstraint),gammaConstraint:qi(this.gammaConstraint)},e=super.getConfig();return Object.assign(t,e),t}}zu.className="BatchNormalization",e.serialization.registerClass(zu);class Iu extends ba{constructor(t){if(null==t&&(t={}),super(t),this.axis=null==t.axis?-1:t.axis,"number"==typeof this.axis){if(!Number.isInteger(this.axis))throw new Error(`Expected axis to be an integer, but received ${this.axis}`)}else{if(!Array.isArray(this.axis))throw new Error(`Expected axis to be an integer or an array of integers, but received ${JSON.stringify(this.axis)}`);for(const t of this.axis)if(!Number.isInteger(t))throw new Error(`Expected axis to be an array of integers, but received ${JSON.stringify(this.axis)}`)}this.epsilon=null==t.epsilon?.001:t.epsilon,this.center=null==t.center||t.center,this.scale=null==t.scale||t.scale,this.betaInitializer=Xr(t.betaInitializer||"zeros"),this.gammaInitializer=Xr(t.gammaInitializer||"ones"),this.betaRegularizer=gl(t.betaRegularizer),this.gammaRegularizer=gl(t.gammaRegularizer),this.supportsMasking=!0}build(t){const e=(t=oa(t)).length;"number"==typeof this.axis&&(this.axis=[this.axis]);for(let t=0;t<this.axis.length;++t)this.axis[t]<0&&(this.axis[t]+=e);for(const t of this.axis)if(t<0||t>=e)throw new Error(`Invalid axis: ${t}`);if(this.axis.length!==Fi(this.axis).length)throw new Error(`Found duplicate axes in: ${this.axis}`);const n=this.axis.map((e=>t[e]));this.scale?this.gamma=this.addWeight("gamma",n,"float32",this.gammaInitializer,this.gammaRegularizer,true):this.gamma=null,this.center?this.beta=this.addWeight("beta",n,"float32",this.betaInitializer,this.betaRegularizer,true):this.beta=null,this.built=!0}call(t,n){const s=aa(t),i=s.shape,r=i.length;return e.tidy((()=>{let{mean:t,variance:n}=e.moments(s,this.axis,!0);const a=ki(1,r);for(const t of this.axis)a[t]=i[t];const o=t=>null!=t&&t.shape.length!==r&&this.axis!==[r-1]?e.reshape(t,a):t;let l=o(this.gamma.read()),u=o(this.beta.read());const h=[],c=[];for(let t=0;t<r;++t)-1!==this.axis.indexOf(t)?(h.push(i[t]),c.push(1)):(h.push(1),c.push(i[t]));return t=e.tile(t,h),n=e.tile(n,h),l=e.tile(l,c),u=e.tile(u,c),xu(s,t,n,u,l,this.epsilon)}))}getConfig(){const t={axis:this.axis,epsilon:this.epsilon,center:this.center,scale:this.scale,betaInitializer:Yr(this.betaInitializer),gammaInitializer:Yr(this.gammaInitializer),betaRegularizer:dl(this.betaRegularizer),gammaRegularizer:dl(this.gammaRegularizer)},e=super.getConfig();return Object.assign(t,e),t}}Iu.className="LayerNormalization",e.serialization.registerClass(Iu);class Au extends ba{constructor(t){if(null==t&&(t={}),super(t),this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,null==t.padding)this.padding=[[1,1],[1,1]];else if("number"==typeof t.padding)this.padding=[[t.padding,t.padding],[t.padding,t.padding]];else{if(t.padding=t.padding,2!==t.padding.length)throw new yi(`ZeroPadding2D expects padding to be a length-2 array, but received a length-${t.padding.length} array.`);let e,n;if("number"==typeof t.padding[0])e=[t.padding[0],t.padding[0]],n=[t.padding[1],t.padding[1]];else{if(t.padding=t.padding,2!==t.padding[0].length)throw new yi(`ZeroPadding2D expects height padding to be a length-2 array, but received a length-${t.padding[0].length} array.`);if(e=t.padding[0],2!==t.padding[1].length)throw new yi(`ZeroPadding2D expects width padding to be a length-2 array, but received a length-${t.padding[1].length} array.`);n=t.padding[1]}this.padding=[e,n]}this.inputSpec=[new da({ndim:4})]}computeOutputShape(t){let e,n;return t=oa(t),"channelsFirst"===this.dataFormat?(e=null!=t[2]&&t[2]>=0?t[2]+this.padding[0][0]+this.padding[0][1]:null,n=null!=t[3]&&t[3]>=0?t[3]+this.padding[1][0]+this.padding[1][1]:null,[t[0],t[1],e,n]):(e=null!=t[1]&&t[1]>=0?t[1]+this.padding[0][0]+this.padding[0][1]:null,n=null!=t[2]&&t[2]>=0?t[2]+this.padding[1][0]+this.padding[1][1]:null,[t[0],e,n,t[3]])}call(t,n){return e.tidy((()=>{return n=aa(t),s=this.padding,i=this.dataFormat,e.tidy((()=>{if(4!==n.rank)throw new yi(`temporalPadding expects input tensor to be 4-D, but received a ${n.rank}-D tensor.`);if(null==s&&(s=[[1,1],[1,1]]),2!==s.length||2!==s[0].length||2!==s[1].length)throw new yi("spatial2dPadding expects `padding` to be an Array of two Arrays, each of which is an Array of two integers.");if(null==i&&(i="channelsLast"),"channelsLast"!==i&&"channelsFirst"!==i)throw new yi(`Unknown data format: ${i}. Supported data formats are 'channelsLast' and 'channelsFirst.`);let t;return t="channelsFirst"===i?[[0,0],[0,0],s[0],s[1]]:[[0,0],s[0],s[1],[0,0]],e.pad(n,t)}));var n,s,i}))}getConfig(){const t={padding:this.padding,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}function Cu(t,n,s,i,r,a){return e.tidy((()=>{let o;nr(r),ir(a),sr(i),null==s&&(s=[1,1]),null==i&&(i="valid"),null==r&&(r="channelsLast"),null==a&&(a="max"),t=zl(t,r);const l="same"===i?"same":"valid";return o="max"===a?e.maxPool(t,n,s,l):e.avgPool(t,n,s,l),"channelsFirst"===r&&(o=e.transpose(o,[0,3,1,2])),o}))}function Tu(t,n,s,i,r,a){return e.tidy((()=>{let o;nr(r),ir(a),sr(i),null==s&&(s=[1,1,1]),null==i&&(i="valid"),null==r&&(r="channelsLast"),null==a&&(a="max"),t=Il(t,r);const l="same"===i?"same":"valid";return o="max"===a?e.maxPool3d(t,n,s,l):e.avgPool3d(t,n,s,l),"channelsFirst"===r&&(o=e.transpose(o,[0,4,1,2,3])),o}))}Au.className="ZeroPadding2D",e.serialization.registerClass(Au);class $u extends ba{constructor(t){if(null==t.poolSize&&(t.poolSize=2),super(t),"number"==typeof t.poolSize)this.poolSize=[t.poolSize];else{if(!Array.isArray(t.poolSize)||1!==t.poolSize.length||"number"!=typeof t.poolSize[0])throw new yi(`poolSize for 1D convolutional layer must be a number or an Array of a single number, but received ${JSON.stringify(t.poolSize)}`);this.poolSize=t.poolSize}if(Ri(this.poolSize,"poolSize"),null==t.strides)this.strides=this.poolSize;else if("number"==typeof t.strides)this.strides=[t.strides];else{if(!Array.isArray(t.strides)||1!==t.strides.length||"number"!=typeof t.strides[0])throw new yi(`strides for 1D convolutional layer must be a number or an Array of a single number, but received ${JSON.stringify(t.strides)}`);this.strides=t.strides}Ri(this.strides,"strides"),this.padding=null==t.padding?"valid":t.padding,sr(this.padding),this.inputSpec=[new da({ndim:3})]}computeOutputShape(t){const e=xl((t=oa(t))[1],this.poolSize[0],this.padding,this.strides[0]);return[t[0],e,t[2]]}call(t,n){return e.tidy((()=>{this.invokeCallHook(t,n),t=mr(aa(t),2);const s=this.poolingFunction(aa(t),[this.poolSize[0],1],[this.strides[0],1],this.padding,"channelsLast");return e.squeeze(s,[2])}))}getConfig(){const t={poolSize:this.poolSize,padding:this.padding,strides:this.strides},e=super.getConfig();return Object.assign(t,e),t}}class Eu extends $u{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return nr(i),sr(s),Cu(t,e,n,s,i,"max")}}Eu.className="MaxPooling1D",e.serialization.registerClass(Eu);class Fu extends $u{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return nr(i),sr(s),Cu(t,e,n,s,i,"avg")}}Fu.className="AveragePooling1D",e.serialization.registerClass(Fu);class Du extends ba{constructor(t){if(null==t.poolSize&&(t.poolSize=[2,2]),super(t),this.poolSize=Array.isArray(t.poolSize)?t.poolSize:[t.poolSize,t.poolSize],null==t.strides)this.strides=this.poolSize;else if(Array.isArray(t.strides)){if(2!==t.strides.length)throw new yi(`If the strides property of a 2D pooling layer is an Array, it is expected to have a length of 2, but received length ${t.strides.length}.`);this.strides=t.strides}else this.strides=[t.strides,t.strides];Ri(this.poolSize,"poolSize"),Ri(this.strides,"strides"),this.padding=null==t.padding?"valid":t.padding,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,nr(this.dataFormat),sr(this.padding),this.inputSpec=[new da({ndim:4})]}computeOutputShape(t){t=oa(t);let e="channelsFirst"===this.dataFormat?t[2]:t[1],n="channelsFirst"===this.dataFormat?t[3]:t[2];return e=xl(e,this.poolSize[0],this.padding,this.strides[0]),n=xl(n,this.poolSize[1],this.padding,this.strides[1]),"channelsFirst"===this.dataFormat?[t[0],t[1],e,n]:[t[0],e,n,t[3]]}call(t,n){return e.tidy((()=>(this.invokeCallHook(t,n),this.poolingFunction(aa(t),this.poolSize,this.strides,this.padding,this.dataFormat))))}getConfig(){const t={poolSize:this.poolSize,padding:this.padding,strides:this.strides,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}class Lu extends Du{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return nr(i),sr(s),Cu(t,e,n,s,i,"max")}}Lu.className="MaxPooling2D",e.serialization.registerClass(Lu);class _u extends Du{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return nr(i),sr(s),Cu(t,e,n,s,i,"avg")}}_u.className="AveragePooling2D",e.serialization.registerClass(_u);class Ru extends ba{constructor(t){if(null==t.poolSize&&(t.poolSize=[2,2,2]),super(t),this.poolSize=Array.isArray(t.poolSize)?t.poolSize:[t.poolSize,t.poolSize,t.poolSize],null==t.strides)this.strides=this.poolSize;else if(Array.isArray(t.strides)){if(3!==t.strides.length)throw new yi(`If the strides property of a 3D pooling layer is an Array, it is expected to have a length of 3, but received length ${t.strides.length}.`);this.strides=t.strides}else this.strides=[t.strides,t.strides,t.strides];Ri(this.poolSize,"poolSize"),Ri(this.strides,"strides"),this.padding=null==t.padding?"valid":t.padding,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,nr(this.dataFormat),sr(this.padding),this.inputSpec=[new da({ndim:5})]}computeOutputShape(t){t=oa(t);let e="channelsFirst"===this.dataFormat?t[2]:t[1],n="channelsFirst"===this.dataFormat?t[3]:t[2],s="channelsFirst"===this.dataFormat?t[4]:t[3];return e=xl(e,this.poolSize[0],this.padding,this.strides[0]),n=xl(n,this.poolSize[1],this.padding,this.strides[1]),s=xl(s,this.poolSize[2],this.padding,this.strides[2]),"channelsFirst"===this.dataFormat?[t[0],t[1],e,n,s]:[t[0],e,n,s,t[4]]}call(t,n){return e.tidy((()=>(this.invokeCallHook(t,n),this.poolingFunction(aa(t),this.poolSize,this.strides,this.padding,this.dataFormat))))}getConfig(){const t={poolSize:this.poolSize,padding:this.padding,strides:this.strides,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}class Mu extends Ru{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return nr(i),sr(s),Tu(t,e,n,s,i,"max")}}Mu.className="MaxPooling3D",e.serialization.registerClass(Mu);class Ou extends Ru{constructor(t){super(t)}poolingFunction(t,e,n,s,i){return nr(i),sr(s),Tu(t,e,n,s,i,"avg")}}Ou.className="AveragePooling3D",e.serialization.registerClass(Ou);class Bu extends ba{constructor(t){super(t),this.inputSpec=[new da({ndim:3})]}computeOutputShape(t){return[t[0],t[2]]}call(t,e){throw new bi}}class Pu extends Bu{constructor(t){super(t||{})}call(t,n){return e.tidy((()=>{const n=aa(t);return e.mean(n,1)}))}}Pu.className="GlobalAveragePooling1D",e.serialization.registerClass(Pu);class Wu extends Bu{constructor(t){super(t||{})}call(t,n){return e.tidy((()=>{const n=aa(t);return e.max(n,1)}))}}Wu.className="GlobalMaxPooling1D",e.serialization.registerClass(Wu);class Uu extends ba{constructor(t){super(t),this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,nr(this.dataFormat),this.inputSpec=[new da({ndim:4})]}computeOutputShape(t){return t=t,"channelsLast"===this.dataFormat?[t[0],t[3]]:[t[0],t[1]]}call(t,e){throw new bi}getConfig(){const t={dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}class ju extends Uu{call(t,n){return e.tidy((()=>{const n=aa(t);return"channelsLast"===this.dataFormat?e.mean(n,[1,2]):e.mean(n,[2,3])}))}}ju.className="GlobalAveragePooling2D",e.serialization.registerClass(ju);class Vu extends Uu{call(t,n){return e.tidy((()=>{const n=aa(t);return"channelsLast"===this.dataFormat?e.max(n,[1,2]):e.max(n,[2,3])}))}}Vu.className="GlobalMaxPooling2D",e.serialization.registerClass(Vu);class Ku extends ba{constructor(t){super(t),this.layer=t.layer}build(t){this.built=!0}get trainable(){return null!=this.layer&&this.layer.trainable}set trainable(t){null!=this.layer&&(this.layer.trainable=t)}get trainableWeights(){return this.layer.trainableWeights}get nonTrainableWeights(){return this.layer.nonTrainableWeights}get updates(){return this.layer._updates}get losses(){return this.layer.losses}getWeights(){return this.layer.getWeights()}setWeights(t){this.layer.setWeights(t)}getConfig(){const t={layer:{className:this.layer.getClassName(),config:this.layer.getConfig()}},e=super.getConfig();return Object.assign(t,e),t}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.layer&&this.layer.setFastWeightInitDuringBuild(t)}static fromConfig(t,e,n={}){const s=Da(e.layer,n);delete e.layer;const i={layer:s};return Object.assign(i,e),new t(i)}}class qu extends Ku{constructor(t){super(t),this.supportsMasking=!0}build(t){if((t=oa(t)).length<3)throw new yi(`TimeDistributed layer expects an input shape >= 3D, but received input shape ${JSON.stringify(t)}`);this.inputSpec=[{shape:t}];const e=[t[0]].concat(t.slice(2));this.layer.built||(this.layer.build(e),this.layer.built=!0),super.build(t)}computeOutputShape(t){const e=[(t=oa(t))[0]].concat(t.slice(2)),n=this.layer.computeOutputShape(e),s=t[1];return[n[0],s].concat(n.slice(1))}call(t,n){return e.tidy((()=>Wl(((t,e)=>[aa(this.layer.call(t,n)),[]]),t=aa(t),[],!1,null,null,!1,!0)[1]))}}qu.className="TimeDistributed",e.serialization.registerClass(qu);class Gu extends Ku{constructor(t){super(t);const e=t.layer.getConfig(),n={};n.className=t.layer.getClassName(),n.config=e,this.forwardLayer=Da(n),e.goBackwards=!0!==e.goBackwards;const s={};var i;if(s.className=t.layer.getClassName(),s.config=e,this.backwardLayer=Da(s),this.forwardLayer.name="forward_"+this.forwardLayer.name,this.backwardLayer.name="backward_"+this.backwardLayer.name,this.mergeMode=void 0===t.mergeMode?"concat":t.mergeMode,i=this.mergeMode,Li(tr,"BidirectionalMergeMode",i),t.weights)throw new bi("weights support is not implemented for Bidirectional layer yet.");this._stateful=t.layer.stateful,this.returnSequences=t.layer.returnSequences,this.returnState=t.layer.returnState,this.supportsMasking=!0,this._trainable=!0,this.inputSpec=t.layer.inputSpec,this.numConstants=null}get trainable(){return this._trainable}set trainable(t){this._trainable=t,null!=this.forwardLayer&&(this.forwardLayer.trainable=t),null!=this.backwardLayer&&(this.backwardLayer.trainable=t)}getWeights(){return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights())}setWeights(t){const e=t.length,n=Math.floor(e/2);this.forwardLayer.setWeights(t.slice(0,n)),this.backwardLayer.setWeights(t.slice(n))}computeOutputShape(t){let e,n,s,i=this.forwardLayer.computeOutputShape(t);return Array.isArray(i)&&Array.isArray(i[0])||(i=[i]),i=i,this.returnState?(s=i.slice(1),e=i[0]):e=i[0],e=e,"concat"===this.mergeMode?(e[e.length-1]*=2,n=[e]):n=null==this.mergeMode?[e,e.slice()]:[e],this.returnState?null==this.mergeMode?n.concat(s).concat(s.slice()):[e].concat(s).concat(s.slice()):xi(n)}apply(t,e){let n=null==e?null:e.initialState,s=null==e?null:e.constants;null==e&&(e={});const i=Pl(t,n,s,this.numConstants);if(t=i.inputs,n=i.initialState,s=i.constants,Array.isArray(t)&&(n=t.slice(1),t=t[0]),(null==n||0===n.length)&&null==s)return super.apply(t,e);const r=[],a=[];if(null!=n){const t=n.length;if(t%2>0)throw new yi("When passing `initialState` to a Bidrectional RNN, the state should be an Array containing the states of the underlying RNNs.");e.initialState=n,r.push(...n);const s=n.map((t=>new da({shape:t.shape})));this.forwardLayer.stateSpec=s.slice(0,t/2),this.backwardLayer.stateSpec=s.slice(t/2),a.push(...s)}if(null!=s)throw new bi("Support for constants in Bidirectional layers is not implemented yet.");const o=r[0]instanceof fa;for(const t of r)if(t instanceof fa!==o)throw new yi("The initial state of a Bidirectional layer cannot be specified as a mix of symbolic and non-symbolic tensors");if(o){const n=[t].concat(r),s=this.inputSpec.concat(a),i=this.inputSpec;this.inputSpec=s;const o=super.apply(n,e);return this.inputSpec=i,o}return super.apply(t,e)}call(t,n){return e.tidy((()=>{const s=n.initialState;let i,r,a,o;if(null==s)i=this.forwardLayer.call(t,n),r=this.backwardLayer.call(t,n);else{const e=s.slice(0,s.length/2),a=s.slice(s.length/2);i=this.forwardLayer.call(t,Object.assign(n,{initialState:e})),r=this.backwardLayer.call(t,Object.assign(n,{initialState:a}))}return this.returnState&&(Array.isArray(i)&&(a=i.slice(1).concat(r.slice(1))),i=i[0],r=r[0]),this.returnSequences&&(r=e.reverse(r,1)),"concat"===this.mergeMode?o=kr([i,r]):"sum"===this.mergeMode?o=e.add(i,r):"ave"===this.mergeMode?o=e.mul(.5,e.add(i,r)):"mul"===this.mergeMode?o=e.mul(i,r):null==this.mergeMode&&(o=[i,r]),this.returnState?null==this.mergeMode?o.concat(a):[o].concat(a):o}))}resetStates(t){this.forwardLayer.resetStates(),this.backwardLayer.resetStates()}build(t){ar(this.forwardLayer.name,(()=>{this.forwardLayer.build(t)})),ar(this.backwardLayer.name,(()=>{this.backwardLayer.build(t)})),this.built=!0}computeMask(t,e){let n;if(Array.isArray(e)&&(e=e[0]),n=this.returnSequences?null==this.mergeMode?[e,e]:e:null==this.mergeMode?[null,null]:null,this.returnState){const t=this.forwardLayer.states.map((t=>null));return Array.isArray(n)?n.concat(t).concat(t):[n].concat(t).concat(t)}return n}get trainableWeights(){return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights)}get nonTrainableWeights(){return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights)}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.forwardLayer&&this.forwardLayer.setFastWeightInitDuringBuild(t),null!=this.backwardLayer&&this.backwardLayer.setFastWeightInitDuringBuild(t)}getConfig(){const t={mergeMode:this.mergeMode},e=super.getConfig();return Object.assign(t,e),t}static fromConfig(t,e){const n=Da(e.layer);if(delete e.layer,null!=e.numConstants)throw new bi("Deserialization of a Bidirectional layer with numConstants present is not supported yet.");const s=e;return s.layer=n,new t(s)}}function Hu(t){return new Fu(t)}function Ju(t){return new _u(t)}function Zu(t){return new Ou(t)}function Yu(t){return new Wu(t)}function Xu(t){return new Vu(t)}function Qu(t){return new Eu(t)}function th(t){return new Lu(t)}Gu.className="Bidirectional",e.serialization.registerClass(Gu);const eh=Yu,nh=Xu,sh=Qu,ih=th;var rh=Object.freeze({__proto__:null,inputLayer:function(t){return new ka(t)},elu:function(t){return new wl(t)},reLU:function(t){return new ml(t)},leakyReLU:function(t){return new yl(t)},prelu:function(t){return new bl(t)},softmax:function(t){return new vl(t)},thresholdedReLU:function(t){return new kl(t)},conv1d:function(t){return new Rl(t)},conv2d:function(t){return new $l(t)},conv2dTranspose:function(t){return new Fl(t)},conv3d:function(t){return new El(t)},conv3dTranspose:function(t){return new Dl(t)},separableConv2d:function(t){return new _l(t)},cropping2D:function(t){return new Ml(t)},upSampling2d:function(t){return new Ol(t)},depthwiseConv2d:function(t){return new Bl(t)},activation:function(t){return new ru(t)},dense:function(t){return new su(t)},dropout:function(t){return new eu(t)},spatialDropout1d:function(t){return new nu(t)},flatten:function(t){return new iu(t)},repeatVector:function(t){return new au(t)},reshape:function(t){return new ou(t)},permute:function(t){return new lu(t)},embedding:function(t){return new hu(t)},add:function(t){return new pu(t)},average:function(t){return new fu(t)},concatenate:function(t){return new yu(t)},maximum:function(t){return new gu(t)},minimum:function(t){return new mu(t)},multiply:function(t){return new du(t)},dot:function(t){return new wu(t)},batchNormalization:function(t){return new zu(t)},layerNormalization:function(t){return new Iu(t)},zeroPadding2d:function(t){return new Au(t)},averagePooling1d:Hu,avgPool1d:function(t){return Hu(t)},avgPooling1d:function(t){return Hu(t)},averagePooling2d:Ju,avgPool2d:function(t){return Ju(t)},avgPooling2d:function(t){return Ju(t)},averagePooling3d:Zu,avgPool3d:function(t){return Zu(t)},avgPooling3d:function(t){return Zu(t)},globalAveragePooling1d:function(t){return new Pu(t)},globalAveragePooling2d:function(t){return new ju(t)},globalMaxPooling1d:Yu,globalMaxPooling2d:Xu,maxPooling1d:Qu,maxPooling2d:th,maxPooling3d:function(t){return new Mu(t)},gru:function(t){return new Gl(t)},gruCell:function(t){return new ql(t)},lstm:function(t){return new Jl(t)},lstmCell:function(t){return new Hl(t)},simpleRNN:function(t){return new Kl(t)},simpleRNNCell:function(t){return new Vl(t)},convLstm2d:function(t){return new tu(t)},convLstm2dCell:function(t){return new Ql(t)},rnn:function(t){return new Ul(t)},stackedRNNCells:function(t){return new Zl(t)},bidirectional:function(t){return new Gu(t)},timeDistributed:function(t){return new qu(t)},globalMaxPool1d:eh,globalMaxPool2d:nh,maxPool1d:sh,maxPool2d:ih,Layer:ba,RNN:Ul,RNNCell:jl,input:Vo,gaussianNoise:function(t){return new ku(t)},gaussianDropout:function(t){return new vu(t)},alphaDropout:function(t){return new Su(t)},masking:function(t){return new uu(t)}});var ah=Object.freeze({__proto__:null,binaryAccuracy:function(t,e){return Va(t,e)},binaryCrossentropy:function(t,e){return Ja(t,e)},sparseCategoricalAccuracy:function(t,e){return Za(t,e)},categoricalAccuracy:function(t,e){return Ka(t,e)},categoricalCrossentropy:function(t,e){return Ya(t,e)},precision:function(t,e){return Ga(t,e)},recall:function(t,e){return Ha(t,e)},cosineProximity:function(t,e){return Wa(t,e)},meanAbsoluteError:function(t,e){return Ra(t,e)},meanAbsolutePercentageError:function(t,e){return Ma(t,e)},MAPE:function(t,e){return Ma(t,e)},mape:function(t,e){return Ma(t,e)},meanSquaredError:function(t,e){return _a(t,e)},MSE:function(t,e){return _a(t,e)},mse:function(t,e){return _a(t,e)}}),oh=Object.freeze({__proto__:null,modelFromJSON:async function(t,n){"modelTopology"in t||(t={modelTopology:t});let s=(t=t).modelTopology;null!=s.model_config&&(s=s.model_config);const i=Da(ho(s),n);if(null!=t.weightsManifest){const n=await e.io.loadWeights(t.weightsManifest,t.pathPrefix,i.weights.map((t=>t.originalName))),s={};for(const t of i.weights)s[t.originalName]=n[t.originalName];i.loadWeights(s),e.dispose(n)}return i}});var lh=Object.freeze({__proto__:null,l1l2:function(t){return new cl(t)},l1:function(t){return ul(e=t),new cl({l1:null!=e?e.l1:null,l2:0});var e},l2:function(t){return ul(e=t),new cl({l2:null!=e?e.l2:null,l1:0});var e}});class uh extends za{constructor(){super(...arguments),this.model=null}setModel(t){if(!(t instanceof Po))throw new Error("model must be a LayersModel, not some other Container");this.model=t}}function hh(t,e){return t<e}function ch(t,e){return t>e}class ph extends uh{constructor(t){if(super(),null==t&&(t={}),t.restoreBestWeights)throw new bi("restoreBestWeights = True is not implemented in EarlyStopping yet.");this.monitor=t.monitor||"val_loss",this.minDelta=Math.abs(t.minDelta||0),this.patience=t.patience||0,this.verbose=t.verbose||0,this.mode=t.mode||"auto",this.baseline=t.baseline,-1===["auto","min","max"].indexOf(this.mode)&&(console.warn(`EarlyStopping mode '${this.mode}' is invalid. Falling back to mode 'auto'.`),this.mode="auto"),"min"===this.mode?this.monitorFunc=hh:"max"===this.mode||-1!==this.monitor.indexOf("acc")?this.monitorFunc=ch:this.monitorFunc=hh,this.monitorFunc===hh&&(this.minDelta*=-1)}async onTrainBegin(t){this.wait=0,this.stoppedEpoch=0,null!=this.baseline?this.best=this.baseline:this.best=this.monitorFunc===hh?1/0:-1/0}async onEpochEnd(t,e){await Sa(e);const n=this.getMonitorValue(e);null!=n&&(this.monitorFunc(n-this.minDelta,this.best)?(this.best=n,this.wait=0):(this.wait++,this.wait>=this.patience&&(this.stoppedEpoch=t,this.model.stopTraining=!0)))}async onTrainEnd(t){this.stoppedEpoch>0&&this.verbose&&console.log(`Epoch ${this.stoppedEpoch}: early stopping.`)}getMonitorValue(t){null==t&&(t={});const e=t[this.monitor];return null==e&&console.warn(`Metric for EarlyStopping ${this.monitor} is not available. Available metrics are: ${Object.keys(t)}`),e}}const dh={earlyStopping:function(t){return new ph(t)}};t.Callback=uh,t.CallbackList=Ia,t.CustomCallback=Ta,t.EarlyStopping=ph,t.History=Ca,t.InputSpec=da,t.LayerVariable=ha,t.LayersModel=Po,t.RNN=Ul,t.Sequential=jo,t.SymbolicTensor=fa,t.callbacks=dh,t.constraints=Ji,t.initializers=Qr,t.input=Vo,t.layers=rh,t.loadLayersModel=function(t,e){return null==e&&(e={}),Uo(t,e)},t.metrics=ah,t.model=function(t){return new Po(t)},t.models=oh,t.registerCallbackConstructor=function(t,e){Ea.registerCallbackConstructor(t,e)},t.regularizers=lh,t.sequential=function(t){return new jo(t)},t.version_layers=po,Object.defineProperty(t,"__esModule",{value:!0})}));
//# sourceMappingURL=tf-layers.es2017.min.js.map
